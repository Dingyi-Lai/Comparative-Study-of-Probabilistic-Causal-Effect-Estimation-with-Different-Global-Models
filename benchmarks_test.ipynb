{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# imports for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'darts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm_notebook \u001b[38;5;28;01mas\u001b[39;00m tqdm\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdarts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TimeSeries, concatenate\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdarts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Scaler\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdarts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TFTModel\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'darts'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from darts import TimeSeries, concatenate\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.models import TFTModel\n",
    "from darts.metrics import mape\n",
    "from darts.utils.statistics import check_seasonality, plot_acf\n",
    "from darts.datasets import AirPassengersDataset, IceCreamHeaterDataset\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "from darts.utils.likelihood_models import QuantileRegression\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "\n",
    "logging.disable(logging.CRITICAL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from benchmarks.mqrnn import MQRNN\n",
    "# import tensorflow as tf\n",
    "import pandas as pd\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset_name = 'calls911_benchmarks'\n",
    "data = pd.read_csv('./datasets/text_data/calls911/'+dataset_name+'.csv')\n",
    "# data[\"time_idx\"] = data.index.to_list()\n",
    "# data_melted = pd.melt(data, id_vars=['date','time_idx'])\n",
    "# data_melted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-12-01</td>\n",
       "      <td>0</td>\n",
       "      <td>ABINGTON</td>\n",
       "      <td>514.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>ABINGTON</td>\n",
       "      <td>727.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-02-01</td>\n",
       "      <td>2</td>\n",
       "      <td>ABINGTON</td>\n",
       "      <td>713.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-03-01</td>\n",
       "      <td>3</td>\n",
       "      <td>ABINGTON</td>\n",
       "      <td>668.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>4</td>\n",
       "      <td>ABINGTON</td>\n",
       "      <td>728.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3467</th>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>51</td>\n",
       "      <td>WORCESTER</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3468</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>52</td>\n",
       "      <td>WORCESTER</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3469</th>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>53</td>\n",
       "      <td>WORCESTER</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3470</th>\n",
       "      <td>2020-06-01</td>\n",
       "      <td>54</td>\n",
       "      <td>WORCESTER</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3471</th>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>55</td>\n",
       "      <td>WORCESTER</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3472 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  time_idx   variable  value\n",
       "0     2015-12-01         0   ABINGTON  514.0\n",
       "1     2016-01-01         1   ABINGTON  727.0\n",
       "2     2016-02-01         2   ABINGTON  713.0\n",
       "3     2016-03-01         3   ABINGTON  668.0\n",
       "4     2016-04-01         4   ABINGTON  728.0\n",
       "...          ...       ...        ...    ...\n",
       "3467  2020-03-01        51  WORCESTER   56.0\n",
       "3468  2020-04-01        52  WORCESTER   55.0\n",
       "3469  2020-05-01        53  WORCESTER   83.0\n",
       "3470  2020-06-01        54  WORCESTER   82.0\n",
       "3471  2020-07-01        55  WORCESTER   85.0\n",
       "\n",
       "[3472 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "treated = [\"ABINGTON\",  \"AMBLER\",  \"CHELTENHAM\",  \"COLLEGEVILLE\",  \"CONSHOHOCKEN\", \n",
    "                   \"EAST GREENVILLE\",  \"EAST NORRITON\",  \"FRANCONIA\" , \"GREEN LANE\", \"HATFIELD TOWNSHIP\", \n",
    "                   \"HORSHAM\" , \"JENKINTOWN\",  \"LANSDALE\",  \"LIMERICK\",  \"LOWER GWYNEDD\", \n",
    "                   \"LOWER MERION\",  \"LOWER MORELAND\",  \"LOWER POTTSGROVE\",  \"LOWER PROVIDENCE\",  \"LOWER SALFORD\", \n",
    "                   \"MARLBOROUGH\",  \"MONTGOMERY\",  \"NARBERTH\",  \"PENNSBURG\",  \"PERKIOMEN\", \n",
    "                   \"PLYMOUTH\",  \"POTTSTOWN\",  \"RED HILL\",  \"ROCKLEDGE\",  \"ROYERSFORD\", \n",
    "                   \"SCHWENKSVILLE\",  \"SKIPPACK\",  \"SOUDERTON\",  \"TELFORD\",  \"TOWAMENCIN\", \n",
    "                   \"UPPER DUBLIN\",  \"UPPER FREDERICK\",  \"UPPER GWYNEDD\",  \"UPPER HANOVER\",  \"UPPER MERION\", \n",
    "                   \"UPPER MORELAND\",  \"UPPER POTTSGROVE\",  \"UPPER PROVIDENCE\",  \"UPPER SALFORD\",  \"WEST CONSHOHOCKEN\", \n",
    "                   \"WEST NORRITON\",  \"WEST POTTSGROVE\",  \"WHITEMARSH\",  \"WHITPAIN\",  \"WORCESTER\"]\n",
    "control = [\"BRIDGEPORT\", \"BRYN ATHYN\", \"DOUGLASS\", \"HATBORO\", \"HATFIELD BORO\", \n",
    "                   \"LOWER FREDERICK\", \"NEW HANOVER\", \"NORRISTOWN\", \"NORTH WALES\", \"SALFORD\", \n",
    "                   \"SPRINGFIELD\", \"TRAPPE\"]\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "      <th>tnc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-12-01</td>\n",
       "      <td>0</td>\n",
       "      <td>ABINGTON</td>\n",
       "      <td>514.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>ABINGTON</td>\n",
       "      <td>727.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-02-01</td>\n",
       "      <td>2</td>\n",
       "      <td>ABINGTON</td>\n",
       "      <td>713.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-03-01</td>\n",
       "      <td>3</td>\n",
       "      <td>ABINGTON</td>\n",
       "      <td>668.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>4</td>\n",
       "      <td>ABINGTON</td>\n",
       "      <td>728.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3467</th>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>51</td>\n",
       "      <td>WORCESTER</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3468</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>52</td>\n",
       "      <td>WORCESTER</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3469</th>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>53</td>\n",
       "      <td>WORCESTER</td>\n",
       "      <td>83.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3470</th>\n",
       "      <td>2020-06-01</td>\n",
       "      <td>54</td>\n",
       "      <td>WORCESTER</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3471</th>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>55</td>\n",
       "      <td>WORCESTER</td>\n",
       "      <td>85.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3472 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  time_idx   variable  value  tnc\n",
       "0     2015-12-01         0   ABINGTON  514.0    1\n",
       "1     2016-01-01         1   ABINGTON  727.0    1\n",
       "2     2016-02-01         2   ABINGTON  713.0    1\n",
       "3     2016-03-01         3   ABINGTON  668.0    1\n",
       "4     2016-04-01         4   ABINGTON  728.0    1\n",
       "...          ...       ...        ...    ...  ...\n",
       "3467  2020-03-01        51  WORCESTER   56.0    1\n",
       "3468  2020-04-01        52  WORCESTER   55.0    1\n",
       "3469  2020-05-01        53  WORCESTER   83.0    1\n",
       "3470  2020-06-01        54  WORCESTER   82.0    1\n",
       "3471  2020-07-01        55  WORCESTER   85.0    1\n",
       "\n",
       "[3472 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a binary mask indicating whether each column is treated (1) or control (0)\n",
    "data_melted['tnc'] = [1 if col in treated else 0 for col in data_melted.variable]\n",
    "data_melted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 7 7\n"
     ]
    }
   ],
   "source": [
    "from preprocess_scripts.data_loader import DataLoader\n",
    "dataset_name = 'calls911_benchmarks'\n",
    "batch_size = 21\n",
    "input_size = 56\n",
    "forecast_horizon=7\n",
    "feature_type='MS'\n",
    "target='ABINGTON'\n",
    "data_loader = DataLoader(dataset_name,batch_size,input_size,forecast_horizon,feature_type, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m cate_mask \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m treated \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m all_columns]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Convert the mask to a PyTorch tensor\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m cate_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mtensor(cate_mask, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "all_columns = data.columns.tolist()\n",
    "\n",
    "# Create a binary mask indicating whether each column is treated (1) or control (0)\n",
    "cate_mask = [1 if col in treated else 0 for col in all_columns]\n",
    "\n",
    "# Convert the mask to a PyTorch tensor\n",
    "cate_tensor = torch.tensor(cate_mask, dtype=torch.float32).view(1, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cate_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_loader.get_train()\n",
    "val_data = data_loader.get_val()\n",
    "test_data = data_loader.get_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_data:\n",
    "    x,y = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(21, 15, 62), dtype=float32, numpy=\n",
       "array([[[-1.63818628e-01, -1.74985635e+00, -5.34819961e-01, ...,\n",
       "         -1.09709918e+00,  9.75768030e-01, -1.79793131e+00],\n",
       "        [-7.55820930e-01, -1.25494754e+00, -6.17402494e-01, ...,\n",
       "         -1.12326503e+00, -1.01767220e-01, -1.36080766e+00],\n",
       "        [ 3.35154720e-02, -4.30099368e-01, -1.11289752e+00, ...,\n",
       "         -1.80357873e+00, -2.99315359e-02,  5.20385336e-03],\n",
       "        ...,\n",
       "        [-1.00577748e+00, -1.00749302e+00, -1.85613990e+00, ...,\n",
       "         -1.43725598e+00, -2.36459136e+00, -2.13357985e-01],\n",
       "        [-1.50663018e-01, -1.25494754e+00, -1.60839248e+00, ...,\n",
       "         -1.04476738e+00,  2.21493363e-01,  8.79451215e-01],\n",
       "        [ 3.49250019e-01,  1.47294313e-01, -5.34819961e-01, ...,\n",
       "         -1.09709918e+00, -1.75398791e+00, -9.78324413e-01]],\n",
       "\n",
       "       [[-8.48849863e-02,  1.38456655e+00,  4.56169993e-01, ...,\n",
       "         -7.04610527e-01,  4.01082575e-01, -1.04077064e-01],\n",
       "        [ 2.70316392e-01,  6.48094937e-02,  2.08422497e-01, ...,\n",
       "          7.08348513e-01, -2.81356424e-01,  2.78406143e-01],\n",
       "        [-9.80405882e-02, -1.25494754e+00, -6.99984968e-01, ...,\n",
       "         -2.33624190e-01, -1.10746682e+00, -1.04077064e-01],\n",
       "        ...,\n",
       "        [ 6.64984584e-01,  7.24687994e-01, -4.52237487e-01, ...,\n",
       "          1.33633029e+00,  2.21493363e-01, -4.94366065e-02],\n",
       "        [-7.55820930e-01, -2.65129745e-01, -2.87072480e-01, ...,\n",
       "          7.08348513e-01, -6.04617000e-01, -1.58717528e-01],\n",
       "        [-8.08443308e-01,  7.24687994e-01,  1.28199494e+00, ...,\n",
       "          1.20550084e+00,  6.52507484e-01,  5.51608443e-01]],\n",
       "\n",
       "       [[ 7.43918240e-01,  1.13711202e+00, -1.19547999e+00, ...,\n",
       "         -4.16785538e-01, -9.63795424e-01,  5.51608443e-01],\n",
       "        [-1.11196198e-01, -3.47614557e-01, -3.93249989e-02, ...,\n",
       "         -1.02794640e-01, -6.58493787e-02,  5.98443113e-02],\n",
       "        [ 3.36094409e-01,  9.72142458e-01,  4.32574973e-02, ...,\n",
       "          9.43841696e-01,  1.49657682e-01, -2.13357985e-01],\n",
       "        ...,\n",
       "        [-4.53241974e-01,  8.07172835e-01,  5.38752496e-01, ...,\n",
       "          1.07467127e+00, -9.99713302e-01,  5.98443113e-02],\n",
       "        [-5.05864382e-01,  1.30208170e+00, -7.82567501e-01, ...,\n",
       "          9.17675793e-01,  9.03932333e-01,  1.26193440e+00],\n",
       "        [ 8.09696257e-01, -1.76753178e-02,  3.18139243e+00, ...,\n",
       "          4.99021262e-01,  1.26311076e+00,  1.48049629e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-6.50576055e-01, -5.12584209e-01,  6.21334970e-01, ...,\n",
       "          2.89693981e-01,  2.01738548e+00, -1.04077064e-01],\n",
       "        [ 9.92935076e-02,  8.07172835e-01, -1.19547999e+00, ...,\n",
       "         -4.16785538e-01,  3.29246879e-01,  1.69125229e-01],\n",
       "        [ 4.01872456e-01, -6.77553833e-01, -8.65149975e-01, ...,\n",
       "         -5.47615111e-01,  1.49657682e-01, -5.95841229e-01],\n",
       "        ...,\n",
       "        [ 3.75561237e-01, -1.50240195e+00, -1.21907495e-01, ...,\n",
       "         -5.04628234e-02, -6.04617000e-01,  5.98443113e-02],\n",
       "        [ 1.12543082e+00, -1.76753178e-02,  8.69082451e-01, ...,\n",
       "          2.89693981e-01,  8.68014514e-01,  2.78406143e-01],\n",
       "        [ 4.54494864e-01,  1.21959686e+00,  9.51664984e-01, ...,\n",
       "          1.38866210e+00, -2.99315359e-02,  1.53513670e+00]],\n",
       "\n",
       "       [[-2.78178430e+00, -2.40973496e+00, -1.93872249e+00, ...,\n",
       "         -2.79788327e+00, -2.22091985e+00, -2.50825739e+00],\n",
       "        [ 2.03598663e-02,  3.12263936e-01,  3.73587489e-01, ...,\n",
       "         -1.02794640e-01,  5.08836091e-01, -8.14403057e-01],\n",
       "        [-1.63818628e-01, -1.74985635e+00, -5.34819961e-01, ...,\n",
       "         -1.09709918e+00,  9.75768030e-01, -1.79793131e+00],\n",
       "        ...,\n",
       "        [ 9.92935076e-02,  8.07172835e-01, -1.19547999e+00, ...,\n",
       "         -4.16785538e-01,  3.29246879e-01,  1.69125229e-01],\n",
       "        [ 4.01872456e-01, -6.77553833e-01, -8.65149975e-01, ...,\n",
       "         -5.47615111e-01,  1.49657682e-01, -5.95841229e-01],\n",
       "        [-1.00577748e+00, -1.00749302e+00, -1.85613990e+00, ...,\n",
       "         -1.43725598e+00, -2.36459136e+00, -2.13357985e-01]],\n",
       "\n",
       "       [[-1.00577748e+00, -1.00749302e+00, -1.85613990e+00, ...,\n",
       "         -1.43725598e+00, -2.36459136e+00, -2.13357985e-01],\n",
       "        [-1.50663018e-01, -1.25494754e+00, -1.60839248e+00, ...,\n",
       "         -1.04476738e+00,  2.21493363e-01,  8.79451215e-01],\n",
       "        [ 3.49250019e-01,  1.47294313e-01, -5.34819961e-01, ...,\n",
       "         -1.09709918e+00, -1.75398791e+00, -9.78324413e-01],\n",
       "        ...,\n",
       "        [-7.95287728e-01, -1.00160129e-01,  1.25839993e-01, ...,\n",
       "         -8.09274197e-01, -1.43072736e+00,  8.24810743e-01],\n",
       "        [ 4.54588842e+00,  2.20941472e+00,  7.03917503e-01, ...,\n",
       "          2.64462566e+00,  2.52023530e+00,  2.40938401e+00],\n",
       "        [-5.71642399e-01,  2.29779124e-01,  4.32574973e-02, ...,\n",
       "          1.86899351e-03, -1.50256312e+00, -1.58717528e-01]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(21, 7, 1), dtype=float32, numpy=\n",
       "array([[[-0.08488499],\n",
       "        [ 0.2703164 ],\n",
       "        [-0.09804059],\n",
       "        [ 0.74391824],\n",
       "        [-0.1111962 ],\n",
       "        [ 0.3360944 ],\n",
       "        [ 0.37556124]],\n",
       "\n",
       "       [[-0.45324197],\n",
       "        [-0.5058644 ],\n",
       "        [ 0.80969626],\n",
       "        [ 1.1254308 ],\n",
       "        [-0.5716424 ],\n",
       "        [ 0.09929351],\n",
       "        [-0.78213215]],\n",
       "\n",
       "       [[ 1.1254308 ],\n",
       "        [-0.5716424 ],\n",
       "        [ 0.09929351],\n",
       "        [-0.78213215],\n",
       "        [-0.2953747 ],\n",
       "        [-0.07172938],\n",
       "        [ 0.7702294 ]],\n",
       "\n",
       "       [[-0.5058644 ],\n",
       "        [ 0.80969626],\n",
       "        [ 1.1254308 ],\n",
       "        [-0.5716424 ],\n",
       "        [ 0.09929351],\n",
       "        [-0.78213215],\n",
       "        [-0.2953747 ]],\n",
       "\n",
       "       [[-0.75582093],\n",
       "        [-0.8084433 ],\n",
       "        [-0.45324197],\n",
       "        [-0.5058644 ],\n",
       "        [ 0.80969626],\n",
       "        [ 1.1254308 ],\n",
       "        [-0.5716424 ]],\n",
       "\n",
       "       [[-0.8084433 ],\n",
       "        [-0.45324197],\n",
       "        [-0.5058644 ],\n",
       "        [ 0.80969626],\n",
       "        [ 1.1254308 ],\n",
       "        [-0.5716424 ],\n",
       "        [ 0.09929351]],\n",
       "\n",
       "       [[ 1.1254308 ],\n",
       "        [ 0.45449486],\n",
       "        [-0.7952877 ],\n",
       "        [ 4.5458884 ],\n",
       "        [-0.5716424 ],\n",
       "        [ 0.6649846 ],\n",
       "        [-0.75582093]],\n",
       "\n",
       "       [[ 0.34925002],\n",
       "        [-0.08488499],\n",
       "        [ 0.2703164 ],\n",
       "        [-0.09804059],\n",
       "        [ 0.74391824],\n",
       "        [-0.1111962 ],\n",
       "        [ 0.3360944 ]],\n",
       "\n",
       "       [[-0.1111962 ],\n",
       "        [ 0.3360944 ],\n",
       "        [ 0.37556124],\n",
       "        [ 1.1254308 ],\n",
       "        [ 0.45449486],\n",
       "        [-0.7952877 ],\n",
       "        [ 4.5458884 ]],\n",
       "\n",
       "       [[ 0.37556124],\n",
       "        [ 1.1254308 ],\n",
       "        [ 0.45449486],\n",
       "        [-0.7952877 ],\n",
       "        [ 4.5458884 ],\n",
       "        [-0.5716424 ],\n",
       "        [ 0.6649846 ]],\n",
       "\n",
       "       [[-0.5716424 ],\n",
       "        [ 0.6649846 ],\n",
       "        [-0.75582093],\n",
       "        [-0.8084433 ],\n",
       "        [-0.45324197],\n",
       "        [-0.5058644 ],\n",
       "        [ 0.80969626]],\n",
       "\n",
       "       [[ 0.45449486],\n",
       "        [-0.7952877 ],\n",
       "        [ 4.5458884 ],\n",
       "        [-0.5716424 ],\n",
       "        [ 0.6649846 ],\n",
       "        [-0.75582093],\n",
       "        [-0.8084433 ]],\n",
       "\n",
       "       [[-0.09804059],\n",
       "        [ 0.74391824],\n",
       "        [-0.1111962 ],\n",
       "        [ 0.3360944 ],\n",
       "        [ 0.37556124],\n",
       "        [ 1.1254308 ],\n",
       "        [ 0.45449486]],\n",
       "\n",
       "       [[ 0.74391824],\n",
       "        [-0.1111962 ],\n",
       "        [ 0.3360944 ],\n",
       "        [ 0.37556124],\n",
       "        [ 1.1254308 ],\n",
       "        [ 0.45449486],\n",
       "        [-0.7952877 ]],\n",
       "\n",
       "       [[ 0.2703164 ],\n",
       "        [-0.09804059],\n",
       "        [ 0.74391824],\n",
       "        [-0.1111962 ],\n",
       "        [ 0.3360944 ],\n",
       "        [ 0.37556124],\n",
       "        [ 1.1254308 ]],\n",
       "\n",
       "       [[ 0.80969626],\n",
       "        [ 1.1254308 ],\n",
       "        [-0.5716424 ],\n",
       "        [ 0.09929351],\n",
       "        [-0.78213215],\n",
       "        [-0.2953747 ],\n",
       "        [-0.07172938]],\n",
       "\n",
       "       [[ 4.5458884 ],\n",
       "        [-0.5716424 ],\n",
       "        [ 0.6649846 ],\n",
       "        [-0.75582093],\n",
       "        [-0.8084433 ],\n",
       "        [-0.45324197],\n",
       "        [-0.5058644 ]],\n",
       "\n",
       "       [[ 0.3360944 ],\n",
       "        [ 0.37556124],\n",
       "        [ 1.1254308 ],\n",
       "        [ 0.45449486],\n",
       "        [-0.7952877 ],\n",
       "        [ 4.5458884 ],\n",
       "        [-0.5716424 ]],\n",
       "\n",
       "       [[-0.7952877 ],\n",
       "        [ 4.5458884 ],\n",
       "        [-0.5716424 ],\n",
       "        [ 0.6649846 ],\n",
       "        [-0.75582093],\n",
       "        [-0.8084433 ],\n",
       "        [-0.45324197]],\n",
       "\n",
       "       [[-0.15066302],\n",
       "        [ 0.34925002],\n",
       "        [-0.08488499],\n",
       "        [ 0.2703164 ],\n",
       "        [-0.09804059],\n",
       "        [ 0.74391824],\n",
       "        [-0.1111962 ]],\n",
       "\n",
       "       [[ 0.6649846 ],\n",
       "        [-0.75582093],\n",
       "        [-0.8084433 ],\n",
       "        [-0.45324197],\n",
       "        [-0.5058644 ],\n",
       "        [ 0.80969626],\n",
       "        [ 1.1254308 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_MapDataset element_spec=(TensorSpec(shape=(None, 56, 62), dtype=tf.float32, name=None), TensorSpec(shape=(None, 7, 1), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_MapDataset element_spec=(TensorSpec(shape=(None, 56, 62), dtype=tf.float32, name=None), TensorSpec(shape=(None, 7, 1), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# pd.read_csv('./datasets/text_data/calls911/weather.csv').dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'global_decoder_8' (type GlobalDecoder).\n\nnot enough values to unpack (expected 3, got 2)\n\nCall arguments received by layer 'global_decoder_8' (type GlobalDecoder):\n  â€¢ future=tf.Tensor(shape=(21, 7, 1), dtype=float32)\n  â€¢ hidden=tf.Tensor(shape=(21, 62), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 78\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhere\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 78\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmqrnn_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconti\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcate_tensor_expanded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# Reshape the target tensor to match the output\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     target \u001b[38;5;241m=\u001b[39m forecast_tensor\n",
      "File \u001b[0;32m~/master_thesis/benchmarks/mqrnn.py:233\u001b[0m, in \u001b[0;36mMQRNN.call\u001b[0;34m(self, conti, cate, future)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, conti, cate, future):\n\u001b[1;32m    232\u001b[0m     hidden, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(conti, cate)\n\u001b[0;32m--> 233\u001b[0m     embedded_future, global_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglobal_decoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_decoder(embedded_future, global_output)\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:61\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror_handler\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdebugging\u001b[38;5;241m.\u001b[39mis_traceback_filtering_enabled():\n\u001b[0;32m---> 61\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/base_layer.py:1149\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1148\u001b[0m ):\n\u001b[0;32m-> 1149\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:156\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m e\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\u001b[38;5;241m.\u001b[39mwith_traceback(e\u001b[38;5;241m.\u001b[39m__traceback__) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m signature\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:97\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/master_thesis/benchmarks/mqrnn.py:182\u001b[0m, in \u001b[0;36mGlobalDecoder.call\u001b[0;34m(self, future, hidden)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# Assuming future is a tensor indicating the treated unit\u001b[39;00m\n\u001b[1;32m    180\u001b[0m cate_tensor \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(future, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 182\u001b[0m num_layers, _, _ \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mshape(hidden)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model \u001b[38;5;241m==\u001b[39m num_layers\n\u001b[1;32m    186\u001b[0m x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat([hidden[num_layers \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m], cate_tensor], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'global_decoder_8' (type GlobalDecoder).\n\nnot enough values to unpack (expected 3, got 2)\n\nCall arguments received by layer 'global_decoder_8' (type GlobalDecoder):\n  â€¢ future=tf.Tensor(shape=(21, 7, 1), dtype=float32)\n  â€¢ hidden=tf.Tensor(shape=(21, 62), dtype=float32)"
     ]
    }
   ],
   "source": [
    "from benchmarks.mqrnn import MQRNN\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset_name = 'calls911_benchmarks'\n",
    "data = pd.read_csv('./datasets/text_data/calls911/'+dataset_name+'.csv')\n",
    "# Define parameters for MQRNN model\n",
    "d_model = data_loader.n_feature  # Adjust based on your data\n",
    "# d_model = 62  # You can adjust this value\n",
    "tau = 7  # You can adjust this value\n",
    "num_targets = 1  # You can adjust this value\n",
    "num_quantiles = 3  # You can adjust this value based on your task\n",
    "n_layers = 2  # You can adjust this value\n",
    "dr = 0.1  # You can adjust this value\n",
    "num_epochs = 5\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Update the MQRNN model instantiation\n",
    "mqrnn_model = MQRNN(\n",
    "    d_model=d_model,\n",
    "    tau=tau,\n",
    "    num_targets=num_targets,\n",
    "    num_quantiles=num_quantiles,\n",
    "    n_layers=n_layers,\n",
    "    dr=dr\n",
    ")\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = tf.keras.losses.MeanSquaredError()  # Replace with your actual loss function\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "train_data = data_loader.get_train()\n",
    "val_data = data_loader.get_val()\n",
    "test_data = data_loader.get_test()\n",
    "\n",
    "all_columns = data.columns.tolist()[1:]\n",
    "\n",
    "# Create a binary mask indicating whether each column is treated (1) or control (0)\n",
    "cate_mask = [1 if col in treated else 0 for col in all_columns]\n",
    "\n",
    "# Convert the mask to a TensorFlow tensor\n",
    "cate_tensor = tf.constant([cate_mask], dtype=tf.float32)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_data:\n",
    "        conti, forecast_tensor = batch\n",
    "\n",
    "        # # Create cate_tensor based on your criteria\n",
    "        # treated_unit_index = 0\n",
    "        # future_tensor = tf.zeros_like(forecast_tensor)\n",
    "        # future_tensor[:, :, treated_unit_index] = 1\n",
    "\n",
    "        # # Concatenate conti, cate_tensor, and future_tensor along the last dimension\n",
    "        # input_tensor = tf.concat([conti, cate_tensor, future_tensor], axis=-1)\n",
    "        # Create cate_tensor based on your criteria\n",
    "        treated_unit_index = 0\n",
    "        future_tensor = tf.zeros_like(forecast_tensor)\n",
    "        future_tensor = tf.tensor_scatter_nd_add(future_tensor, indices=[(0, 0, treated_unit_index)], updates=[1.0])\n",
    "\n",
    "        # Assuming cate_tensor has shape [1, 1, 63]\n",
    "        # Expand dimensions of cate_tensor to match conti and future_tensor\n",
    "        cate_tensor_expanded = tf.expand_dims(cate_tensor, axis=0)  # Assuming axis=0 is the batch dimension\n",
    "\n",
    "        # # Repeat the expanded cate_tensor to match the batch size of conti and future_tensor\n",
    "        cate_tensor_expanded = tf.repeat(cate_tensor_expanded, repeats=tf.shape(conti)[0], axis=0)\n",
    "\n",
    "        # # Now, cate_tensor_expanded has shape [5, 1, 62], matching the other tensors\n",
    "        # # Concatenate conti, cate_tensor_expanded, and future_tensor along the last dimension\n",
    "        # input_tensor = tf.concat([conti, cate_tensor_expanded, future_tensor], axis=-1)\n",
    "        # # Now, cate_tensor_expanded has shape [1, 1, 63], matching the other tensors\n",
    "        # Concatenate conti, cate_tensor_expanded, and future_tensor along the last dimension\n",
    "        # input_tensor = tf.concat([conti, cate_tensor, future_tensor], axis=-1)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            print('here')\n",
    "            output = mqrnn_model.call(conti, cate_tensor_expanded, future_tensor)\n",
    "\n",
    "            # Reshape the target tensor to match the output\n",
    "            target = forecast_tensor\n",
    "\n",
    "            loss = criterion(target, output)\n",
    "\n",
    "        gradients = tape.gradient(loss, mqrnn_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, mqrnn_model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_MapDataset element_spec=(TensorSpec(shape=(None, 15, 62), dtype=tf.float32, name=None), TensorSpec(shape=(None, 7, 1), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'sequential_7' (type Sequential).\n\nAll layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.\n\nCall arguments received by layer 'sequential_7' (type Sequential):\n  â€¢ inputs=tf.Tensor(shape=(21, 16, 62), dtype=float32)\n  â€¢ training=None\n  â€¢ mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m hidden, cell\n\u001b[1;32m     17\u001b[0m encoder \u001b[38;5;241m=\u001b[39m Encoder(d_model, n_layers, dr)\n\u001b[0;32m---> 18\u001b[0m hidden, _ \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconti\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcate_tensor_expanded\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:61\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror_handler\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdebugging\u001b[38;5;241m.\u001b[39mis_traceback_filtering_enabled():\n\u001b[0;32m---> 61\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/base_layer.py:1149\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1148\u001b[0m ):\n\u001b[0;32m-> 1149\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:101\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    102\u001b[0m     signature \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(fn)\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;66;03m# The first argument is `self`, so filter it out\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:97\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[39], line 13\u001b[0m, in \u001b[0;36mEncoder.call\u001b[0;34m(self, conti, cate)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, conti, cate):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Use cate in the encoder\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat([conti, cate], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m     _, hidden, cell \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# _, hidden, cell = self.lstm(inputs)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden, cell\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:61\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror_handler\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdebugging\u001b[38;5;241m.\u001b[39mis_traceback_filtering_enabled():\n\u001b[0;32m---> 61\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/training.py:590\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39mcopied_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcopied_kwargs)\n\u001b[1;32m    588\u001b[0m     layout_map_lib\u001b[38;5;241m.\u001b[39m_map_subclass_model_variable(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layout_map)\n\u001b[0;32m--> 590\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:61\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror_handler\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdebugging\u001b[38;5;241m.\u001b[39mis_traceback_filtering_enabled():\n\u001b[0;32m---> 61\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/base_layer.py:1149\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1148\u001b[0m ):\n\u001b[0;32m-> 1149\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:156\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m e\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\u001b[38;5;241m.\u001b[39mwith_traceback(e\u001b[38;5;241m.\u001b[39m__traceback__) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m signature\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:97\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/sequential.py:391\u001b[0m, in \u001b[0;36mSequential.call\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_input_shape \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m    388\u001b[0m             _get_shape_tuple, inputs\n\u001b[1;32m    389\u001b[0m         )\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 391\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_graph_network_for_inferred_shape\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_initialized:\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/trackable/base.py:204\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/sequential.py:344\u001b[0m, in \u001b[0;36mSequential._build_graph_network_for_inferred_shape\u001b[0;34m(self, input_shape, input_dtype)\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(layer_output)) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(SINGLE_LAYER_OUTPUT_ERROR_MSG)\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# Keep track of nodes just created above\u001b[39;00m\n\u001b[1;32m    346\u001b[0m track_nodes_created_by_last_call(layer, created_nodes)\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'sequential_7' (type Sequential).\n\nAll layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.\n\nCall arguments received by layer 'sequential_7' (type Sequential):\n  â€¢ inputs=tf.Tensor(shape=(21, 16, 62), dtype=float32)\n  â€¢ training=None\n  â€¢ mask=None"
     ]
    }
   ],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_layers, dr):\n",
    "        super(Encoder, self).__init__()\n",
    "        # Define your model\n",
    "        self.lstm = tf.keras.Sequential()\n",
    "        for _ in range(n_layers):\n",
    "            self.lstm.add(tf.keras.layers.LSTM(units=d_model, return_state=True, dropout=dr))\n",
    "\n",
    "    def call(self, conti, cate):\n",
    "        # Use cate in the encoder\n",
    "        x = tf.concat([conti, cate], axis=1)\n",
    "\n",
    "        _, hidden, cell = self.lstm(x)\n",
    "        # _, hidden, cell = self.lstm(inputs)\n",
    "        return hidden, cell\n",
    "\n",
    "encoder = Encoder(d_model, n_layers, dr)\n",
    "hidden, _ = encoder(conti, cate_tensor_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = tf.keras.Sequential()\n",
    "for _ in range(2):\n",
    "    lstm.add(tf.keras.layers.LSTM(units=d_model, return_state=True, dropout=dr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(21, 16, 62), dtype=float32, numpy=\n",
       "array([[[ 4.0187246e-01, -6.7755383e-01, -8.6514997e-01, ...,\n",
       "         -5.4761511e-01,  1.4965768e-01, -5.9584123e-01],\n",
       "        [-1.0057775e+00, -1.0074930e+00, -1.8561399e+00, ...,\n",
       "         -1.4372560e+00, -2.3645914e+00, -2.1335799e-01],\n",
       "        [-1.5066302e-01, -1.2549475e+00, -1.6083925e+00, ...,\n",
       "         -1.0447674e+00,  2.2149336e-01,  8.7945122e-01],\n",
       "        ...,\n",
       "        [-7.9528773e-01, -1.0016013e-01,  1.2583999e-01, ...,\n",
       "         -8.0927420e-01, -1.4307274e+00,  8.2481074e-01],\n",
       "        [ 4.5458884e+00,  2.2094147e+00,  7.0391750e-01, ...,\n",
       "          2.6446257e+00,  2.5202353e+00,  2.4093840e+00],\n",
       "        [ 1.0000000e+00,  1.0000000e+00,  0.0000000e+00, ...,\n",
       "          1.0000000e+00,  1.0000000e+00,  1.0000000e+00]],\n",
       "\n",
       "       [[-2.7817843e+00, -2.4097350e+00, -1.9387225e+00, ...,\n",
       "         -2.7978833e+00, -2.2209198e+00, -2.5082574e+00],\n",
       "        [ 2.0359866e-02,  3.1226394e-01,  3.7358749e-01, ...,\n",
       "         -1.0279464e-01,  5.0883609e-01, -8.1440306e-01],\n",
       "        [-1.6381863e-01, -1.7498564e+00, -5.3481996e-01, ...,\n",
       "         -1.0970992e+00,  9.7576803e-01, -1.7979313e+00],\n",
       "        ...,\n",
       "        [ 4.0187246e-01, -6.7755383e-01, -8.6514997e-01, ...,\n",
       "         -5.4761511e-01,  1.4965768e-01, -5.9584123e-01],\n",
       "        [-1.0057775e+00, -1.0074930e+00, -1.8561399e+00, ...,\n",
       "         -1.4372560e+00, -2.3645914e+00, -2.1335799e-01],\n",
       "        [ 1.0000000e+00,  1.0000000e+00,  0.0000000e+00, ...,\n",
       "          1.0000000e+00,  1.0000000e+00,  1.0000000e+00]],\n",
       "\n",
       "       [[ 2.0359866e-02,  3.1226394e-01,  3.7358749e-01, ...,\n",
       "         -1.0279464e-01,  5.0883609e-01, -8.1440306e-01],\n",
       "        [-1.6381863e-01, -1.7498564e+00, -5.3481996e-01, ...,\n",
       "         -1.0970992e+00,  9.7576803e-01, -1.7979313e+00],\n",
       "        [-7.5582093e-01, -1.2549475e+00, -6.1740249e-01, ...,\n",
       "         -1.1232650e+00, -1.0176722e-01, -1.3608077e+00],\n",
       "        ...,\n",
       "        [-1.0057775e+00, -1.0074930e+00, -1.8561399e+00, ...,\n",
       "         -1.4372560e+00, -2.3645914e+00, -2.1335799e-01],\n",
       "        [-1.5066302e-01, -1.2549475e+00, -1.6083925e+00, ...,\n",
       "         -1.0447674e+00,  2.2149336e-01,  8.7945122e-01],\n",
       "        [ 1.0000000e+00,  1.0000000e+00,  0.0000000e+00, ...,\n",
       "          1.0000000e+00,  1.0000000e+00,  1.0000000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-8.4791017e-01, -5.9506899e-01,  1.6949074e+00, ...,\n",
       "         -5.9994692e-01,  3.6516473e-01, -9.7832441e-01],\n",
       "        [-6.7688727e-01,  2.2977912e-01, -3.9324999e-02, ...,\n",
       "          3.1585988e-01,  9.3985021e-01, -8.6904353e-01],\n",
       "        [-5.8573771e-02, -3.4761456e-01, -9.4773245e-01, ...,\n",
       "          5.4200809e-02, -6.7645270e-01, -7.5976259e-01],\n",
       "        ...,\n",
       "        [ 2.7031639e-01,  6.4809494e-02,  2.0842250e-01, ...,\n",
       "          7.0834851e-01, -2.8135642e-01,  2.7840614e-01],\n",
       "        [-9.8040588e-02, -1.2549475e+00, -6.9998497e-01, ...,\n",
       "         -2.3362419e-01, -1.1074668e+00, -1.0407706e-01],\n",
       "        [ 1.0000000e+00,  1.0000000e+00,  0.0000000e+00, ...,\n",
       "          1.0000000e+00,  1.0000000e+00,  1.0000000e+00]],\n",
       "\n",
       "       [[-6.5057606e-01, -5.1258421e-01,  6.2133497e-01, ...,\n",
       "          2.8969398e-01,  2.0173855e+00, -1.0407706e-01],\n",
       "        [ 9.9293508e-02,  8.0717283e-01, -1.1954800e+00, ...,\n",
       "         -4.1678554e-01,  3.2924688e-01,  1.6912523e-01],\n",
       "        [ 4.0187246e-01, -6.7755383e-01, -8.6514997e-01, ...,\n",
       "         -5.4761511e-01,  1.4965768e-01, -5.9584123e-01],\n",
       "        ...,\n",
       "        [ 1.1254308e+00, -1.7675318e-02,  8.6908245e-01, ...,\n",
       "          2.8969398e-01,  8.6801451e-01,  2.7840614e-01],\n",
       "        [ 4.5449486e-01,  1.2195969e+00,  9.5166498e-01, ...,\n",
       "          1.3886621e+00, -2.9931536e-02,  1.5351367e+00],\n",
       "        [ 1.0000000e+00,  1.0000000e+00,  0.0000000e+00, ...,\n",
       "          1.0000000e+00,  1.0000000e+00,  1.0000000e+00]],\n",
       "\n",
       "       [[-1.5066302e-01, -1.2549475e+00, -1.6083925e+00, ...,\n",
       "         -1.0447674e+00,  2.2149336e-01,  8.7945122e-01],\n",
       "        [ 3.4925002e-01,  1.4729431e-01, -5.3481996e-01, ...,\n",
       "         -1.0970992e+00, -1.7539879e+00, -9.7832441e-01],\n",
       "        [-8.4884986e-02,  1.3845665e+00,  4.5616999e-01, ...,\n",
       "         -7.0461053e-01,  4.0108258e-01, -1.0407706e-01],\n",
       "        ...,\n",
       "        [-5.7164240e-01,  2.2977912e-01,  4.3257497e-02, ...,\n",
       "          1.8689935e-03, -1.5025631e+00, -1.5871753e-01],\n",
       "        [ 6.6498458e-01,  7.2468799e-01, -4.5223749e-01, ...,\n",
       "          1.3363303e+00,  2.2149336e-01, -4.9436606e-02],\n",
       "        [ 1.0000000e+00,  1.0000000e+00,  0.0000000e+00, ...,\n",
       "          1.0000000e+00,  1.0000000e+00,  1.0000000e+00]]], dtype=float32)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.concat([conti, cate_tensor_expanded], axis=1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(15, 62), dtype=float32, numpy=\n",
       "array([[-7.29743987e-02, -1.92274880e-02, -8.56410787e-02,\n",
       "        -1.55965686e-01,  5.65690920e-02, -1.55455852e-02,\n",
       "         4.61784936e-02,  2.18769088e-01,  5.16066067e-02,\n",
       "         9.83765870e-02, -6.47029206e-02,  1.89189047e-01,\n",
       "         2.85775393e-01, -4.04633015e-01,  2.85761565e-01,\n",
       "        -6.93058372e-02,  3.49319214e-03,  1.46318585e-01,\n",
       "         2.90825278e-01,  3.67854446e-01,  3.28221411e-01,\n",
       "        -3.01147729e-01, -3.24748993e-01, -3.29498649e-01,\n",
       "         2.05114797e-01,  1.12396898e-02, -3.97291966e-02,\n",
       "         2.17170507e-01, -4.28277962e-02, -3.20963413e-01,\n",
       "        -1.75980344e-01, -7.87312239e-02, -3.56796607e-02,\n",
       "        -2.12128937e-01, -7.44436681e-02, -4.83417481e-01,\n",
       "        -2.66829953e-02, -8.01192448e-02, -2.05800727e-01,\n",
       "        -5.37466943e-01, -1.10658482e-01, -1.17170000e-02,\n",
       "        -1.64912179e-01,  2.75728136e-01,  3.81623693e-02,\n",
       "         2.00527851e-02, -4.80817735e-01, -4.11651433e-01,\n",
       "        -5.36096543e-02,  3.54691386e-01,  1.06789127e-01,\n",
       "        -1.17335476e-01, -7.50427917e-02,  2.50962190e-02,\n",
       "         2.76944965e-01,  8.42928141e-03,  1.62547588e-01,\n",
       "        -1.65144876e-02,  9.06766504e-02, -1.78760618e-01,\n",
       "         4.23071124e-02,  3.56403291e-01],\n",
       "       [ 7.54711479e-02, -7.64873996e-02, -4.86787893e-02,\n",
       "        -1.62350312e-02, -1.49131700e-01,  7.61343911e-02,\n",
       "        -1.26957953e-01,  1.80275157e-01, -1.70046896e-01,\n",
       "         2.87353229e-02, -1.72905892e-01,  1.24162221e-02,\n",
       "         4.43260036e-02, -1.82994559e-01, -9.66460928e-02,\n",
       "        -1.56334564e-01, -6.35176077e-02, -5.92168467e-03,\n",
       "         3.29570770e-01, -3.43576930e-02,  6.50102273e-02,\n",
       "        -4.14843783e-02, -2.62450099e-01, -3.05245519e-01,\n",
       "         1.68068036e-01, -2.07506008e-02,  1.57728478e-01,\n",
       "         1.42394716e-03, -1.89566597e-01, -1.30133450e-01,\n",
       "        -1.15975678e-01,  1.48410471e-02,  2.89264381e-01,\n",
       "        -3.21721472e-02,  3.96072641e-02, -1.14553295e-01,\n",
       "        -1.21422261e-01, -1.06175341e-01, -3.86803448e-01,\n",
       "        -2.47116283e-01,  1.58504188e-01,  2.86703676e-01,\n",
       "         9.59673598e-02,  1.53602824e-01,  1.50452808e-01,\n",
       "        -4.26562764e-02, -3.16353083e-01,  9.19795632e-02,\n",
       "        -2.85421107e-02,  4.97859046e-02, -8.51420686e-03,\n",
       "         1.42043844e-01,  1.41344830e-01, -7.21655712e-02,\n",
       "         3.61506678e-02,  6.04988486e-02, -1.80140719e-01,\n",
       "        -1.35037154e-01,  1.25672325e-01,  3.85364592e-02,\n",
       "        -1.49788395e-01,  2.17575401e-01],\n",
       "       [-7.58105442e-02, -1.10911228e-01, -1.06424570e-01,\n",
       "        -6.40355125e-02, -1.77230742e-02,  1.03771627e-01,\n",
       "        -7.93583617e-02,  7.57445395e-02, -1.51481375e-01,\n",
       "         3.80583890e-02,  2.73075327e-02,  6.73035681e-02,\n",
       "        -9.22603358e-04, -2.06575677e-01, -7.52585679e-02,\n",
       "        -1.76973283e-01,  1.76624320e-02, -3.83046232e-02,\n",
       "         2.81965107e-01,  6.74512535e-02,  1.04310103e-01,\n",
       "        -9.43085253e-02, -2.85613507e-01, -2.69381613e-01,\n",
       "         2.15007678e-01, -3.29107642e-02,  1.24378875e-01,\n",
       "        -1.20602071e-01, -2.60072291e-01, -1.11033484e-01,\n",
       "        -5.96036017e-02,  1.57311074e-02,  1.80433974e-01,\n",
       "        -5.55820316e-02,  2.91295670e-04, -1.60921186e-01,\n",
       "        -8.49619061e-02,  2.47937310e-02, -3.79574120e-01,\n",
       "        -2.55082071e-01,  6.67457134e-02,  2.09639192e-01,\n",
       "        -2.61537246e-02,  1.11317642e-01,  2.41525341e-02,\n",
       "        -1.56969186e-02, -2.76947767e-01,  1.35245636e-01,\n",
       "         1.42618999e-01,  1.29012063e-01, -1.19237956e-02,\n",
       "         1.74678490e-02,  7.66717345e-02, -6.02652505e-02,\n",
       "         8.72334912e-02,  6.28722785e-03, -1.07291512e-01,\n",
       "         2.60099806e-02,  1.15039513e-01, -5.73541299e-02,\n",
       "         3.20364721e-02,  2.02186048e-01],\n",
       "       [-3.43859233e-02, -8.70084912e-02,  2.41261013e-02,\n",
       "        -1.12092994e-01, -5.32028712e-02, -2.32219417e-02,\n",
       "        -1.69067010e-01,  1.72384232e-01, -1.52421921e-01,\n",
       "        -3.28361839e-02, -2.17891172e-01, -4.63370234e-02,\n",
       "        -2.20527515e-01, -2.06208393e-01, -5.60748279e-02,\n",
       "        -1.00981809e-01, -8.30881298e-02,  9.86853540e-02,\n",
       "         2.47465789e-01, -1.05861174e-02,  2.00259879e-01,\n",
       "        -1.74058571e-01, -1.20969832e-01, -3.07220638e-01,\n",
       "         8.99440199e-02, -3.97854000e-02, -2.27479383e-01,\n",
       "         1.04081176e-01, -2.72790521e-01, -3.61775160e-01,\n",
       "        -1.44712359e-01,  1.80651188e-01,  1.51175439e-01,\n",
       "        -1.69517681e-01,  6.01658486e-02,  2.60545462e-02,\n",
       "        -1.36917830e-01, -6.99002147e-02, -3.62840235e-01,\n",
       "        -2.86682725e-01,  1.96188673e-01,  2.23869741e-01,\n",
       "         2.23727882e-01,  2.16848969e-01,  5.23135997e-03,\n",
       "         1.32377565e-01, -3.98047417e-01, -2.92413086e-01,\n",
       "         1.55441687e-01,  3.26738089e-01,  2.23221526e-01,\n",
       "        -1.30113959e-01,  8.67662132e-02, -1.34793818e-01,\n",
       "         1.41155526e-01, -1.27225637e-01, -4.79743257e-02,\n",
       "        -1.75528675e-01,  2.61068698e-02,  2.47475374e-02,\n",
       "         7.49498159e-02,  3.78285587e-01],\n",
       "       [ 9.52884033e-02, -1.44239306e-01,  1.19185142e-01,\n",
       "        -9.41567943e-02,  8.60818848e-02,  5.73212989e-02,\n",
       "        -1.89818174e-01,  2.28349105e-01, -1.28487498e-01,\n",
       "        -8.65074545e-02, -2.41175026e-01, -9.33908448e-02,\n",
       "        -1.95993558e-01, -1.84378788e-01,  1.56507909e-03,\n",
       "        -2.19481185e-01, -1.20438725e-01,  1.23523735e-01,\n",
       "         2.35414684e-01,  1.04772985e-01,  2.52510250e-01,\n",
       "        -2.16586769e-01, -1.11557648e-01, -1.22295313e-01,\n",
       "         2.79674064e-02, -6.61096796e-02, -2.19692141e-01,\n",
       "        -1.86056178e-02, -1.42180741e-01, -2.92539835e-01,\n",
       "        -1.01794757e-01,  2.51217335e-01,  1.84362531e-01,\n",
       "        -2.50211716e-01,  1.00748010e-01,  1.08747587e-01,\n",
       "        -1.65035754e-01, -1.07815802e-01, -3.45556021e-01,\n",
       "        -2.80616850e-01,  9.90742967e-02,  2.07950950e-01,\n",
       "         2.52126962e-01,  1.49017528e-01,  4.03122604e-02,\n",
       "         1.08355336e-01, -2.82799333e-01, -3.59705895e-01,\n",
       "         1.62113309e-01,  3.22584391e-01,  1.43413395e-01,\n",
       "        -7.06447363e-02,  1.47907957e-02, -1.78285986e-01,\n",
       "         3.34454770e-03, -1.59309059e-02, -3.78410183e-02,\n",
       "        -2.28782237e-01,  5.38457483e-02,  1.72717422e-02,\n",
       "        -4.33095619e-02,  3.28071356e-01],\n",
       "       [ 1.57937810e-01, -1.07378565e-01, -1.38195008e-01,\n",
       "        -9.04568583e-02, -9.88684073e-02,  6.43557608e-02,\n",
       "        -1.89483345e-01,  7.40203187e-02, -1.22379526e-01,\n",
       "        -4.94447909e-02, -2.55132169e-01,  3.74118984e-02,\n",
       "        -7.68518299e-02, -1.82766497e-01,  5.70495576e-02,\n",
       "        -1.60445049e-01, -4.98217791e-02,  1.97942890e-02,\n",
       "         2.04035655e-01,  9.30448845e-02,  4.09974121e-02,\n",
       "        -9.07545164e-02, -2.02410370e-01, -2.76141882e-01,\n",
       "         2.00238287e-01, -8.93604308e-02, -7.27156550e-02,\n",
       "        -3.62958163e-02, -2.84472376e-01, -1.94615111e-01,\n",
       "        -1.10110827e-01,  1.07589833e-01,  6.49133846e-02,\n",
       "        -1.24009266e-01,  1.52335754e-02, -1.69295028e-01,\n",
       "        -1.01217560e-01,  7.30937719e-02, -3.44825804e-01,\n",
       "        -2.12072283e-01,  1.56692997e-01,  2.50372767e-01,\n",
       "         3.10419321e-01,  1.85037673e-01, -1.12186968e-01,\n",
       "         1.82552755e-01, -2.09300771e-01, -6.67395294e-02,\n",
       "         2.29622304e-01,  2.49418721e-01,  1.60883572e-02,\n",
       "        -3.04805879e-02,  1.40657470e-01, -1.75920725e-01,\n",
       "         7.83145055e-02,  7.83206522e-02, -9.17923003e-02,\n",
       "        -5.12240678e-02,  1.41409814e-01,  2.22792719e-02,\n",
       "         9.11867770e-04,  2.88607538e-01],\n",
       "       [ 1.81327723e-02, -1.20935440e-01, -7.17296526e-02,\n",
       "        -1.51689783e-01, -9.02019590e-02, -9.56904516e-02,\n",
       "        -1.63867064e-02,  2.83249736e-01, -1.95223317e-02,\n",
       "         8.18418246e-03, -3.03372949e-01,  1.23908006e-01,\n",
       "         2.66353209e-02, -4.11201566e-01, -4.05626697e-03,\n",
       "        -2.73833990e-01, -4.42059617e-03, -1.51241021e-02,\n",
       "         2.87684470e-01,  2.82257169e-01,  3.76952291e-01,\n",
       "        -3.04731518e-01, -2.21908450e-01, -2.48711720e-01,\n",
       "         7.56042898e-02, -2.40227748e-02, -3.84723872e-01,\n",
       "         1.74372166e-01, -7.13348910e-02, -4.34597820e-01,\n",
       "        -1.45957068e-01,  1.83249086e-01,  2.54589051e-01,\n",
       "        -2.75524646e-01,  5.03101535e-02, -2.55700707e-01,\n",
       "        -1.65942058e-01,  1.33861288e-01, -3.41330647e-01,\n",
       "        -5.07184088e-01,  2.91283503e-02,  8.13465565e-02,\n",
       "         2.46360153e-02,  3.22157174e-01,  1.37317196e-01,\n",
       "        -1.14743570e-02, -4.82320935e-01, -4.86636877e-01,\n",
       "         9.07492042e-02,  3.01774025e-01,  8.64641368e-02,\n",
       "        -3.02140247e-02,  2.25066952e-02, -1.25766605e-01,\n",
       "         2.10747302e-01,  5.31904353e-03,  6.73002824e-02,\n",
       "        -1.46850556e-01,  7.82118812e-02, -7.93763921e-02,\n",
       "         3.14919055e-02,  3.86092693e-01],\n",
       "       [ 1.02873452e-01, -1.13469340e-01, -1.50063515e-01,\n",
       "        -1.12333089e-01,  1.33509208e-02,  1.43064871e-01,\n",
       "        -1.06897846e-01,  6.80141449e-02, -7.72671103e-02,\n",
       "         3.76942530e-02, -1.20986238e-01,  1.26776293e-01,\n",
       "         9.62452963e-02, -1.60104170e-01,  3.44964378e-02,\n",
       "        -1.21669598e-01,  2.76264246e-03,  1.09315127e-01,\n",
       "         2.79048622e-01,  9.60189328e-02,  1.07809506e-01,\n",
       "        -9.83618572e-02, -2.90792495e-01, -1.76574260e-01,\n",
       "         1.85457245e-01, -4.70387675e-02,  3.72837577e-03,\n",
       "        -1.43813744e-01, -2.35717908e-01, -2.28639662e-01,\n",
       "        -5.20677865e-02,  7.45856240e-02, -1.13747213e-02,\n",
       "        -1.68923140e-01,  1.68119743e-02, -2.63104111e-01,\n",
       "        -1.10833555e-01,  5.51388972e-02, -3.76764506e-01,\n",
       "        -2.85389394e-01,  1.45139068e-01,  1.92838013e-01,\n",
       "         1.28547683e-01,  1.71803966e-01,  3.80509719e-03,\n",
       "         6.29817415e-03, -2.93892384e-01,  2.39262164e-01,\n",
       "         1.09662607e-01,  1.95263639e-01, -9.34226662e-02,\n",
       "         3.30603383e-02,  1.03605829e-01, -1.03559017e-01,\n",
       "         5.85794747e-02,  1.53181911e-01,  2.82840505e-02,\n",
       "        -2.88214581e-03,  1.11149020e-01, -6.60180673e-02,\n",
       "         4.87151034e-02,  2.32515126e-01],\n",
       "       [-1.88478366e-01,  4.59941067e-02, -1.06732555e-01,\n",
       "        -8.12083855e-02, -1.20964661e-01,  5.49948737e-02,\n",
       "         2.54362941e-01,  3.00974607e-01, -2.12638788e-02,\n",
       "         8.81487876e-02, -1.81288421e-01,  1.31444514e-01,\n",
       "        -2.20910758e-01, -3.66415709e-01, -2.20203996e-01,\n",
       "        -1.40408918e-01, -3.20932195e-02,  1.36350274e-01,\n",
       "         2.66313076e-01,  1.91396490e-01,  2.71409303e-01,\n",
       "        -3.21765900e-01, -3.23606730e-01, -2.11903960e-01,\n",
       "         3.30841571e-01,  3.07664997e-03,  1.11385718e-01,\n",
       "         2.72843957e-01, -1.36691213e-01, -1.15995198e-01,\n",
       "        -1.67470217e-01,  2.26642713e-02,  2.32026309e-01,\n",
       "        -2.79565454e-01,  3.75644229e-02, -4.93551105e-01,\n",
       "         2.96396241e-02, -2.53895491e-01, -3.37104291e-01,\n",
       "        -4.44201976e-01, -2.74657961e-02, -2.60794759e-01,\n",
       "        -1.75415710e-01,  3.48755747e-01,  1.48433059e-01,\n",
       "        -6.54386804e-02, -4.43867505e-01, -3.64555955e-01,\n",
       "        -1.59177214e-01,  2.39493415e-01,  1.11513086e-01,\n",
       "        -1.55640900e-01, -5.71129099e-03, -6.67872503e-02,\n",
       "         9.10990611e-02, -9.94052589e-02,  1.21313132e-01,\n",
       "         2.54858974e-02, -1.72152668e-02, -2.29208931e-01,\n",
       "         2.82806177e-02, -8.57879445e-02],\n",
       "       [ 2.97272131e-02, -1.34125933e-01,  1.14420205e-01,\n",
       "        -1.23587988e-01,  4.57916185e-02,  2.23349072e-02,\n",
       "        -1.30152896e-01,  2.92423189e-01,  9.67591703e-02,\n",
       "         1.60607521e-03, -2.65713453e-01, -1.97574100e-05,\n",
       "         4.72540408e-02, -2.82254577e-01, -9.26838908e-03,\n",
       "        -2.49484241e-01, -1.10937573e-01,  5.30735962e-02,\n",
       "         2.26291224e-01,  2.50617474e-01,  3.24138612e-01,\n",
       "        -2.69165277e-01, -1.35911867e-01, -2.01075658e-01,\n",
       "        -1.01675428e-02, -1.50614241e-02, -3.37828785e-01,\n",
       "         1.41051143e-01, -1.30427971e-01, -3.68614405e-01,\n",
       "        -1.32368743e-01,  2.66995370e-01,  9.96352583e-02,\n",
       "        -2.66886085e-01,  3.18320654e-02, -1.23841330e-01,\n",
       "        -2.09364742e-01,  5.59523217e-02, -3.22568804e-01,\n",
       "        -4.57386434e-01, -1.78933907e-02,  1.33800536e-01,\n",
       "         8.03751522e-04,  2.98693717e-01,  5.36216535e-02,\n",
       "         3.04143969e-02, -3.11725616e-01, -4.21124130e-01,\n",
       "         1.07041657e-01,  3.84630710e-01,  1.15725808e-01,\n",
       "        -7.07961768e-02, -6.40932918e-02, -1.33506790e-01,\n",
       "         9.94107723e-02, -5.85071556e-02, -3.77280973e-02,\n",
       "        -2.20195591e-01,  8.43392760e-02,  1.02358665e-02,\n",
       "        -1.42375929e-02,  3.91661584e-01],\n",
       "       [ 6.58412129e-02, -1.04094237e-01, -1.21351793e-01,\n",
       "        -1.25689745e-01,  3.47382054e-02, -6.01972863e-02,\n",
       "        -1.67539679e-02,  1.32827923e-01, -6.06889278e-02,\n",
       "        -8.99349898e-02, -1.94251370e-02,  1.98848337e-01,\n",
       "         4.00744602e-02, -3.73931497e-01,  6.56672046e-02,\n",
       "        -1.46885201e-01,  6.36479855e-02,  7.99988955e-02,\n",
       "         2.00853661e-01,  1.85646266e-01,  3.14431816e-01,\n",
       "        -2.44172618e-01, -1.55464172e-01, -1.57625332e-01,\n",
       "         1.07876644e-01, -2.26144139e-02, -3.09993744e-01,\n",
       "         1.27054274e-01, -5.58799133e-02, -4.25404757e-01,\n",
       "        -1.18553266e-01,  1.49561062e-01,  1.05028436e-01,\n",
       "        -2.06375897e-01, -2.49664132e-02, -2.29804397e-01,\n",
       "        -1.44244656e-01,  8.97451714e-02, -2.44467452e-01,\n",
       "        -3.80186170e-01, -3.07856174e-03, -1.17022665e-02,\n",
       "        -1.33848071e-01,  2.46568277e-01,  7.45192450e-03,\n",
       "        -6.32931739e-02, -4.60422635e-01, -3.15833092e-01,\n",
       "        -6.12270646e-03,  2.58010924e-01,  1.68908108e-02,\n",
       "        -2.63566058e-02, -1.60990059e-02, -9.47834700e-02,\n",
       "         1.29537150e-01,  8.45429078e-02,  7.41080567e-02,\n",
       "        -5.84136248e-02,  7.79627115e-02, -1.44100681e-01,\n",
       "         1.35628805e-01,  1.44546032e-01],\n",
       "       [-1.16917498e-01,  6.96686050e-03, -7.78832212e-02,\n",
       "        -8.87806863e-02, -1.13988161e-01,  9.45289508e-02,\n",
       "         2.20644563e-01,  2.59570837e-01, -1.32947311e-01,\n",
       "         9.74171236e-02, -2.39193082e-01,  1.94038942e-01,\n",
       "        -7.49729797e-02, -2.74094105e-01,  3.33030650e-04,\n",
       "        -8.73242244e-02, -4.50466760e-02, -1.23389121e-02,\n",
       "         2.85781980e-01,  2.30977967e-01,  2.49458343e-01,\n",
       "        -3.42538387e-01, -3.56316239e-01, -4.64823022e-02,\n",
       "         1.88270733e-01, -5.64242760e-03, -1.59324095e-01,\n",
       "         1.21916361e-01,  1.81856193e-02, -2.46232986e-01,\n",
       "        -7.09760636e-02,  8.40320960e-02,  2.08142295e-01,\n",
       "        -2.72042036e-01,  9.47500952e-03, -4.74665850e-01,\n",
       "         1.16078369e-01, -1.51592404e-01, -1.22498415e-01,\n",
       "        -3.65460217e-01, -1.83077589e-01, -2.18619287e-01,\n",
       "        -2.40715593e-02,  3.38355511e-01,  1.22614771e-01,\n",
       "        -3.58406156e-02, -2.83320338e-01, -2.74434388e-01,\n",
       "        -2.53036171e-01,  2.95266241e-01,  4.83167954e-02,\n",
       "        -5.88380769e-02,  1.39453998e-02, -1.16054587e-01,\n",
       "         2.49057449e-02, -1.90535747e-02,  2.98690230e-01,\n",
       "        -3.96072417e-02, -7.90171847e-02, -2.90654898e-01,\n",
       "        -1.63989570e-02, -1.24799483e-01],\n",
       "       [-9.35901925e-02,  5.84854968e-02, -8.93929601e-02,\n",
       "        -1.39520079e-01, -8.44408497e-02,  7.40242898e-02,\n",
       "         2.48547062e-01,  3.09992254e-01, -8.71246755e-02,\n",
       "         2.47877181e-01, -2.19675854e-01,  1.34589538e-01,\n",
       "         4.30398472e-02, -1.19910173e-01,  7.01902658e-02,\n",
       "        -1.25826389e-01,  2.51817312e-02,  1.19987868e-01,\n",
       "         1.84927329e-01,  2.72105098e-01,  3.03134263e-01,\n",
       "        -3.27035695e-01, -2.49822959e-01,  1.27048483e-02,\n",
       "         1.67247489e-01,  4.45005596e-02, -8.74428302e-02,\n",
       "         7.33471438e-02,  6.03400506e-02, -1.67723566e-01,\n",
       "        -6.77069351e-02,  4.83119972e-02,  3.58130425e-01,\n",
       "        -2.13858694e-01,  3.93753238e-02, -4.19935346e-01,\n",
       "         4.09471011e-03, -1.01055317e-01, -7.21803010e-02,\n",
       "        -2.54135698e-01, -3.28867197e-01, -2.15777397e-01,\n",
       "         1.61169142e-01,  3.51508051e-01,  1.03302479e-01,\n",
       "        -2.33695973e-02, -5.32923877e-01, -3.87302667e-01,\n",
       "        -1.84168711e-01,  3.40231568e-01,  8.41731653e-02,\n",
       "        -7.11521208e-02, -3.51348445e-02, -2.95515135e-02,\n",
       "         2.18952268e-01, -4.92892787e-02,  2.24545568e-01,\n",
       "        -1.56983957e-01, -9.11104009e-02, -2.63485879e-01,\n",
       "         9.25647691e-02,  2.12239817e-01],\n",
       "       [-7.01158792e-02, -2.59190015e-02, -5.33095300e-02,\n",
       "        -1.75255567e-01, -1.17111050e-01,  2.17807945e-02,\n",
       "         6.54812604e-02,  1.80739239e-01,  1.18346229e-01,\n",
       "         9.73577723e-02, -4.00073640e-02,  5.39754294e-02,\n",
       "         2.87548155e-01, -2.75960058e-01,  2.08693832e-01,\n",
       "        -4.02738899e-02,  2.80174538e-02,  1.48383379e-01,\n",
       "         1.91051036e-01,  3.31087232e-01,  2.80925035e-01,\n",
       "        -3.42878342e-01, -1.18943274e-01, -1.71858832e-01,\n",
       "         9.03794393e-02,  3.54913883e-02, -1.43540308e-01,\n",
       "         9.60562006e-02, -5.70866950e-02, -2.56449789e-01,\n",
       "        -5.65393753e-02, -1.05575211e-01, -1.03416508e-02,\n",
       "        -1.56336620e-01, -4.88988347e-02, -4.73843932e-01,\n",
       "        -5.72585966e-03, -1.18970253e-01, -3.18164378e-01,\n",
       "        -5.17049432e-01, -4.03913260e-01,  6.53015450e-02,\n",
       "        -1.35252371e-01,  2.56660730e-01,  8.33183974e-02,\n",
       "         9.70398113e-02, -4.87862110e-01, -2.40393192e-01,\n",
       "         9.22377035e-02,  3.45714271e-01,  1.22234151e-01,\n",
       "        -7.11056441e-02, -5.88583760e-02, -5.20274714e-02,\n",
       "         3.09836477e-01,  1.61709432e-02,  1.64199516e-01,\n",
       "        -1.23741962e-01, -3.04087345e-02, -1.81519777e-01,\n",
       "         1.03877205e-02,  8.21713656e-02],\n",
       "       [ 4.83225547e-02, -8.55284929e-03, -9.94266421e-02,\n",
       "        -1.25151187e-01,  9.39988811e-03,  3.46051455e-02,\n",
       "         3.24963816e-02,  1.23305969e-01, -5.48772812e-02,\n",
       "         4.42276374e-02,  4.63231374e-03,  1.67621076e-01,\n",
       "         3.34884435e-01, -3.61083895e-01,  1.81252882e-01,\n",
       "        -6.27296567e-02,  8.70351866e-02,  9.59727541e-02,\n",
       "         2.50107110e-01,  2.97910094e-01,  2.52399445e-01,\n",
       "        -2.97472030e-01, -2.74789661e-01, -2.01112404e-01,\n",
       "         7.19319880e-02,  3.28988098e-02,  6.39527366e-02,\n",
       "         1.57380566e-01, -1.96526702e-02, -2.74713904e-01,\n",
       "        -1.38821006e-01, -9.24643800e-02,  7.99807087e-02,\n",
       "        -1.64393291e-01, -2.76516061e-02, -4.02179897e-01,\n",
       "        -1.00200912e-02, -6.34507239e-02, -2.63492227e-01,\n",
       "        -4.89613533e-01, -3.29477876e-01,  5.89277335e-02,\n",
       "         1.99678261e-03,  2.41107717e-01,  8.00694525e-03,\n",
       "         9.60023105e-02, -4.14432436e-01, -2.71402329e-01,\n",
       "        -4.12282236e-02,  2.99767315e-01,  1.00992657e-01,\n",
       "        -8.88272561e-03, -2.46580038e-02, -3.33910696e-02,\n",
       "         2.84053743e-01,  1.05172038e-01,  1.90823719e-01,\n",
       "        -1.10347800e-01,  5.05382903e-02, -2.39280909e-01,\n",
       "         7.11950809e-02,  2.63301164e-01]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 7, 1, 1), dtype=float32, numpy=\n",
       "array([[[[1.]],\n",
       "\n",
       "        [[0.]],\n",
       "\n",
       "        [[0.]],\n",
       "\n",
       "        [[0.]],\n",
       "\n",
       "        [[0.]],\n",
       "\n",
       "        [[0.]],\n",
       "\n",
       "        [[0.]]],\n",
       "\n",
       "\n",
       "       [[[0.]],\n",
       "\n",
       "        [[0.]],\n",
       "\n",
       "        [[0.]],\n",
       "\n",
       "        [[0.]],\n",
       "\n",
       "        [[0.]],\n",
       "\n",
       "        [[0.]],\n",
       "\n",
       "        [[0.]]],\n",
       "\n",
       "\n",
       "       [[[0.]],\n",
       "\n",
       "        [[0.]],\n",
       "\n",
       "        [[0.]],\n",
       "\n",
       "        [[0.]],\n",
       "\n",
       "        [[0.]],\n",
       "\n",
       "        [[0.]],\n",
       "\n",
       "        [[0.]]],\n",
       "\n",
       "\n",
       "       [[[0.]],\n",
       "\n",
       "        [[0.]],\n",
       "\n",
       "        [[0.]],\n",
       "\n",
       "        [[0.]],\n",
       "\n",
       "        [[0.]],\n",
       "\n",
       "        [[0.]],\n",
       "\n",
       "        [[0.]]],\n",
       "\n",
       "\n",
       "       [[[0.]],\n",
       "\n",
       "        [[0.]],\n",
       "\n",
       "        [[0.]],\n",
       "\n",
       "        [[0.]],\n",
       "\n",
       "        [[0.]],\n",
       "\n",
       "        [[0.]],\n",
       "\n",
       "        [[0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.expand_dims(future_tensor, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 62, 1), dtype=float32, numpy=\n",
       "array([[[-0.00316266],\n",
       "        [-0.00064736],\n",
       "        [-0.19473381],\n",
       "        [ 0.03555915],\n",
       "        [-0.02805869],\n",
       "        [-0.09891547],\n",
       "        [ 0.03250901],\n",
       "        [-0.1499267 ],\n",
       "        [ 0.03983374],\n",
       "        [-0.1209138 ],\n",
       "        [ 0.09328417],\n",
       "        [-0.05761017],\n",
       "        [-0.40352422],\n",
       "        [-0.04340634],\n",
       "        [ 0.31057784],\n",
       "        [ 0.06992783],\n",
       "        [-0.18012914],\n",
       "        [-0.09889585],\n",
       "        [ 0.13981047],\n",
       "        [ 0.04054002],\n",
       "        [ 0.04222865],\n",
       "        [-0.11550292],\n",
       "        [-0.06337526],\n",
       "        [ 0.11566366],\n",
       "        [-0.21819176],\n",
       "        [-0.05861421],\n",
       "        [ 0.18389235],\n",
       "        [ 0.2039906 ],\n",
       "        [ 0.19449301],\n",
       "        [ 0.19363399],\n",
       "        [ 0.09303418],\n",
       "        [ 0.25356674],\n",
       "        [ 0.03670028],\n",
       "        [ 0.06142342],\n",
       "        [ 0.21268721],\n",
       "        [ 0.04982827],\n",
       "        [-0.08481552],\n",
       "        [-0.12171865],\n",
       "        [-0.18368185],\n",
       "        [ 0.06950779],\n",
       "        [ 0.10259786],\n",
       "        [-0.11437722],\n",
       "        [-0.1502667 ],\n",
       "        [ 0.09697984],\n",
       "        [-0.02043213],\n",
       "        [ 0.07262134],\n",
       "        [ 0.1108633 ],\n",
       "        [ 0.17208779],\n",
       "        [ 0.23454897],\n",
       "        [ 0.17105398],\n",
       "        [ 0.09787981],\n",
       "        [-0.05222023],\n",
       "        [-0.16468991],\n",
       "        [-0.30843833],\n",
       "        [ 0.06614448],\n",
       "        [ 0.10395995],\n",
       "        [ 0.3097714 ],\n",
       "        [ 0.15052372],\n",
       "        [-0.00861667],\n",
       "        [ 0.1633726 ],\n",
       "        [-0.01047937],\n",
       "        [ 0.0059868 ]],\n",
       "\n",
       "       [[ 0.0289396 ],\n",
       "        [-0.00990225],\n",
       "        [-0.1556219 ],\n",
       "        [ 0.03445876],\n",
       "        [-0.15240964],\n",
       "        [-0.09723881],\n",
       "        [ 0.02261517],\n",
       "        [-0.03662501],\n",
       "        [ 0.00235764],\n",
       "        [-0.08879872],\n",
       "        [ 0.0470163 ],\n",
       "        [-0.05271237],\n",
       "        [-0.28392637],\n",
       "        [-0.16442601],\n",
       "        [ 0.30010515],\n",
       "        [ 0.01694503],\n",
       "        [-0.1507926 ],\n",
       "        [ 0.01221128],\n",
       "        [ 0.28540248],\n",
       "        [ 0.0853649 ],\n",
       "        [ 0.02987948],\n",
       "        [-0.1355397 ],\n",
       "        [-0.11864881],\n",
       "        [ 0.12002031],\n",
       "        [-0.16562097],\n",
       "        [-0.02798449],\n",
       "        [ 0.20239882],\n",
       "        [ 0.20056136],\n",
       "        [ 0.12522687],\n",
       "        [ 0.17783119],\n",
       "        [ 0.17640653],\n",
       "        [ 0.20615608],\n",
       "        [ 0.02356426],\n",
       "        [ 0.04741582],\n",
       "        [ 0.16339993],\n",
       "        [ 0.08993468],\n",
       "        [-0.02393302],\n",
       "        [-0.07941639],\n",
       "        [-0.00727098],\n",
       "        [ 0.0513688 ],\n",
       "        [-0.03008323],\n",
       "        [ 0.05163593],\n",
       "        [-0.09467873],\n",
       "        [ 0.02709782],\n",
       "        [ 0.0406679 ],\n",
       "        [ 0.16865337],\n",
       "        [ 0.13686077],\n",
       "        [ 0.2350451 ],\n",
       "        [ 0.19152398],\n",
       "        [ 0.09874503],\n",
       "        [ 0.21749014],\n",
       "        [-0.02194661],\n",
       "        [-0.05868797],\n",
       "        [-0.1148537 ],\n",
       "        [ 0.00688514],\n",
       "        [ 0.09259195],\n",
       "        [ 0.28933832],\n",
       "        [ 0.11190559],\n",
       "        [ 0.12059475],\n",
       "        [ 0.06308703],\n",
       "        [ 0.01898031],\n",
       "        [ 0.02699102]],\n",
       "\n",
       "       [[ 0.00455462],\n",
       "        [ 0.04242299],\n",
       "        [ 0.0037244 ],\n",
       "        [ 0.06006996],\n",
       "        [-0.11869352],\n",
       "        [ 0.03117332],\n",
       "        [ 0.1066066 ],\n",
       "        [ 0.13560708],\n",
       "        [ 0.0659202 ],\n",
       "        [-0.27390203],\n",
       "        [-0.17756526],\n",
       "        [-0.31870294],\n",
       "        [-0.5088969 ],\n",
       "        [-0.0803673 ],\n",
       "        [ 0.27988362],\n",
       "        [ 0.13981694],\n",
       "        [-0.1458523 ],\n",
       "        [-0.01217494],\n",
       "        [ 0.16204011],\n",
       "        [ 0.12354858],\n",
       "        [ 0.06641321],\n",
       "        [ 0.05784118],\n",
       "        [ 0.10984817],\n",
       "        [ 0.07411798],\n",
       "        [-0.15168673],\n",
       "        [ 0.01265387],\n",
       "        [ 0.2558215 ],\n",
       "        [-0.1250118 ],\n",
       "        [ 0.04498831],\n",
       "        [ 0.22066016],\n",
       "        [ 0.30888397],\n",
       "        [ 0.46309218],\n",
       "        [ 0.22856183],\n",
       "        [ 0.02072213],\n",
       "        [ 0.04058695],\n",
       "        [-0.20365323],\n",
       "        [-0.07736829],\n",
       "        [ 0.14054112],\n",
       "        [ 0.14763555],\n",
       "        [-0.19730154],\n",
       "        [-0.11740726],\n",
       "        [ 0.0558104 ],\n",
       "        [ 0.0558343 ],\n",
       "        [ 0.17945625],\n",
       "        [-0.09753098],\n",
       "        [ 0.09466686],\n",
       "        [ 0.03407938],\n",
       "        [ 0.14672713],\n",
       "        [ 0.23744088],\n",
       "        [ 0.00997414],\n",
       "        [-0.10305478],\n",
       "        [ 0.02926401],\n",
       "        [-0.34139025],\n",
       "        [-0.10424554],\n",
       "        [-0.02190825],\n",
       "        [-0.08762099],\n",
       "        [ 0.45609346],\n",
       "        [ 0.14970548],\n",
       "        [-0.10664557],\n",
       "        [-0.44355366],\n",
       "        [-0.22843173],\n",
       "        [-0.04396512]],\n",
       "\n",
       "       [[ 0.03459943],\n",
       "        [-0.05095249],\n",
       "        [-0.30686125],\n",
       "        [ 0.10946693],\n",
       "        [ 0.05125926],\n",
       "        [-0.17507245],\n",
       "        [ 0.02358243],\n",
       "        [-0.11638487],\n",
       "        [ 0.02731644],\n",
       "        [ 0.05422685],\n",
       "        [ 0.03990587],\n",
       "        [ 0.0413317 ],\n",
       "        [-0.10092065],\n",
       "        [-0.12430186],\n",
       "        [ 0.15306295],\n",
       "        [-0.09960113],\n",
       "        [-0.15966064],\n",
       "        [-0.07343797],\n",
       "        [ 0.08940557],\n",
       "        [ 0.03101785],\n",
       "        [ 0.01093725],\n",
       "        [-0.12888667],\n",
       "        [-0.2412018 ],\n",
       "        [ 0.10142936],\n",
       "        [-0.14980276],\n",
       "        [-0.09331899],\n",
       "        [ 0.2412378 ],\n",
       "        [ 0.20630226],\n",
       "        [ 0.18671449],\n",
       "        [ 0.07726872],\n",
       "        [ 0.06923714],\n",
       "        [-0.1249494 ],\n",
       "        [ 0.00394632],\n",
       "        [-0.03356306],\n",
       "        [ 0.20857774],\n",
       "        [ 0.17704245],\n",
       "        [-0.00582925],\n",
       "        [-0.08676518],\n",
       "        [-0.24034382],\n",
       "        [ 0.10639567],\n",
       "        [ 0.08460974],\n",
       "        [ 0.05279385],\n",
       "        [-0.09495618],\n",
       "        [ 0.11691646],\n",
       "        [-0.06292404],\n",
       "        [ 0.08977588],\n",
       "        [ 0.01195239],\n",
       "        [ 0.06831332],\n",
       "        [ 0.18314755],\n",
       "        [ 0.23788452],\n",
       "        [-0.09475413],\n",
       "        [-0.03303981],\n",
       "        [-0.11357199],\n",
       "        [-0.33664653],\n",
       "        [ 0.09130872],\n",
       "        [ 0.13118154],\n",
       "        [-0.05453951],\n",
       "        [ 0.22377631],\n",
       "        [ 0.05350903],\n",
       "        [ 0.1393247 ],\n",
       "        [ 0.01459609],\n",
       "        [-0.06274391]],\n",
       "\n",
       "       [[-0.01490838],\n",
       "        [-0.05645332],\n",
       "        [ 0.1448233 ],\n",
       "        [ 0.03108594],\n",
       "        [-0.0555402 ],\n",
       "        [ 0.06366435],\n",
       "        [-0.03973962],\n",
       "        [ 0.03057819],\n",
       "        [ 0.04553152],\n",
       "        [-0.06170498],\n",
       "        [-0.00617378],\n",
       "        [ 0.01486831],\n",
       "        [-0.27157453],\n",
       "        [-0.06642973],\n",
       "        [-0.15368336],\n",
       "        [-0.08753301],\n",
       "        [-0.04892436],\n",
       "        [-0.19534439],\n",
       "        [-0.01541219],\n",
       "        [-0.06160044],\n",
       "        [-0.00596446],\n",
       "        [-0.1602587 ],\n",
       "        [-0.29927504],\n",
       "        [ 0.08235867],\n",
       "        [-0.23987168],\n",
       "        [-0.17255242],\n",
       "        [ 0.0133534 ],\n",
       "        [ 0.00966583],\n",
       "        [ 0.06539761],\n",
       "        [ 0.0890826 ],\n",
       "        [ 0.03502611],\n",
       "        [-0.18017481],\n",
       "        [ 0.01742599],\n",
       "        [-0.03363411],\n",
       "        [ 0.30392712],\n",
       "        [ 0.15774207],\n",
       "        [ 0.0113405 ],\n",
       "        [-0.21315543],\n",
       "        [ 0.02298073],\n",
       "        [ 0.08650354],\n",
       "        [-0.09391604],\n",
       "        [ 0.09941538],\n",
       "        [-0.13738367],\n",
       "        [-0.00499601],\n",
       "        [-0.08989532],\n",
       "        [-0.12153702],\n",
       "        [ 0.0383666 ],\n",
       "        [-0.09964025],\n",
       "        [ 0.20899963],\n",
       "        [ 0.24001965],\n",
       "        [-0.16719356],\n",
       "        [-0.0177884 ],\n",
       "        [-0.05795734],\n",
       "        [-0.16564351],\n",
       "        [-0.09853375],\n",
       "        [ 0.11100244],\n",
       "        [ 0.0590837 ],\n",
       "        [ 0.09473059],\n",
       "        [ 0.00666687],\n",
       "        [ 0.074062  ],\n",
       "        [ 0.03963988],\n",
       "        [-0.02302067]]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.expand_dims(hidden, axis=-1)  # Assuming axis=0 is the batch dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 15, 62), dtype=float32, numpy=\n",
       "array([[[-0.75582093, -1.2549475 , -0.6174025 , ..., -1.123265  ,\n",
       "         -0.10176722, -1.3608077 ],\n",
       "        [ 0.03351547, -0.43009937, -1.1128975 , ..., -1.8035787 ,\n",
       "         -0.02993154,  0.00520385],\n",
       "        [-0.84791017, -0.595069  ,  1.6949074 , ..., -0.5999469 ,\n",
       "          0.36516473, -0.9783244 ],\n",
       "        ...,\n",
       "        [-0.15066302, -1.2549475 , -1.6083925 , ..., -1.0447674 ,\n",
       "          0.22149336,  0.8794512 ],\n",
       "        [ 0.34925002,  0.14729431, -0.53481996, ..., -1.0970992 ,\n",
       "         -1.7539879 , -0.9783244 ],\n",
       "        [-0.08488499,  1.3845665 ,  0.45617   , ..., -0.7046105 ,\n",
       "          0.40108258, -0.10407706]],\n",
       "\n",
       "       [[ 0.40187246, -0.67755383, -0.86515   , ..., -0.5476151 ,\n",
       "          0.14965768, -0.5958412 ],\n",
       "        [-1.0057775 , -1.007493  , -1.8561399 , ..., -1.437256  ,\n",
       "         -2.3645914 , -0.21335799],\n",
       "        [-0.15066302, -1.2549475 , -1.6083925 , ..., -1.0447674 ,\n",
       "          0.22149336,  0.8794512 ],\n",
       "        ...,\n",
       "        [ 0.45449486,  1.2195969 ,  0.951665  , ...,  1.3886621 ,\n",
       "         -0.02993154,  1.5351367 ],\n",
       "        [-0.7952877 , -0.10016013,  0.12584   , ..., -0.8092742 ,\n",
       "         -1.4307274 ,  0.82481074],\n",
       "        [ 4.5458884 ,  2.2094147 ,  0.7039175 , ...,  2.6446257 ,\n",
       "          2.5202353 ,  2.409384  ]],\n",
       "\n",
       "       [[-0.08488499,  1.3845665 ,  0.45617   , ..., -0.7046105 ,\n",
       "          0.40108258, -0.10407706],\n",
       "        [ 0.2703164 ,  0.06480949,  0.2084225 , ...,  0.7083485 ,\n",
       "         -0.28135642,  0.27840614],\n",
       "        [-0.09804059, -1.2549475 , -0.69998497, ..., -0.23362419,\n",
       "         -1.1074668 , -0.10407706],\n",
       "        ...,\n",
       "        [ 0.6649846 ,  0.724688  , -0.4522375 , ...,  1.3363303 ,\n",
       "          0.22149336, -0.04943661],\n",
       "        [-0.75582093, -0.26512975, -0.28707248, ...,  0.7083485 ,\n",
       "         -0.604617  , -0.15871753],\n",
       "        [-0.8084433 ,  0.724688  ,  1.2819949 , ...,  1.2055008 ,\n",
       "          0.6525075 ,  0.55160844]],\n",
       "\n",
       "       [[-0.65057606, -0.5125842 ,  0.62133497, ...,  0.28969398,\n",
       "          2.0173855 , -0.10407706],\n",
       "        [ 0.09929351,  0.80717283, -1.19548   , ..., -0.41678554,\n",
       "          0.32924688,  0.16912523],\n",
       "        [ 0.40187246, -0.67755383, -0.86515   , ..., -0.5476151 ,\n",
       "          0.14965768, -0.5958412 ],\n",
       "        ...,\n",
       "        [ 0.37556124, -1.502402  , -0.12190749, ..., -0.05046282,\n",
       "         -0.604617  ,  0.05984431],\n",
       "        [ 1.1254308 , -0.01767532,  0.86908245, ...,  0.28969398,\n",
       "          0.8680145 ,  0.27840614],\n",
       "        [ 0.45449486,  1.2195969 ,  0.951665  , ...,  1.3886621 ,\n",
       "         -0.02993154,  1.5351367 ]],\n",
       "\n",
       "       [[ 0.74391824,  1.137112  , -1.19548   , ..., -0.41678554,\n",
       "         -0.9637954 ,  0.55160844],\n",
       "        [-0.1111962 , -0.34761456, -0.039325  , ..., -0.10279464,\n",
       "         -0.06584938,  0.05984431],\n",
       "        [ 0.3360944 ,  0.97214246,  0.0432575 , ...,  0.9438417 ,\n",
       "          0.14965768, -0.21335799],\n",
       "        ...,\n",
       "        [-0.45324197,  0.80717283,  0.5387525 , ...,  1.0746713 ,\n",
       "         -0.9997133 ,  0.05984431],\n",
       "        [-0.5058644 ,  1.3020817 , -0.7825675 , ...,  0.9176758 ,\n",
       "          0.90393233,  1.2619344 ],\n",
       "        [ 0.80969626, -0.01767532,  3.1813924 , ...,  0.49902126,\n",
       "          1.2631108 ,  1.4804963 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-27 17:53:25.107128: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-27 17:53:25.107204: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-27 17:53:25.108971: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-27 17:53:27.236343: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 22 22\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'treated' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m all_columns \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Create a binary mask indicating whether each column is treated (1) or control (0)\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m cate_mask \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m treated \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m all_columns]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Convert the mask to a PyTorch tensor\u001b[39;00m\n\u001b[1;32m     54\u001b[0m cate_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(cate_mask, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[1], line 51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     48\u001b[0m all_columns \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Create a binary mask indicating whether each column is treated (1) or control (0)\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m cate_mask \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtreated\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m all_columns]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Convert the mask to a PyTorch tensor\u001b[39;00m\n\u001b[1;32m     54\u001b[0m cate_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(cate_mask, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'treated' is not defined"
     ]
    }
   ],
   "source": [
    "from benchmarks.mqrnn import MQRNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from preprocess_scripts.data_loader import DataLoader\n",
    "dataset_name = 'calls911_benchmarks'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "data = pd.read_csv('./datasets/text_data/calls911/' + dataset_name + '.csv')\n",
    "batch_size = 5\n",
    "input_size = 15\n",
    "forecast_horizon=7\n",
    "feature_type='MS'\n",
    "target='ABINGTON'\n",
    "data_loader = DataLoader(dataset_name,batch_size,input_size,forecast_horizon,feature_type, target)\n",
    "\n",
    "# Define parameters for MQRNN model\n",
    "d_input = data_loader.n_feature  # Adjust based on your data\n",
    "d_model = 62  # You can adjust this value\n",
    "tau = 7  # You can adjust this value\n",
    "num_targets = 1  # You can adjust this value\n",
    "num_quantiles = 3  # You can adjust this value based on your task\n",
    "n_layers = 2  # You can adjust this value\n",
    "dr = 0.1  # You can adjust this value\n",
    "num_epochs = 5\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Update the MQRNN model instantiation\n",
    "mqrnn_model = MQRNN(\n",
    "    d_input=d_input,\n",
    "    d_model=d_model,\n",
    "    tau=tau,\n",
    "    num_targets=num_targets,\n",
    "    num_quantiles=num_quantiles,\n",
    "    n_layers=n_layers,\n",
    "    dr=dr\n",
    ").to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Replace with your actual loss function\n",
    "optimizer = optim.Adam(mqrnn_model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_data = data_loader.get_train()\n",
    "val_data = data_loader.get_val()\n",
    "test_data = data_loader.get_test()\n",
    "\n",
    "all_columns = data.columns.tolist()\n",
    "\n",
    "\n",
    "# Create a binary mask indicating whether each column is treated (1) or control (0)\n",
    "cate_mask = [1 if col in treated else 0 for col in all_columns]\n",
    "\n",
    "# Convert the mask to a PyTorch tensor\n",
    "cate_tensor = torch.tensor(cate_mask, dtype=torch.float32).view(1, 1, -1).to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_data:\n",
    "        conti, forecast_tensor = batch\n",
    "\n",
    "        # Create cate_tensor based on your criteria\n",
    "        treated_unit_index = 0\n",
    "        future_tensor = torch.zeros_like(forecast_tensor).to(device)\n",
    "        future_tensor[:, :, treated_unit_index] = 1\n",
    "\n",
    "        # Concatenate conti, cate_tensor, and future_tensor along the last dimension\n",
    "        input_tensor = torch.cat([conti, cate_tensor, future_tensor], dim=-1).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = mqrnn_model(input_tensor)\n",
    "\n",
    "        # Reshape the target tensor to match the output\n",
    "        target = forecast_tensor.to(device)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "zeros_like(): argument 'input' (position 1) must be Tensor, not tensorflow.python.framework.ops.EagerTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Create cate_tensor based on your criteria\u001b[39;00m\n\u001b[1;32m     12\u001b[0m treated_unit_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 13\u001b[0m future_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforecast_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m future_tensor[:, :, treated_unit_index] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Concatenate conti, cate_tensor, and future_tensor along the last dimension\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: zeros_like(): argument 'input' (position 1) must be Tensor, not tensorflow.python.framework.ops.EagerTensor"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 62), dtype=float32, numpy=\n",
       "array([[[ 3.35154720e-02, -4.30099368e-01, -1.11289752e+00,\n",
       "         -1.21783900e+00, -3.15799624e-01, -1.53396741e-01,\n",
       "         -5.50933659e-01,  8.85367215e-01,  8.16210389e-01,\n",
       "         -1.52368045e+00, -2.90683061e-01, -9.78549778e-01,\n",
       "         -8.48706186e-01,  1.12148929e+00, -6.56743348e-01,\n",
       "         -5.20974219e-01, -8.50343406e-01,  1.08714633e-01,\n",
       "         -4.56157058e-01, -1.27903569e+00, -7.68515944e-01,\n",
       "         -1.91132426e-01,  2.69135892e-01, -7.37774611e-01,\n",
       "         -7.84782469e-01,  3.05532515e-01, -1.48782730e-01,\n",
       "         -1.17373967e+00, -1.07498932e+00, -8.86303723e-01,\n",
       "          9.65268135e-01, -1.33656108e+00, -1.51987255e+00,\n",
       "         -9.55359519e-01, -5.83113497e-03, -8.71230781e-01,\n",
       "          1.56340134e+00, -6.12545907e-01,  9.10261750e-01,\n",
       "         -3.97071630e-01, -1.02930643e-01, -3.79987121e-01,\n",
       "          1.74299642e-01, -1.23852587e+00, -1.72188854e+00,\n",
       "         -5.07075906e-01, -2.52038896e-01, -1.22431362e+00,\n",
       "         -2.15928626e+00,  8.18330348e-01,  5.24921417e-01,\n",
       "         -7.76178896e-01, -8.29035282e-01,  4.99233305e-01,\n",
       "         -1.11362946e+00, -1.77030712e-01, -8.83580148e-01,\n",
       "         -7.80191302e-01, -7.13124752e-01, -1.80357873e+00,\n",
       "         -2.99315359e-02,  5.20385336e-03],\n",
       "        [-8.47910166e-01, -5.95068991e-01,  1.69490743e+00,\n",
       "          1.55294910e-01, -6.31975651e-01,  1.85690805e-01,\n",
       "         -1.05948782e+00, -1.21227205e+00, -1.69981432e+00,\n",
       "         -1.04534411e+00,  1.07967985e+00, -9.78549778e-01,\n",
       "          3.64189595e-01, -1.70799255e+00,  5.86444080e-01,\n",
       "         -5.89209914e-01, -2.57924080e-01, -5.74053884e-01,\n",
       "          3.50890048e-02, -1.02241385e+00,  5.12343980e-02,\n",
       "         -2.70525903e-01, -7.98436463e-01, -1.65363282e-01,\n",
       "         -9.76974130e-01, -5.05434811e-01, -6.78348422e-01,\n",
       "         -6.97439551e-01, -1.17755011e-01, -8.86303723e-01,\n",
       "         -2.39095703e-01,  5.69929838e-01,  1.13106787e+00,\n",
       "         -8.70168209e-01,  5.81947267e-01, -7.41319478e-01,\n",
       "         -2.52161510e-02, -1.30166009e-01, -5.67264557e-01,\n",
       "         -1.33927548e+00, -1.02930643e-01, -7.24435925e-01,\n",
       "         -1.22480832e-01, -3.85232329e-01, -8.50571394e-01,\n",
       "         -6.33593857e-01, -8.33667099e-01, -6.83265924e-01,\n",
       "         -6.62339568e-01,  4.29420859e-01,  6.11379087e-01,\n",
       "         -4.47274208e-01,  2.87101507e-01, -1.10136199e+00,\n",
       "         -1.44029403e+00,  1.72865286e-01, -9.99550045e-01,\n",
       "          1.84993818e-01,  1.12745428e+00, -5.99946916e-01,\n",
       "          3.65164727e-01, -9.78324413e-01],\n",
       "        [-6.76887274e-01,  2.29779124e-01, -3.93249989e-02,\n",
       "          4.98578370e-01,  1.42655611e-01, -1.93360639e+00,\n",
       "         -7.41641462e-01,  2.49718964e-01, -2.84550399e-01,\n",
       "         -3.27839434e-01, -1.20425832e+00,  5.66528797e-01,\n",
       "          9.03254390e-01,  7.88609087e-01, -8.12141776e-01,\n",
       "          2.75109053e-01, -5.96449435e-01,  2.79406756e-01,\n",
       "         -6.66691124e-01, -1.36457622e+00, -1.87859461e-01,\n",
       "          3.20514381e-01, -7.62551650e-02, -1.27202526e-01,\n",
       "         -4.24423188e-01, -1.17329037e+00,  1.68956667e-01,\n",
       "          3.08083057e-01,  2.11579180e+00, -4.96515721e-01,\n",
       "          4.92730998e-02,  2.52181351e-01,  4.94842231e-01,\n",
       "          1.08923161e+00,  1.04727185e+00,  5.57793915e-01,\n",
       "         -4.22370523e-01,  1.91420600e-01, -6.59609914e-01,\n",
       "         -1.33927548e+00,  1.19399548e+00, -8.39252174e-01,\n",
       "          5.70006907e-01, -2.21137419e-01, -9.47384417e-01,\n",
       "          2.94204444e-01,  2.19079947e+00, -9.86252606e-01,\n",
       "         -1.90979517e+00, -5.42852819e-01, -2.93338442e+00,\n",
       "         -6.11726522e-01,  2.27308467e-01, -4.61123884e-01,\n",
       "         -4.30603385e-01, -7.01874673e-01, -7.17908889e-02,\n",
       "          4.02160473e-02, -1.46608889e+00,  3.15859884e-01,\n",
       "          9.39850211e-01, -8.69043529e-01],\n",
       "        [-5.85737713e-02, -3.47614557e-01, -9.47732449e-01,\n",
       "          4.98578370e-01,  1.26846820e-01, -3.22940528e-01,\n",
       "          1.54685223e+00,  9.48932052e-01,  1.60246813e+00,\n",
       "         -2.25338757e-01,  1.24578446e-01, -6.69534087e-01,\n",
       "         -1.58992028e+00, -1.04223216e+00,  9.74940121e-01,\n",
       "          7.52758980e-01,  8.06012750e-02,  2.79406756e-01,\n",
       "          3.50890040e-01, -1.67007849e-01,  2.56172001e-01,\n",
       "          4.17550832e-01, -2.33251095e-01,  2.92565793e-01,\n",
       "         -4.96495038e-01, -4.57730889e-01, -7.84261525e-01,\n",
       "         -4.06367213e-01,  4.17840369e-02,  2.32016686e-02,\n",
       "          9.14379537e-01,  8.87678325e-01,  8.12955081e-01,\n",
       "         -4.44211751e-01,  6.79910302e-01,  1.14239490e+00,\n",
       "         -5.54755330e-01, -4.51752633e-01, -4.74919170e-01,\n",
       "         -2.01900825e-02, -2.47033536e-01,  9.40399826e-01,\n",
       "          2.73226470e-01, -3.85232329e-01,  4.07997668e-01,\n",
       "         -6.33593857e-01, -2.52038896e-01,  9.58427340e-02,\n",
       "         -5.37593961e-01,  1.01278508e+00, -8.58400941e-01,\n",
       "         -2.98182797e-02,  2.87101507e-01, -6.09750599e-02,\n",
       "         -9.05751944e-01, -1.57661462e+00, -1.28947484e+00,\n",
       "         -2.01080233e-01,  2.07164735e-01,  5.42008094e-02,\n",
       "         -6.76452696e-01, -7.59762585e-01],\n",
       "        [ 3.36094409e-01, -1.83234119e+00,  6.21334970e-01,\n",
       "         -1.56112242e+00,  3.32361251e-01,  2.70462692e-01,\n",
       "          7.20451713e-01, -3.85929316e-01,  6.58958852e-01,\n",
       "          1.50497004e-01, -5.81366122e-01,  8.75544548e-01,\n",
       "          2.29423404e-01, -3.76471668e-01, -6.17893755e-01,\n",
       "         -4.52738494e-01,  1.18080866e+00,  6.20791018e-01,\n",
       "          8.07047129e-01, -3.38089049e-01, -5.97734630e-01,\n",
       "          4.44015324e-01, -1.39053538e-01, -1.11938214e+00,\n",
       "         -6.16614819e-01, -3.62322956e-01, -1.41974032e+00,\n",
       "         -4.85750556e-01,  2.01323092e-01, -8.21339071e-01,\n",
       "         -7.81907558e-01,  6.75845981e-01,  1.13106787e+00,\n",
       "         -1.03446573e-01,  1.41463327e+00,  3.62926900e-01,\n",
       "         -1.21667922e+00,  1.79935372e+00,  3.56189370e-01,\n",
       "          1.68250695e-01, -5.35239339e-01,  8.82991672e-01,\n",
       "         -8.14968586e-01, -2.86775380e-01, -1.14101040e+00,\n",
       "         -6.75766528e-01,  5.62240601e-01,  6.80174232e-01,\n",
       "          1.33358955e+00, -3.48398060e-01, -1.03131616e+00,\n",
       "         -4.34624016e-01, -9.15877521e-02, -7.81242967e-01,\n",
       "          3.11816245e-01,  9.60131228e-01, -1.38059398e-02,\n",
       "          2.81512320e-01,  1.37844229e+00, -1.28960550e-01,\n",
       "         -6.04617000e-01, -1.85257173e+00],\n",
       "        [-4.26930755e-01, -5.95068991e-01, -6.17402494e-01,\n",
       "          3.26936632e-01,  9.52292085e-02,  2.22021604e+00,\n",
       "         -1.69518054e-01,  5.67543089e-01, -1.27298862e-01,\n",
       "         -3.27839434e-01, -4.98313785e-01,  1.18456030e+00,\n",
       "         -7.81323075e-01, -7.09351897e-01, -1.12293863e+00,\n",
       "         -1.08960509e+00, -8.86613950e-02, -5.31380892e-01,\n",
       "         -4.56157058e-01, -1.10795450e+00, -1.04176605e+00,\n",
       "          6.11623764e-01,  2.69135892e-01, -8.90417635e-01,\n",
       "         -2.80279458e-01, -3.14619005e-01, -4.66522127e-01,\n",
       "         -1.46481204e+00, -2.77294070e-01,  2.32016686e-02,\n",
       "         -3.55412550e-02, -8.06980312e-01, -1.41383484e-01,\n",
       "         -8.70168209e-01, -2.26248026e-01,  8.82572234e-01,\n",
       "         -1.61383367e+00, -2.90959328e-01, -8.44300747e-01,\n",
       "         -4.91292030e-01,  4.73480940e-01, -9.29464921e-02,\n",
       "          3.72153282e-01, -5.82146227e-01,  4.07997668e-01,\n",
       "          3.36377084e-01,  9.69380364e-02,  5.25589176e-02,\n",
       "         -2.88102865e-01, -2.51170695e-01, -1.66739747e-01,\n",
       "         -7.00277805e-01, -1.31449789e-01, -4.61123884e-01,\n",
       "          5.19693732e-01, -3.51978689e-01,  5.08058608e-01,\n",
       "         -3.94117266e-01, -2.11148679e-01, -1.22792876e+00,\n",
       "          1.13739833e-01,  2.78406143e-01],\n",
       "        [ 8.62318695e-01,  4.77233559e-01, -2.87072480e-01,\n",
       "         -8.74555528e-01,  6.64346099e-01,  1.11818159e+00,\n",
       "          5.29743910e-01, -1.65722585e+00,  5.01707315e-01,\n",
       "         -8.86712074e-02,  1.66104600e-01, -3.60518336e-01,\n",
       "          9.46572125e-02,  2.28657007e+00,  1.59098387e-01,\n",
       "          3.20599526e-01, -4.03006375e-03,  1.17554045e+00,\n",
       "          4.56157058e-01,  6.02857590e-01,  3.92797053e-01,\n",
       "          6.38088226e-01,  7.08724499e-01,  9.03137922e-01,\n",
       "         -2.80279458e-01,  1.62420630e-01, -5.72435260e-01,\n",
       "          4.40388650e-01,  3.60862136e-01,  8.02777767e-01,\n",
       "          3.03716153e-01,  2.52181351e-01,  1.23710549e+00,\n",
       "         -7.84976959e-01,  6.76411614e-02,  1.68059900e-01,\n",
       "         -4.22370523e-01,  3.52213919e-01, -7.51955330e-01,\n",
       "         -1.62193668e+00, -1.25575376e+00,  1.57188928e+00,\n",
       "         -7.16041744e-01,  1.28853571e+00, -9.47384417e-01,\n",
       "          2.09859133e-01, -1.29896963e+00,  1.45928288e+00,\n",
       "         -1.63357288e-01, -1.05329648e-01,  6.17554598e-03,\n",
       "         -2.82821864e-01,  4.79293428e-02, -3.81094128e-01,\n",
       "          8.16661596e-01, -9.64296699e-01,  1.84171247e+00,\n",
       "          4.74549353e-01,  7.09140837e-01,  1.86899351e-03,\n",
       "          9.39850211e-01, -3.22638899e-01],\n",
       "        [-6.50576055e-01, -5.12584209e-01,  6.21334970e-01,\n",
       "         -5.31272054e-01,  1.74273223e-01, -5.77256203e-01,\n",
       "         -3.60225856e-01,  5.90244830e-02,  1.44521654e+00,\n",
       "          1.16330117e-01, -6.64418399e-01, -6.69534087e-01,\n",
       "         -1.07492089e-01,  7.88609087e-01,  2.75647193e-01,\n",
       "          4.11580443e-01, -7.65712082e-01, -1.21414936e+00,\n",
       "         -1.12284815e+00,  4.31776375e-01,  2.90328264e-01,\n",
       "          4.96944308e-01,  3.63333434e-01,  9.41298664e-01,\n",
       "          8.00798461e-03, -9.82474446e-01,  5.92609227e-01,\n",
       "          3.61005276e-01,  3.60862136e-01,  8.81663412e-02,\n",
       "          4.05493379e-01, -1.33656108e+00, -3.53458732e-01,\n",
       "         -1.03446573e-01,  3.12548816e-01,  4.27882582e-01,\n",
       "          1.56340134e+00,  1.91420600e-01,  2.63843983e-01,\n",
       "         -3.97071630e-01, -8.23445141e-01, -5.52211523e-01,\n",
       "          2.73226470e-01,  5.33699155e-01, -5.60132384e-01,\n",
       "         -2.54040003e-01,  1.72549701e+00,  2.25694180e-01,\n",
       "         -7.87085116e-01,  8.18330348e-01, -4.26112682e-01,\n",
       "         -6.62327230e-01,  2.47239485e-01,  1.79114237e-01,\n",
       "          1.48483925e-02, -2.08271411e-03, -2.45745733e-01,\n",
       "         -4.42376524e-01, -9.64112818e-01,  2.89693981e-01,\n",
       "          2.01738548e+00, -1.04077064e-01],\n",
       "        [ 9.92935076e-02,  8.07172835e-01, -1.19547999e+00,\n",
       "          3.26936632e-01, -1.26094013e-01, -1.53396741e-01,\n",
       "         -7.41641462e-01,  3.13283801e-01,  3.01773190e+00,\n",
       "         -3.27839434e-01, -4.56787646e-01,  2.57513106e-01,\n",
       "          2.72741113e-02,  1.28792942e+00,  4.25495692e-02,\n",
       "          3.20599526e-01,  7.57651925e-01,  4.50098902e-01,\n",
       "          5.96513093e-01, -1.36457622e+00,  2.22015724e-01,\n",
       "         -3.32276374e-01, -1.34567944e-02,  9.41298664e-01,\n",
       "          9.44942176e-01, -7.91658640e-01, -1.48782730e-01,\n",
       "          2.02238560e-01, -1.17755011e-01,  1.12760115e+00,\n",
       "          9.82231021e-01, -7.01064169e-01,  6.00879848e-01,\n",
       "         -6.99785650e-01, -1.03794202e-01,  5.79445779e-01,\n",
       "          1.56340134e+00, -6.12545907e-01,  1.09495246e+00,\n",
       "         -1.90459788e+00, -1.39985669e+00, -8.39252174e-01,\n",
       "          1.55927515e+00,  8.94707918e-01,  2.07456443e-02,\n",
       "         -6.75766528e-01, -3.68364513e-01, -1.11610401e+00,\n",
       "         -2.88102865e-01, -8.10228102e-03,  6.97836697e-01,\n",
       "          6.27991080e-01,  3.66825551e-01, -3.01064372e-01,\n",
       "          7.27571249e-01,  4.35287267e-01, -9.99550045e-01,\n",
       "         -1.52820975e-01,  1.96408105e+00, -4.16785538e-01,\n",
       "          3.29246879e-01,  1.69125229e-01],\n",
       "        [ 4.01872456e-01, -6.77553833e-01, -8.65149975e-01,\n",
       "          4.98578370e-01, -1.09043097e+00, -6.62028074e-01,\n",
       "         -1.05948783e-01,  3.13283801e-01, -5.99053502e-01,\n",
       "         -2.25338757e-01, -2.07630754e-01,  1.18456030e+00,\n",
       "         -1.74875185e-01,  2.89288759e-01, -1.08408904e+00,\n",
       "          1.61382869e-01, -6.81080759e-01,  8.34156215e-01,\n",
       "         -2.80712038e-01,  4.07336187e-03, -5.97734630e-01,\n",
       "         -7.99815655e-01, -9.24033165e-01,  7.88655639e-01,\n",
       "          2.24223569e-01,  2.57828563e-01,  1.12217486e+00,\n",
       "         -6.70978427e-01, -4.36833113e-01, -8.86303723e-01,\n",
       "          8.31988454e-02, -1.01881266e+00,  7.06917465e-01,\n",
       "         -1.82552785e-02, -1.05893409e+00, -3.73237342e-01,\n",
       "          9.01477396e-01, -1.30166009e-01, -1.97882980e-01,\n",
       "         -2.01900825e-02,  7.61686742e-01,  2.03115416e+00,\n",
       "          2.73226470e-01,  1.05880284e+00,  4.07997668e-01,\n",
       "          5.47240317e-01,  1.49284565e+00, -1.24595547e+00,\n",
       "          2.10879415e-01,  3.32193494e-01,  1.56241322e+00,\n",
       "         -1.06713295e+00, -1.08813846e+00, -6.21183455e-01,\n",
       "          2.22725883e-01,  8.53912830e-02, -6.51640356e-01,\n",
       "          7.15845644e-01, -8.80450130e-01, -5.47615111e-01,\n",
       "          1.49657682e-01, -5.95841229e-01],\n",
       "        [-1.00577748e+00, -1.00749302e+00, -1.85613990e+00,\n",
       "         -5.31272054e-01, -1.53307736e+00, -6.62028074e-01,\n",
       "         -1.31376493e+00, -8.94447923e-01,  5.01707315e-01,\n",
       "         -8.06175828e-01, -1.16273224e+00, -1.59658122e+00,\n",
       "          1.10540366e+00, -3.76471668e-01, -1.74453235e+00,\n",
       "         -2.70776629e-01, -9.34974730e-01, -1.72622585e+00,\n",
       "         -1.68427229e+00, -1.27903569e+00, -5.12343980e-02,\n",
       "         -9.93888617e-01, -1.39502096e+00, -5.46970844e-01,\n",
       "         -1.31330943e+00, -6.48546755e-01, -1.52565348e+00,\n",
       "         -1.59711766e+00,  2.01323092e-01, -6.26445055e-01,\n",
       "         -1.39257097e+00, -1.54839349e+00, -7.77609169e-01,\n",
       "         -1.21093345e+00, -1.42629552e+00, -1.02279401e+00,\n",
       "         -8.19524884e-01, -7.73339272e-01, -1.05537593e-01,\n",
       "         -1.33927548e+00,  1.19399548e+00, -1.41333342e+00,\n",
       "         -1.80423677e+00, -1.73081052e+00, -5.60132384e-01,\n",
       "         -1.22401094e+00, -7.17341423e-01, -1.80864513e+00,\n",
       "          3.35624963e-01, -2.51170695e-01,  6.17554598e-03,\n",
       "         -1.81349361e+00, -6.29725158e-01, -1.82162988e+00,\n",
       "         -4.89996940e-01, -7.01874673e-01, -1.57939959e+00,\n",
       "         -1.60059869e+00, -5.45799434e-01, -1.43725598e+00,\n",
       "         -2.36459136e+00, -2.13357985e-01],\n",
       "        [-1.50663018e-01, -1.25494754e+00, -1.60839248e+00,\n",
       "         -7.02913761e-01, -2.20946833e-01,  1.20295346e+00,\n",
       "          2.11897567e-02,  5.90244830e-02, -5.99053502e-01,\n",
       "         -1.91171870e-01,  5.39839923e-01,  1.49357593e+00,\n",
       "         -5.11790693e-01, -5.42911768e-01,  8.58391285e-01,\n",
       "         -5.66464663e-01,  3.34495276e-01, -1.89996600e-01,\n",
       "         -1.26320422e+00,  8.96139666e-02,  4.95265841e-01,\n",
       "         -1.99953914e-01, -7.62551650e-02, -3.94327819e-01,\n",
       "          7.28726566e-01, -5.53138793e-01, -7.84261525e-01,\n",
       "          3.87466401e-01,  6.79940224e-01, -1.27609181e+00,\n",
       "         -4.59613025e-01,  1.46265179e-01, -5.65533936e-01,\n",
       "          5.78083813e-01, -3.24211091e-01, -1.78370327e-01,\n",
       "          1.29863179e+00, -4.51752633e-01,  2.63843983e-01,\n",
       "         -1.14410475e-01, -2.47033536e-01, -6.09619617e-01,\n",
       "         -1.70530999e+00, -2.42235325e-02, -3.66506398e-01,\n",
       "         -3.38385314e-01,  4.45914954e-01, -2.93711603e-01,\n",
       "         -3.86117212e-02, -8.83148611e-01,  9.26331952e-02,\n",
       "         -9.30691808e-02, -4.30415004e-01, -3.81094128e-01,\n",
       "          7.42419586e-02, -8.76822710e-01,  3.34103763e-01,\n",
       "          8.84753019e-02, -7.13124752e-01, -1.04476738e+00,\n",
       "          2.21493363e-01,  8.79451215e-01],\n",
       "        [ 3.49250019e-01,  1.47294313e-01, -5.34819961e-01,\n",
       "         -3.59630316e-01, -5.52931666e-01,  2.70462692e-01,\n",
       "         -1.69518054e-01, -2.10217977e+00,  2.99526751e-02,\n",
       "         -4.30340081e-01, -1.12120605e+00,  1.49357593e+00,\n",
       "         -5.11790693e-01, -4.35914584e-02, -1.90548062e-01,\n",
       "         -7.93917060e-01,  7.57651925e-01, -9.15438175e-01,\n",
       "         -1.08775914e+00,  4.07336187e-03, -5.12343980e-02,\n",
       "         -3.05811882e-01, -1.33222258e+00,  1.59003150e+00,\n",
       "         -8.32830369e-01, -2.55670524e+00, -9.96087790e-01,\n",
       "         -4.85750556e-01, -1.39406741e+00,  5.42919040e-01,\n",
       "         -1.18901646e+00, -1.71483308e-01, -3.53458732e-01,\n",
       "          1.00404024e+00, -1.27935100e+00, -5.15521178e-03,\n",
       "          6.36707783e-01, -2.90959328e-01, -1.39837313e+00,\n",
       "         -2.08630860e-01, -3.91136438e-01,  8.25583577e-01,\n",
       "         -8.14968586e-01, -1.04161191e+00, -2.69693375e-01,\n",
       "         -1.05532038e+00, -8.33667099e-01, -1.00789452e+00,\n",
       "         -3.86117212e-02, -1.53943330e-01, -5.99027991e-01,\n",
       "         -1.21893513e+00, -5.50001085e-01,  2.59144008e-01,\n",
       "         -8.46358359e-01, -7.01874673e-01, -2.45745733e-01,\n",
       "         -1.64885795e+00, -2.94811368e-01, -1.09709918e+00,\n",
       "         -1.75398791e+00, -9.78324413e-01],\n",
       "        [-8.48849863e-02,  1.38456655e+00,  4.56169993e-01,\n",
       "          1.55294910e-01,  2.84934849e-01, -2.38168627e-01,\n",
       "         -1.05948783e-01, -6.81051761e-02, -1.27298862e-01,\n",
       "          1.84663892e-01, -4.56787646e-01,  2.57513106e-01,\n",
       "          9.46572125e-02, -8.75792027e-01,  5.86444080e-01,\n",
       "         -7.71171808e-01,  1.65232614e-01, -2.32669637e-01,\n",
       "          3.50890040e-01, -1.02241385e+00,  1.19546928e-01,\n",
       "         -4.91063297e-01, -6.10041320e-01, -8.52256894e-01,\n",
       "          4.88487065e-01, -4.57730889e-01, -2.54695863e-01,\n",
       "          3.61005276e-01, -4.36833113e-01, -1.47098577e+00,\n",
       "          1.00161716e-01, -2.77399480e-01, -3.53458710e-02,\n",
       "          8.33657682e-01, -1.03794202e-01, -1.02279401e+00,\n",
       "          1.43101656e+00, -2.05968571e+00, -1.21368229e+00,\n",
       "         -3.97071630e-01,  1.04989254e+00,  2.51502275e-01,\n",
       "          6.68933749e-01,  3.69604230e-01, -4.63319391e-01,\n",
       "         -2.96212673e-01, -2.52038896e-01,  3.98829430e-01,\n",
       "         -1.63357288e-01, -4.45625424e-01, -8.58400941e-01,\n",
       "         -4.21973854e-01, -9.08759296e-01, -1.41004831e-01,\n",
       "          4.60300177e-01, -6.14400685e-01, -7.09625304e-01,\n",
       "          1.84993818e-01,  2.90827423e-01, -7.04610527e-01,\n",
       "          4.01082575e-01, -1.04077064e-01],\n",
       "        [ 2.70316392e-01,  6.48094937e-02,  2.08422497e-01,\n",
       "          2.04335403e+00,  4.11405236e-01,  3.55234563e-01,\n",
       "         -3.60225856e-01, -7.03753471e-01, -4.41801965e-01,\n",
       "         -5.67007661e-01, -4.15261507e-01, -3.60518336e-01,\n",
       "         -1.52253723e+00, -1.87443268e+00,  4.31045622e-01,\n",
       "          5.02561390e-01,  5.88389277e-01,  3.64752829e-01,\n",
       "         -3.50890040e-01, -2.52548456e-01, -5.63578367e-01,\n",
       "          3.91086340e-01,  3.31934243e-01,  3.68887305e-01,\n",
       "          8.72870326e-01,  4.48644400e-01,  2.07539320e+00,\n",
       "          3.34544152e-01,  1.79671359e+00, -2.36657023e-01,\n",
       "          5.24233460e-01,  3.58097523e-01,  1.13106787e+00,\n",
       "          1.68557060e+00,  1.16972566e+00,  2.76319355e-01,\n",
       "          5.04323006e-01,  3.52213919e-01,  9.10261750e-01,\n",
       "         -6.79732800e-01,  4.73480940e-01,  1.85892987e+00,\n",
       "          6.68933749e-01,  6.64975047e-01, -7.53758430e-01,\n",
       "         -4.31767590e-02,  1.49284565e+00,  5.28680861e-01,\n",
       "         -1.63357288e-01, -7.85921216e-01, -9.44858551e-01,\n",
       "          3.34326178e-02, -2.70966887e-01, -1.41004831e-01,\n",
       "          3.41513038e-01, -8.95567089e-02, -2.45745733e-01,\n",
       "         -2.49339491e-01,  3.98393720e-02,  7.08348513e-01,\n",
       "         -2.81356424e-01,  2.78406143e-01]]], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 7, 1), dtype=float32, numpy=\n",
       "array([[[-0.09804059],\n",
       "        [ 0.74391824],\n",
       "        [-0.1111962 ],\n",
       "        [ 0.3360944 ],\n",
       "        [ 0.37556124],\n",
       "        [ 1.1254308 ],\n",
       "        [ 0.45449486]]], dtype=float32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "torch.cat(): expected a non-empty list of Tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m tmp_feature_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m emb_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp_feature_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m emb_output \u001b[38;5;241m=\u001b[39m emb_output\u001b[38;5;241m.\u001b[39mview(conti\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), conti\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([conti, emb_output], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: torch.cat(): expected a non-empty list of Tensors"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "tmp_feature_list = []\n",
    "emb_output = torch.cat(tmp_feature_list, axis=-2)\n",
    "emb_output = emb_output.view(conti.size(0), conti.size(1), -1)\n",
    "\n",
    "x = torch.cat([conti, emb_output], axis=-1)\n",
    "\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 7, 1), dtype=float32, numpy=\n",
       "array([[[ 0.3360944 ],\n",
       "        [ 0.37556124],\n",
       "        [ 1.1254308 ],\n",
       "        [ 0.45449486],\n",
       "        [-0.7952877 ],\n",
       "        [ 4.5458884 ],\n",
       "        [-0.5716424 ]],\n",
       "\n",
       "       [[-0.5716424 ],\n",
       "        [ 0.6649846 ],\n",
       "        [-0.75582093],\n",
       "        [-0.8084433 ],\n",
       "        [-0.45324197],\n",
       "        [-0.5058644 ],\n",
       "        [ 0.80969626]],\n",
       "\n",
       "       [[ 4.5458884 ],\n",
       "        [-0.5716424 ],\n",
       "        [ 0.6649846 ],\n",
       "        [-0.75582093],\n",
       "        [-0.8084433 ],\n",
       "        [-0.45324197],\n",
       "        [-0.5058644 ]],\n",
       "\n",
       "       [[ 0.6649846 ],\n",
       "        [-0.75582093],\n",
       "        [-0.8084433 ],\n",
       "        [-0.45324197],\n",
       "        [-0.5058644 ],\n",
       "        [ 0.80969626],\n",
       "        [ 1.1254308 ]],\n",
       "\n",
       "       [[ 0.74391824],\n",
       "        [-0.1111962 ],\n",
       "        [ 0.3360944 ],\n",
       "        [ 0.37556124],\n",
       "        [ 1.1254308 ],\n",
       "        [ 0.45449486],\n",
       "        [-0.7952877 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pycausalimpact\n",
    "from causalimpact import CausalImpact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset_name = 'calls911_benchmarks'\n",
    "data_row = pd.read_csv('./datasets/text_data/calls911/'+dataset_name+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_horizon=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ABINGTON</th>\n",
       "      <th>AMBLER</th>\n",
       "      <th>BRIDGEPORT</th>\n",
       "      <th>BRYN ATHYN</th>\n",
       "      <th>CHELTENHAM</th>\n",
       "      <th>COLLEGEVILLE</th>\n",
       "      <th>CONSHOHOCKEN</th>\n",
       "      <th>DOUGLASS</th>\n",
       "      <th>EAST GREENVILLE</th>\n",
       "      <th>...</th>\n",
       "      <th>UPPER MORELAND</th>\n",
       "      <th>UPPER POTTSGROVE</th>\n",
       "      <th>UPPER PROVIDENCE</th>\n",
       "      <th>UPPER SALFORD</th>\n",
       "      <th>WEST CONSHOHOCKEN</th>\n",
       "      <th>WEST NORRITON</th>\n",
       "      <th>WEST POTTSGROVE</th>\n",
       "      <th>WHITEMARSH</th>\n",
       "      <th>WHITPAIN</th>\n",
       "      <th>WORCESTER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-12-01</td>\n",
       "      <td>514.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>339.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>276.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>727.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>586.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>460.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>316.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-02-01</td>\n",
       "      <td>713.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>548.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>...</td>\n",
       "      <td>389.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>281.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-03-01</td>\n",
       "      <td>668.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>488.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>338.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>728.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>372.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016-05-01</td>\n",
       "      <td>661.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>511.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>428.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016-06-01</td>\n",
       "      <td>674.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>425.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>279.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>721.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>559.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>...</td>\n",
       "      <td>428.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016-08-01</td>\n",
       "      <td>751.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>572.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>409.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>693.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>557.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>407.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>115.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>791.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>593.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>...</td>\n",
       "      <td>416.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>321.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2016-11-01</td>\n",
       "      <td>676.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>562.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>...</td>\n",
       "      <td>426.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2016-12-01</td>\n",
       "      <td>733.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>543.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>...</td>\n",
       "      <td>432.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>756.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>482.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>359.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>301.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2017-02-01</td>\n",
       "      <td>649.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>454.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>...</td>\n",
       "      <td>382.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>106.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>714.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>537.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>392.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>752.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>516.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>386.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2017-05-01</td>\n",
       "      <td>719.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>569.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>368.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>309.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>293.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>746.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>400.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>347.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>115.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>718.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>566.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>411.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>782.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>532.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>427.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>274.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>717.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>319.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>316.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>111.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>751.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>562.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>360.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>334.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>106.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>754.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>552.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>387.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>111.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>811.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>627.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>495.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>337.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>115.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>760.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>574.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>...</td>\n",
       "      <td>496.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>373.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>138.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>665.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>407.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>1071.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>724.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>578.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>421.0</td>\n",
       "      <td>314.0</td>\n",
       "      <td>154.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>682.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>491.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>388.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>107.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2018-05-01</td>\n",
       "      <td>776.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>625.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>419.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>668.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>631.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>467.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>348.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>347.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>107.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2018-07-01</td>\n",
       "      <td>664.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>575.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>455.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>274.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>366.0</td>\n",
       "      <td>262.0</td>\n",
       "      <td>120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2018-08-01</td>\n",
       "      <td>691.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>542.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>442.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>111.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2018-09-01</td>\n",
       "      <td>687.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>612.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>427.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>355.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>133.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>787.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>578.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>480.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>353.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>339.0</td>\n",
       "      <td>279.0</td>\n",
       "      <td>137.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2018-11-01</td>\n",
       "      <td>811.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>665.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>...</td>\n",
       "      <td>454.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>368.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>386.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2018-12-01</td>\n",
       "      <td>682.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>442.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>323.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>118.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>733.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>498.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>...</td>\n",
       "      <td>417.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>344.0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>111.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>666.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>497.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>387.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>333.0</td>\n",
       "      <td>262.0</td>\n",
       "      <td>131.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>703.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>570.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>403.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>317.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>720.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>603.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>388.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>328.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>784.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>580.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>429.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>344.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>337.0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>665.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>572.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>...</td>\n",
       "      <td>447.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>387.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>115.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>777.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>603.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>...</td>\n",
       "      <td>460.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2019-08-01</td>\n",
       "      <td>718.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>612.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>420.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>367.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>111.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2019-09-01</td>\n",
       "      <td>736.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>583.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>361.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>111.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>807.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>625.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>433.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>429.0</td>\n",
       "      <td>309.0</td>\n",
       "      <td>117.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>751.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>393.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>796.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>590.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>437.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>143.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>693.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>545.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>459.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>328.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>111.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>639.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>482.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>400.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>575.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>478.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>360.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>518.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>392.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>273.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>504.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>425.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>330.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2020-06-01</td>\n",
       "      <td>685.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>454.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>...</td>\n",
       "      <td>421.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>262.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>614.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>514.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>367.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56 rows Ã— 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  ABINGTON  AMBLER  BRIDGEPORT  BRYN ATHYN  CHELTENHAM  \\\n",
       "0   2015-12-01     514.0    49.0        43.0        19.0       339.0   \n",
       "1   2016-01-01     727.0    82.0        71.0        15.0       586.0   \n",
       "2   2016-02-01     713.0    57.0        60.0        18.0       548.0   \n",
       "3   2016-03-01     668.0    63.0        59.0        18.0       488.0   \n",
       "4   2016-04-01     728.0    73.0        53.0        16.0       531.0   \n",
       "5   2016-05-01     661.0    71.0        87.0        24.0       511.0   \n",
       "6   2016-06-01     674.0    81.0        66.0        26.0       560.0   \n",
       "7   2016-07-01     721.0    74.0        55.0        26.0       559.0   \n",
       "8   2016-08-01     751.0    56.0        74.0        14.0       572.0   \n",
       "9   2016-09-01     693.0    71.0        59.0        25.0       557.0   \n",
       "10  2016-10-01     791.0    84.0        63.0        18.0       593.0   \n",
       "11  2016-11-01     676.0    72.0        74.0        20.0       562.0   \n",
       "12  2016-12-01     733.0    88.0        52.0        25.0       543.0   \n",
       "13  2017-01-01     756.0    70.0        56.0        26.0       482.0   \n",
       "14  2017-02-01     649.0    66.0        44.0        20.0       454.0   \n",
       "15  2017-03-01     714.0    63.0        47.0        19.0       537.0   \n",
       "16  2017-04-01     752.0    80.0        60.0        21.0       516.0   \n",
       "17  2017-05-01     719.0    95.0        72.0        24.0       569.0   \n",
       "18  2017-06-01     746.0    79.0        69.0        35.0       577.0   \n",
       "19  2017-07-01     718.0    63.0        58.0        22.0       566.0   \n",
       "20  2017-08-01     782.0    92.0        52.0        23.0       532.0   \n",
       "21  2017-09-01     717.0    74.0        66.0        22.0       512.0   \n",
       "22  2017-10-01     751.0    90.0        67.0        29.0       562.0   \n",
       "23  2017-11-01     754.0    60.0        65.0        19.0       552.0   \n",
       "24  2017-12-01     811.0    78.0        77.0        24.0       627.0   \n",
       "25  2018-01-01     760.0    93.0        78.0        29.0       574.0   \n",
       "26  2018-02-01     665.0    77.0        68.0        24.0       438.0   \n",
       "27  2018-03-01    1071.0   105.0        75.0        34.0       724.0   \n",
       "28  2018-04-01     682.0    81.0        67.0        17.0       491.0   \n",
       "29  2018-05-01     776.0    87.0        61.0        17.0       625.0   \n",
       "30  2018-06-01     668.0    75.0        63.0        23.0       631.0   \n",
       "31  2018-07-01     664.0    87.0        82.0        26.0       575.0   \n",
       "32  2018-08-01     691.0    88.0        73.0        20.0       542.0   \n",
       "33  2018-09-01     687.0    94.0        57.0        39.0       612.0   \n",
       "34  2018-10-01     787.0    78.0       105.0        38.0       578.0   \n",
       "35  2018-11-01     811.0    83.0        71.0        28.0       665.0   \n",
       "36  2018-12-01     682.0   101.0        79.0        21.0       503.0   \n",
       "37  2019-01-01     733.0    85.0        86.0        21.0       498.0   \n",
       "38  2019-02-01     666.0    79.0        60.0        25.0       497.0   \n",
       "39  2019-03-01     703.0    84.0        72.0        24.0       570.0   \n",
       "40  2019-04-01     720.0    85.0        67.0        23.0       603.0   \n",
       "41  2019-05-01     784.0    72.0        79.0        13.0       580.0   \n",
       "42  2019-06-01     665.0    96.0        65.0        21.0       572.0   \n",
       "43  2019-07-01     777.0    95.0        85.0        17.0       603.0   \n",
       "44  2019-08-01     718.0    91.0        69.0        19.0       612.0   \n",
       "45  2019-09-01     736.0    85.0        63.0        32.0       583.0   \n",
       "46  2019-10-01     807.0   105.0        90.0        21.0       625.0   \n",
       "47  2019-11-01     751.0   100.0        69.0        19.0       558.0   \n",
       "48  2019-12-01     796.0    80.0        63.0        20.0       590.0   \n",
       "49  2020-01-01     693.0    67.0        66.0        25.0       545.0   \n",
       "50  2020-02-01     639.0    69.0        60.0        15.0       482.0   \n",
       "51  2020-03-01     575.0    82.0        61.0        18.0       478.0   \n",
       "52  2020-04-01     518.0    83.0        44.0        10.0       392.0   \n",
       "53  2020-05-01     504.0    58.0        50.0        14.0       425.0   \n",
       "54  2020-06-01     685.0   105.0        53.0        29.0       454.0   \n",
       "55  2020-07-01     614.0    53.0        65.0        24.0       514.0   \n",
       "\n",
       "    COLLEGEVILLE  CONSHOHOCKEN  DOUGLASS  EAST GREENVILLE  ...  \\\n",
       "0           43.0          51.0      63.0             18.0  ...   \n",
       "1           58.0         110.0     146.0             31.0  ...   \n",
       "2           89.0          95.0     109.0             27.0  ...   \n",
       "3           52.0         111.0     121.0             13.0  ...   \n",
       "4           52.0          94.0     115.0             29.0  ...   \n",
       "5           56.0          86.0      82.0             13.0  ...   \n",
       "6           31.0          91.0     105.0             22.0  ...   \n",
       "7           50.0         127.0     116.0             34.0  ...   \n",
       "8           57.0         114.0      95.0             28.0  ...   \n",
       "9           80.0         100.0     110.0             23.0  ...   \n",
       "10          67.0         111.0      75.0             27.0  ...   \n",
       "11          47.0          97.0     102.0             33.0  ...   \n",
       "12          52.0          91.0     106.0             43.0  ...   \n",
       "13          46.0         101.0     106.0             20.0  ...   \n",
       "14          46.0          82.0      87.0             27.0  ...   \n",
       "15          68.0         103.0     102.0             20.0  ...   \n",
       "16          57.0         100.0      68.0             24.0  ...   \n",
       "17          51.0         101.0     100.0             23.0  ...   \n",
       "18          58.0          97.0      90.0             21.0  ...   \n",
       "19          48.0         105.0     108.0             18.0  ...   \n",
       "20          44.0          96.0     102.0             18.0  ...   \n",
       "21          55.0          87.0     105.0             28.0  ...   \n",
       "22          50.0         108.0     116.0             22.0  ...   \n",
       "23          31.0          86.0      88.0             22.0  ...   \n",
       "24          40.0         103.0     113.0             24.0  ...   \n",
       "25          54.0         134.0     117.0             34.0  ...   \n",
       "26          60.0          87.0     119.0             21.0  ...   \n",
       "27          85.0         129.0      99.0             31.0  ...   \n",
       "28          60.0         114.0      91.0             22.0  ...   \n",
       "29          43.0          94.0      82.0             20.0  ...   \n",
       "30          51.0         101.0     110.0             24.0  ...   \n",
       "31          44.0         115.0     103.0             23.0  ...   \n",
       "32          62.0         122.0     115.0             21.0  ...   \n",
       "33          44.0         107.0      91.0             25.0  ...   \n",
       "34          64.0          84.0     100.0             22.0  ...   \n",
       "35          52.0         121.0     101.0             33.0  ...   \n",
       "36          50.0         119.0      87.0             30.0  ...   \n",
       "37          59.0         133.0     105.0             17.0  ...   \n",
       "38          57.0          90.0      72.0             14.0  ...   \n",
       "39          57.0          92.0     101.0             20.0  ...   \n",
       "40          49.0         116.0     119.0             12.0  ...   \n",
       "41          41.0         107.0     103.0             23.0  ...   \n",
       "42          64.0         107.0      59.0             37.0  ...   \n",
       "43          46.0         136.0     114.0             43.0  ...   \n",
       "44          51.0         101.0      81.0             20.0  ...   \n",
       "45          57.0          90.0      87.0             15.0  ...   \n",
       "46          60.0         104.0     126.0             21.0  ...   \n",
       "47          65.0          96.0     123.0             21.0  ...   \n",
       "48          43.0         103.0      75.0             30.0  ...   \n",
       "49          47.0         114.0      90.0             22.0  ...   \n",
       "50          41.0          90.0      71.0             16.0  ...   \n",
       "51          43.0          87.0      87.0              9.0  ...   \n",
       "52          42.0          66.0      91.0             16.0  ...   \n",
       "53          31.0          74.0      82.0              9.0  ...   \n",
       "54          28.0          88.0     120.0             35.0  ...   \n",
       "55          38.0          87.0      99.0             22.0  ...   \n",
       "\n",
       "    UPPER MORELAND  UPPER POTTSGROVE  UPPER PROVIDENCE  UPPER SALFORD  \\\n",
       "0            276.0              46.0             191.0           11.0   \n",
       "1            460.0              99.0             285.0           34.0   \n",
       "2            389.0              65.0             281.0           42.0   \n",
       "3            338.0              52.0             248.0           19.0   \n",
       "4            372.0              69.0             256.0           33.0   \n",
       "5            428.0              49.0             245.0           37.0   \n",
       "6            425.0              57.0             279.0           27.0   \n",
       "7            428.0              62.0             263.0           17.0   \n",
       "8            409.0              53.0             304.0           46.0   \n",
       "9            407.0              57.0             311.0           31.0   \n",
       "10           416.0              58.0             321.0           24.0   \n",
       "11           426.0              65.0             294.0           35.0   \n",
       "12           432.0              59.0             318.0           40.0   \n",
       "13           359.0              55.0             301.0           36.0   \n",
       "14           382.0              40.0             277.0           27.0   \n",
       "15           392.0              58.0             296.0           25.0   \n",
       "16           386.0              66.0             265.0           27.0   \n",
       "17           368.0              61.0             309.0           28.0   \n",
       "18           400.0              61.0             305.0           34.0   \n",
       "19           411.0              44.0             283.0           39.0   \n",
       "20           427.0              75.0             274.0           55.0   \n",
       "21           319.0              57.0             252.0           32.0   \n",
       "22           360.0              54.0             334.0           23.0   \n",
       "23           387.0              51.0             265.0           33.0   \n",
       "24           495.0              65.0             337.0           42.0   \n",
       "25           496.0              67.0             326.0           52.0   \n",
       "26           407.0              61.0             285.0           41.0   \n",
       "27           578.0              84.0             318.0           77.0   \n",
       "28           388.0              64.0             263.0           45.0   \n",
       "29           419.0              75.0             277.0           34.0   \n",
       "30           467.0              54.0             348.0           33.0   \n",
       "31           455.0              69.0             274.0           31.0   \n",
       "32           442.0              71.0             271.0           42.0   \n",
       "33           427.0              48.0             276.0           27.0   \n",
       "34           480.0              59.0             353.0           25.0   \n",
       "35           454.0              96.0             368.0           41.0   \n",
       "36           442.0              61.0             302.0           30.0   \n",
       "37           417.0              87.0             305.0           28.0   \n",
       "38           387.0              56.0             326.0           37.0   \n",
       "39           403.0              71.0             295.0           44.0   \n",
       "40           388.0              59.0             302.0           32.0   \n",
       "41           429.0              76.0             344.0           55.0   \n",
       "42           447.0              85.0             273.0           31.0   \n",
       "43           460.0              74.0             284.0           35.0   \n",
       "44           420.0              69.0             300.0           39.0   \n",
       "45           361.0              59.0             294.0           24.0   \n",
       "46           433.0              81.0             292.0           30.0   \n",
       "47           393.0              45.0             326.0           37.0   \n",
       "48           437.0              68.0             289.0           45.0   \n",
       "49           459.0              84.0             320.0           39.0   \n",
       "50           400.0              65.0             240.0           34.0   \n",
       "51           360.0              64.0             228.0           26.0   \n",
       "52           273.0              56.0             188.0           25.0   \n",
       "53           330.0              59.0             251.0           19.0   \n",
       "54           421.0              50.0             262.0           45.0   \n",
       "55           367.0              62.0             248.0           13.0   \n",
       "\n",
       "    WEST CONSHOHOCKEN  WEST NORRITON  WEST POTTSGROVE  WHITEMARSH  WHITPAIN  \\\n",
       "0                81.0          121.0             32.0       213.0     182.0   \n",
       "1               107.0          202.0             75.0       316.0     258.0   \n",
       "2                79.0          182.0             83.0       278.0     271.0   \n",
       "3                76.0          168.0             52.0       277.0     241.0   \n",
       "4                82.0          182.0             48.0       251.0     243.0   \n",
       "5                80.0          202.0             70.0       297.0     254.0   \n",
       "6                96.0          199.0             39.0       332.0     270.0   \n",
       "7                75.0          194.0             59.0       322.0     225.0   \n",
       "8                97.0          204.0             73.0       315.0     227.0   \n",
       "9               106.0          190.0             54.0       273.0     247.0   \n",
       "10              129.0          208.0             65.0       320.0     270.0   \n",
       "11               93.0          189.0             45.0       331.0     300.0   \n",
       "12               80.0          195.0             80.0       304.0     253.0   \n",
       "13               86.0          213.0             46.0       299.0     248.0   \n",
       "14               70.0          165.0             50.0       265.0     178.0   \n",
       "15              103.0          200.0             48.0       280.0     250.0   \n",
       "16               93.0          164.0             53.0       278.0     195.0   \n",
       "17               85.0          202.0             60.0       293.0     255.0   \n",
       "18               93.0          193.0             57.0       347.0     236.0   \n",
       "19              121.0          205.0             58.0       311.0     213.0   \n",
       "20               98.0          197.0             52.0       304.0     217.0   \n",
       "21              121.0          205.0             40.0       316.0     242.0   \n",
       "22               93.0          181.0             60.0       356.0     248.0   \n",
       "23               64.0          197.0             44.0       318.0     227.0   \n",
       "24              110.0          214.0             63.0       331.0     268.0   \n",
       "25               93.0          232.0             70.0       373.0     243.0   \n",
       "26               80.0          191.0             43.0       289.0     204.0   \n",
       "27              146.0          223.0             48.0       421.0     314.0   \n",
       "28               92.0          232.0             59.0       320.0     202.0   \n",
       "29              117.0          203.0             67.0       371.0     250.0   \n",
       "30              100.0          214.0             75.0       347.0     227.0   \n",
       "31               98.0          213.0             52.0       366.0     262.0   \n",
       "32              119.0          206.0             46.0       361.0     216.0   \n",
       "33              106.0          192.0             66.0       355.0     269.0   \n",
       "34              119.0          210.0             49.0       339.0     279.0   \n",
       "35              106.0          222.0             68.0       386.0     254.0   \n",
       "36               96.0          204.0             48.0       323.0     248.0   \n",
       "37              115.0          217.0             75.0       344.0     251.0   \n",
       "38               84.0          171.0             42.0       333.0     262.0   \n",
       "39               88.0          208.0             56.0       317.0     250.0   \n",
       "40               87.0          179.0             54.0       328.0     226.0   \n",
       "41              120.0          234.0             50.0       337.0     266.0   \n",
       "42              100.0          213.0             50.0       387.0     235.0   \n",
       "43              111.0          214.0             75.0       332.0     250.0   \n",
       "44               74.0          232.0             52.0       367.0     252.0   \n",
       "45               87.0          208.0             45.0       315.0     235.0   \n",
       "46              124.0          264.0             53.0       429.0     309.0   \n",
       "47               83.0          198.0             54.0       371.0     302.0   \n",
       "48               94.0          201.0             52.0       343.0     253.0   \n",
       "49               75.0          206.0             73.0       328.0     268.0   \n",
       "50               82.0          192.0             55.0       273.0     220.0   \n",
       "51               69.0          197.0             48.0       230.0     182.0   \n",
       "52               49.0          152.0             27.0       220.0     188.0   \n",
       "53               39.0          192.0             50.0       204.0     171.0   \n",
       "54               69.0          198.0             49.0       299.0     210.0   \n",
       "55               76.0          197.0             46.0       219.0     164.0   \n",
       "\n",
       "    WORCESTER  \n",
       "0        64.0  \n",
       "1        95.0  \n",
       "2        77.0  \n",
       "3        85.0  \n",
       "4       110.0  \n",
       "5        92.0  \n",
       "6        94.0  \n",
       "7        96.0  \n",
       "8        76.0  \n",
       "9       115.0  \n",
       "10      104.0  \n",
       "11      108.0  \n",
       "12      113.0  \n",
       "13       99.0  \n",
       "14      106.0  \n",
       "15      126.0  \n",
       "16       92.0  \n",
       "17      108.0  \n",
       "18      115.0  \n",
       "19      108.0  \n",
       "20      120.0  \n",
       "21      111.0  \n",
       "22      106.0  \n",
       "23      111.0  \n",
       "24      115.0  \n",
       "25      138.0  \n",
       "26      125.0  \n",
       "27      154.0  \n",
       "28      107.0  \n",
       "29      109.0  \n",
       "30      107.0  \n",
       "31      120.0  \n",
       "32      111.0  \n",
       "33      133.0  \n",
       "34      137.0  \n",
       "35      141.0  \n",
       "36      118.0  \n",
       "37      111.0  \n",
       "38      131.0  \n",
       "39      120.0  \n",
       "40      128.0  \n",
       "41       80.0  \n",
       "42      115.0  \n",
       "43      136.0  \n",
       "44      111.0  \n",
       "45      111.0  \n",
       "46      117.0  \n",
       "47      108.0  \n",
       "48      143.0  \n",
       "49      111.0  \n",
       "50      108.0  \n",
       "51       56.0  \n",
       "52       55.0  \n",
       "53       83.0  \n",
       "54       82.0  \n",
       "55       85.0  \n",
       "\n",
       "[56 rows x 63 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "for i in data_row.columns[1:]:\n",
    "   ci = CausalImpact(data_row.loc[:,[i] + [col for col in \\\n",
    "            data_row.columns[1:] if col != i]],\n",
    "         [0,len(data_row['date'])-forecast_horizon-1],\n",
    "         [len(data_row['date'])-forecast_horizon,\n",
    "         len(data_row['date'])-1])\n",
    "   # evaluate the model\n",
    "   y_pred = ci.inferences.loc[(len(data_row['date'])-\\\n",
    "            forecast_horizon):(len(data_row['date'])-1),'preds']\n",
    "   y_pred_list.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'calls911_benchmarks'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = 'results/benchmarks/predicted/' + dataset_name + '_causalimpact.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_pred_df\u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults/nn_model_results/rnn/processed_ensemble_forecasts/sim_10_60_l_he_LSTMcell_cocob_without_stl_decomposition_0.5.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m y_pred_df\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred_df= pd.read_csv(\"results/nn_model_results/rnn/processed_ensemble_forecasts/sim_10_60_l_he_LSTMcell_cocob_without_stl_decomposition_0.5.txt\", header=None)\n",
    "y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_time_series = len(y_pred_df.index) \n",
    "num_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = \"sim\"\n",
    "dataset_name = \"sim_10_60_l_he\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_df_A = pd.read_csv(\"datasets/text_data/\" + dataset_type +  \\\n",
    "    \"/\" + dataset_name + \"_test_actual.csv\")\n",
    "# Reading the original data to calculate the MASE errors\n",
    "y_true_df_B = pd.read_csv(\"datasets/text_data/\" + dataset_type +  \\\n",
    "    \"/\" + dataset_name + \"_train.csv\")\n",
    "data_row_A = y_true_df_A.pivot(index='time', columns='series_id', values='value')\n",
    "data_row_B = y_true_df_B.pivot(index='time', columns='series_id', values='value')\n",
    "data_row = pd.concat([data_row_B, data_row_A],ignore_index=True)\n",
    "data_row_A = data_row_A.T\n",
    "data_row_B = data_row_B.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.629724</td>\n",
       "      <td>6.708806</td>\n",
       "      <td>6.573927</td>\n",
       "      <td>6.606805</td>\n",
       "      <td>6.469340</td>\n",
       "      <td>6.340422</td>\n",
       "      <td>6.221756</td>\n",
       "      <td>6.121868</td>\n",
       "      <td>5.726090</td>\n",
       "      <td>5.626291</td>\n",
       "      <td>5.356708</td>\n",
       "      <td>5.415047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.310534</td>\n",
       "      <td>6.200516</td>\n",
       "      <td>6.156707</td>\n",
       "      <td>6.083326</td>\n",
       "      <td>6.093747</td>\n",
       "      <td>6.103312</td>\n",
       "      <td>6.005089</td>\n",
       "      <td>6.035113</td>\n",
       "      <td>6.110431</td>\n",
       "      <td>6.200161</td>\n",
       "      <td>6.303062</td>\n",
       "      <td>6.606723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.688976</td>\n",
       "      <td>1.691827</td>\n",
       "      <td>1.816579</td>\n",
       "      <td>1.901221</td>\n",
       "      <td>2.242351</td>\n",
       "      <td>2.412698</td>\n",
       "      <td>2.542423</td>\n",
       "      <td>2.654905</td>\n",
       "      <td>2.768438</td>\n",
       "      <td>2.768851</td>\n",
       "      <td>2.828596</td>\n",
       "      <td>2.972987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.135915</td>\n",
       "      <td>4.376139</td>\n",
       "      <td>4.423117</td>\n",
       "      <td>4.481943</td>\n",
       "      <td>4.158025</td>\n",
       "      <td>3.853407</td>\n",
       "      <td>3.646660</td>\n",
       "      <td>3.328388</td>\n",
       "      <td>2.983174</td>\n",
       "      <td>3.024314</td>\n",
       "      <td>2.793119</td>\n",
       "      <td>2.867337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.734496</td>\n",
       "      <td>2.946369</td>\n",
       "      <td>3.119134</td>\n",
       "      <td>3.080817</td>\n",
       "      <td>2.595675</td>\n",
       "      <td>2.472294</td>\n",
       "      <td>2.780016</td>\n",
       "      <td>3.059133</td>\n",
       "      <td>3.283427</td>\n",
       "      <td>2.750075</td>\n",
       "      <td>1.842087</td>\n",
       "      <td>1.239818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9.942944</td>\n",
       "      <td>9.710226</td>\n",
       "      <td>9.403375</td>\n",
       "      <td>9.216754</td>\n",
       "      <td>8.964634</td>\n",
       "      <td>8.856621</td>\n",
       "      <td>8.872820</td>\n",
       "      <td>8.870498</td>\n",
       "      <td>8.984103</td>\n",
       "      <td>9.164515</td>\n",
       "      <td>9.741502</td>\n",
       "      <td>9.684794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.030451</td>\n",
       "      <td>3.039801</td>\n",
       "      <td>3.200372</td>\n",
       "      <td>3.355505</td>\n",
       "      <td>3.887331</td>\n",
       "      <td>4.240273</td>\n",
       "      <td>4.410506</td>\n",
       "      <td>4.705541</td>\n",
       "      <td>4.621328</td>\n",
       "      <td>4.357219</td>\n",
       "      <td>4.171184</td>\n",
       "      <td>4.403385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12.576810</td>\n",
       "      <td>12.840830</td>\n",
       "      <td>12.976706</td>\n",
       "      <td>13.129715</td>\n",
       "      <td>12.596846</td>\n",
       "      <td>12.314832</td>\n",
       "      <td>12.065861</td>\n",
       "      <td>11.859237</td>\n",
       "      <td>11.461786</td>\n",
       "      <td>11.189771</td>\n",
       "      <td>10.925467</td>\n",
       "      <td>10.848368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.442896</td>\n",
       "      <td>10.540597</td>\n",
       "      <td>10.592790</td>\n",
       "      <td>10.791485</td>\n",
       "      <td>10.696101</td>\n",
       "      <td>10.628230</td>\n",
       "      <td>10.654056</td>\n",
       "      <td>10.762355</td>\n",
       "      <td>10.436309</td>\n",
       "      <td>10.143882</td>\n",
       "      <td>9.852842</td>\n",
       "      <td>9.695397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6.132347</td>\n",
       "      <td>6.080677</td>\n",
       "      <td>6.031597</td>\n",
       "      <td>6.025407</td>\n",
       "      <td>6.196535</td>\n",
       "      <td>6.324553</td>\n",
       "      <td>6.439402</td>\n",
       "      <td>6.533186</td>\n",
       "      <td>6.632051</td>\n",
       "      <td>6.616722</td>\n",
       "      <td>6.757506</td>\n",
       "      <td>6.674554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0          1          2          3          4          5   \\\n",
       "0   6.629724   6.708806   6.573927   6.606805   6.469340   6.340422   \n",
       "1   6.310534   6.200516   6.156707   6.083326   6.093747   6.103312   \n",
       "2   1.688976   1.691827   1.816579   1.901221   2.242351   2.412698   \n",
       "3   4.135915   4.376139   4.423117   4.481943   4.158025   3.853407   \n",
       "4   2.734496   2.946369   3.119134   3.080817   2.595675   2.472294   \n",
       "5   9.942944   9.710226   9.403375   9.216754   8.964634   8.856621   \n",
       "6   3.030451   3.039801   3.200372   3.355505   3.887331   4.240273   \n",
       "7  12.576810  12.840830  12.976706  13.129715  12.596846  12.314832   \n",
       "8  10.442896  10.540597  10.592790  10.791485  10.696101  10.628230   \n",
       "9   6.132347   6.080677   6.031597   6.025407   6.196535   6.324553   \n",
       "\n",
       "          6          7          8          9          10         11  \n",
       "0   6.221756   6.121868   5.726090   5.626291   5.356708   5.415047  \n",
       "1   6.005089   6.035113   6.110431   6.200161   6.303062   6.606723  \n",
       "2   2.542423   2.654905   2.768438   2.768851   2.828596   2.972987  \n",
       "3   3.646660   3.328388   2.983174   3.024314   2.793119   2.867337  \n",
       "4   2.780016   3.059133   3.283427   2.750075   1.842087   1.239818  \n",
       "5   8.872820   8.870498   8.984103   9.164515   9.741502   9.684794  \n",
       "6   4.410506   4.705541   4.621328   4.357219   4.171184   4.403385  \n",
       "7  12.065861  11.859237  11.461786  11.189771  10.925467  10.848368  \n",
       "8  10.654056  10.762355  10.436309  10.143882   9.852842   9.695397  \n",
       "9   6.439402   6.533186   6.632051   6.616722   6.757506   6.674554  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_id</th>\n",
       "      <th>time</th>\n",
       "      <th>value</th>\n",
       "      <th>c_t</th>\n",
       "      <th>time_series_length</th>\n",
       "      <th>amount_of_time_series</th>\n",
       "      <th>dgp</th>\n",
       "      <th>te_intervention</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_1_60_linear_heterogeneous</td>\n",
       "      <td>1</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>control</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>linear</td>\n",
       "      <td>heterogeneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_2_60_linear_heterogeneous</td>\n",
       "      <td>1</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>control</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>linear</td>\n",
       "      <td>heterogeneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10_3_60_linear_heterogeneous</td>\n",
       "      <td>1</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>control</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>linear</td>\n",
       "      <td>heterogeneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10_4_60_linear_heterogeneous</td>\n",
       "      <td>1</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>control</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>linear</td>\n",
       "      <td>heterogeneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10_5_60_linear_heterogeneous</td>\n",
       "      <td>1</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>control</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>linear</td>\n",
       "      <td>heterogeneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>10_6_60_linear_heterogeneous</td>\n",
       "      <td>48</td>\n",
       "      <td>9.990315</td>\n",
       "      <td>treated</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>linear</td>\n",
       "      <td>heterogeneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>10_7_60_linear_heterogeneous</td>\n",
       "      <td>48</td>\n",
       "      <td>3.043457</td>\n",
       "      <td>treated</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>linear</td>\n",
       "      <td>heterogeneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>10_8_60_linear_heterogeneous</td>\n",
       "      <td>48</td>\n",
       "      <td>12.360816</td>\n",
       "      <td>treated</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>linear</td>\n",
       "      <td>heterogeneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>10_9_60_linear_heterogeneous</td>\n",
       "      <td>48</td>\n",
       "      <td>10.435537</td>\n",
       "      <td>treated</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>linear</td>\n",
       "      <td>heterogeneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>10_10_60_linear_heterogeneous</td>\n",
       "      <td>48</td>\n",
       "      <td>6.305574</td>\n",
       "      <td>treated</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>linear</td>\n",
       "      <td>heterogeneous</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>480 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         series_id  time      value      c_t  \\\n",
       "0     10_1_60_linear_heterogeneous     1   4.000000  control   \n",
       "1     10_2_60_linear_heterogeneous     1   4.000000  control   \n",
       "2     10_3_60_linear_heterogeneous     1   4.000000  control   \n",
       "3     10_4_60_linear_heterogeneous     1   4.000000  control   \n",
       "4     10_5_60_linear_heterogeneous     1   4.000000  control   \n",
       "..                             ...   ...        ...      ...   \n",
       "475   10_6_60_linear_heterogeneous    48   9.990315  treated   \n",
       "476   10_7_60_linear_heterogeneous    48   3.043457  treated   \n",
       "477   10_8_60_linear_heterogeneous    48  12.360816  treated   \n",
       "478   10_9_60_linear_heterogeneous    48  10.435537  treated   \n",
       "479  10_10_60_linear_heterogeneous    48   6.305574  treated   \n",
       "\n",
       "     time_series_length  amount_of_time_series     dgp te_intervention  \n",
       "0                    60                     10  linear   heterogeneous  \n",
       "1                    60                     10  linear   heterogeneous  \n",
       "2                    60                     10  linear   heterogeneous  \n",
       "3                    60                     10  linear   heterogeneous  \n",
       "4                    60                     10  linear   heterogeneous  \n",
       "..                  ...                    ...     ...             ...  \n",
       "475                  60                     10  linear   heterogeneous  \n",
       "476                  60                     10  linear   heterogeneous  \n",
       "477                  60                     10  linear   heterogeneous  \n",
       "478                  60                     10  linear   heterogeneous  \n",
       "479                  60                     10  linear   heterogeneous  \n",
       "\n",
       "[480 rows x 8 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_df_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>time</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>series_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10_10_60_linear_heterogeneous</th>\n",
       "      <td>6.345929</td>\n",
       "      <td>6.504824</td>\n",
       "      <td>6.825071</td>\n",
       "      <td>6.582064</td>\n",
       "      <td>6.828245</td>\n",
       "      <td>6.541944</td>\n",
       "      <td>6.735053</td>\n",
       "      <td>6.955601</td>\n",
       "      <td>6.737537</td>\n",
       "      <td>6.496371</td>\n",
       "      <td>6.283894</td>\n",
       "      <td>5.751389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10_1_60_linear_heterogeneous</th>\n",
       "      <td>6.378587</td>\n",
       "      <td>6.152957</td>\n",
       "      <td>5.852596</td>\n",
       "      <td>6.090985</td>\n",
       "      <td>6.367565</td>\n",
       "      <td>6.807419</td>\n",
       "      <td>7.006682</td>\n",
       "      <td>7.163756</td>\n",
       "      <td>6.792528</td>\n",
       "      <td>6.509721</td>\n",
       "      <td>6.274091</td>\n",
       "      <td>5.958255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10_2_60_linear_heterogeneous</th>\n",
       "      <td>6.418603</td>\n",
       "      <td>6.869903</td>\n",
       "      <td>7.254626</td>\n",
       "      <td>7.166583</td>\n",
       "      <td>7.134166</td>\n",
       "      <td>6.563448</td>\n",
       "      <td>6.348515</td>\n",
       "      <td>6.497834</td>\n",
       "      <td>6.242976</td>\n",
       "      <td>5.796329</td>\n",
       "      <td>5.375151</td>\n",
       "      <td>4.985877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10_3_60_linear_heterogeneous</th>\n",
       "      <td>1.326354</td>\n",
       "      <td>1.456909</td>\n",
       "      <td>1.594475</td>\n",
       "      <td>2.159289</td>\n",
       "      <td>3.119506</td>\n",
       "      <td>3.677154</td>\n",
       "      <td>3.487978</td>\n",
       "      <td>3.143146</td>\n",
       "      <td>2.858583</td>\n",
       "      <td>2.672392</td>\n",
       "      <td>2.159707</td>\n",
       "      <td>1.447951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10_4_60_linear_heterogeneous</th>\n",
       "      <td>4.238426</td>\n",
       "      <td>4.366579</td>\n",
       "      <td>4.770059</td>\n",
       "      <td>4.225180</td>\n",
       "      <td>4.357252</td>\n",
       "      <td>4.460943</td>\n",
       "      <td>4.765281</td>\n",
       "      <td>4.812774</td>\n",
       "      <td>5.467025</td>\n",
       "      <td>5.657111</td>\n",
       "      <td>5.536041</td>\n",
       "      <td>6.042750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10_5_60_linear_heterogeneous</th>\n",
       "      <td>3.305205</td>\n",
       "      <td>3.460611</td>\n",
       "      <td>3.428427</td>\n",
       "      <td>3.619114</td>\n",
       "      <td>3.786007</td>\n",
       "      <td>3.757834</td>\n",
       "      <td>3.931773</td>\n",
       "      <td>4.299621</td>\n",
       "      <td>4.329619</td>\n",
       "      <td>4.414485</td>\n",
       "      <td>4.389320</td>\n",
       "      <td>4.032280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10_6_60_linear_heterogeneous</th>\n",
       "      <td>9.617934</td>\n",
       "      <td>9.426890</td>\n",
       "      <td>9.041199</td>\n",
       "      <td>8.898806</td>\n",
       "      <td>8.518961</td>\n",
       "      <td>8.204192</td>\n",
       "      <td>7.634701</td>\n",
       "      <td>7.409369</td>\n",
       "      <td>7.507990</td>\n",
       "      <td>6.546740</td>\n",
       "      <td>5.857119</td>\n",
       "      <td>5.331190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10_7_60_linear_heterogeneous</th>\n",
       "      <td>2.880725</td>\n",
       "      <td>2.291242</td>\n",
       "      <td>1.940018</td>\n",
       "      <td>1.746893</td>\n",
       "      <td>1.750235</td>\n",
       "      <td>1.905635</td>\n",
       "      <td>2.479341</td>\n",
       "      <td>2.418841</td>\n",
       "      <td>2.676017</td>\n",
       "      <td>2.850685</td>\n",
       "      <td>3.149199</td>\n",
       "      <td>3.879921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10_8_60_linear_heterogeneous</th>\n",
       "      <td>15.099995</td>\n",
       "      <td>15.001862</td>\n",
       "      <td>12.792860</td>\n",
       "      <td>15.069770</td>\n",
       "      <td>15.151043</td>\n",
       "      <td>15.578149</td>\n",
       "      <td>15.005937</td>\n",
       "      <td>12.775508</td>\n",
       "      <td>12.766202</td>\n",
       "      <td>12.455028</td>\n",
       "      <td>12.585050</td>\n",
       "      <td>12.673118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10_9_60_linear_heterogeneous</th>\n",
       "      <td>10.254313</td>\n",
       "      <td>10.363391</td>\n",
       "      <td>10.535353</td>\n",
       "      <td>11.079612</td>\n",
       "      <td>11.118020</td>\n",
       "      <td>11.333678</td>\n",
       "      <td>11.225567</td>\n",
       "      <td>10.941201</td>\n",
       "      <td>11.048612</td>\n",
       "      <td>9.987595</td>\n",
       "      <td>9.345481</td>\n",
       "      <td>8.400910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "time                                  49         50         51         52  \\\n",
       "series_id                                                                   \n",
       "10_10_60_linear_heterogeneous   6.345929   6.504824   6.825071   6.582064   \n",
       "10_1_60_linear_heterogeneous    6.378587   6.152957   5.852596   6.090985   \n",
       "10_2_60_linear_heterogeneous    6.418603   6.869903   7.254626   7.166583   \n",
       "10_3_60_linear_heterogeneous    1.326354   1.456909   1.594475   2.159289   \n",
       "10_4_60_linear_heterogeneous    4.238426   4.366579   4.770059   4.225180   \n",
       "10_5_60_linear_heterogeneous    3.305205   3.460611   3.428427   3.619114   \n",
       "10_6_60_linear_heterogeneous    9.617934   9.426890   9.041199   8.898806   \n",
       "10_7_60_linear_heterogeneous    2.880725   2.291242   1.940018   1.746893   \n",
       "10_8_60_linear_heterogeneous   15.099995  15.001862  12.792860  15.069770   \n",
       "10_9_60_linear_heterogeneous   10.254313  10.363391  10.535353  11.079612   \n",
       "\n",
       "time                                  53         54         55         56  \\\n",
       "series_id                                                                   \n",
       "10_10_60_linear_heterogeneous   6.828245   6.541944   6.735053   6.955601   \n",
       "10_1_60_linear_heterogeneous    6.367565   6.807419   7.006682   7.163756   \n",
       "10_2_60_linear_heterogeneous    7.134166   6.563448   6.348515   6.497834   \n",
       "10_3_60_linear_heterogeneous    3.119506   3.677154   3.487978   3.143146   \n",
       "10_4_60_linear_heterogeneous    4.357252   4.460943   4.765281   4.812774   \n",
       "10_5_60_linear_heterogeneous    3.786007   3.757834   3.931773   4.299621   \n",
       "10_6_60_linear_heterogeneous    8.518961   8.204192   7.634701   7.409369   \n",
       "10_7_60_linear_heterogeneous    1.750235   1.905635   2.479341   2.418841   \n",
       "10_8_60_linear_heterogeneous   15.151043  15.578149  15.005937  12.775508   \n",
       "10_9_60_linear_heterogeneous   11.118020  11.333678  11.225567  10.941201   \n",
       "\n",
       "time                                  57         58         59         60  \n",
       "series_id                                                                  \n",
       "10_10_60_linear_heterogeneous   6.737537   6.496371   6.283894   5.751389  \n",
       "10_1_60_linear_heterogeneous    6.792528   6.509721   6.274091   5.958255  \n",
       "10_2_60_linear_heterogeneous    6.242976   5.796329   5.375151   4.985877  \n",
       "10_3_60_linear_heterogeneous    2.858583   2.672392   2.159707   1.447951  \n",
       "10_4_60_linear_heterogeneous    5.467025   5.657111   5.536041   6.042750  \n",
       "10_5_60_linear_heterogeneous    4.329619   4.414485   4.389320   4.032280  \n",
       "10_6_60_linear_heterogeneous    7.507990   6.546740   5.857119   5.331190  \n",
       "10_7_60_linear_heterogeneous    2.676017   2.850685   3.149199   3.879921  \n",
       "10_8_60_linear_heterogeneous   12.766202  12.455028  12.585050  12.673118  \n",
       "10_9_60_linear_heterogeneous   11.048612   9.987595   9.345481   8.400910  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_row_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Index' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata_row_B\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_sort_key\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/pandas/core/indexes/base.py:5755\u001b[0m, in \u001b[0;36mIndex.sort_values\u001b[0;34m(self, return_indexer, ascending, na_position, key)\u001b[0m\n\u001b[1;32m   5692\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msort_values\u001b[39m(\n\u001b[1;32m   5693\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5694\u001b[0m     return_indexer: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5697\u001b[0m     key: Callable \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5698\u001b[0m ):\n\u001b[1;32m   5699\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5700\u001b[0m \u001b[38;5;124;03m    Return a sorted copy of the index.\u001b[39;00m\n\u001b[1;32m   5701\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5753\u001b[0m \u001b[38;5;124;03m    (Int64Index([1000, 100, 10, 1], dtype='int64'), array([3, 1, 0, 2]))\u001b[39;00m\n\u001b[1;32m   5754\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5755\u001b[0m     idx \u001b[38;5;241m=\u001b[39m \u001b[43mensure_key_mapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5757\u001b[0m     \u001b[38;5;66;03m# GH 35584. Sort missing values according to na_position kwarg\u001b[39;00m\n\u001b[1;32m   5758\u001b[0m     \u001b[38;5;66;03m# ignore na_position for MultiIndex\u001b[39;00m\n\u001b[1;32m   5759\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCMultiIndex):\n",
      "File \u001b[0;32m/usr/local/anaconda3-2023.03/lib/python3.10/site-packages/pandas/core/sorting.py:566\u001b[0m, in \u001b[0;36mensure_key_mapped\u001b[0;34m(values, key, levels)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, ABCMultiIndex):\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _ensure_key_mapped_multiindex(values, key, level\u001b[38;5;241m=\u001b[39mlevels)\n\u001b[0;32m--> 566\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mkey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(values):\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    569\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-provided `key` function must not change the shape of the array.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[58], line 3\u001b[0m, in \u001b[0;36mcustom_sort_key\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcustom_sort_key\u001b[39m(index):\n\u001b[0;32m----> 3\u001b[0m     parts \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(parts[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Index' object has no attribute 'split'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>time</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>series_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10_1_60_linear_heterogeneous</th>\n",
       "      <td>6.378587</td>\n",
       "      <td>6.152957</td>\n",
       "      <td>5.852596</td>\n",
       "      <td>6.090985</td>\n",
       "      <td>6.367565</td>\n",
       "      <td>6.807419</td>\n",
       "      <td>7.006682</td>\n",
       "      <td>7.163756</td>\n",
       "      <td>6.792528</td>\n",
       "      <td>6.509721</td>\n",
       "      <td>6.274091</td>\n",
       "      <td>5.958255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10_2_60_linear_heterogeneous</th>\n",
       "      <td>6.418603</td>\n",
       "      <td>6.869903</td>\n",
       "      <td>7.254626</td>\n",
       "      <td>7.166583</td>\n",
       "      <td>7.134166</td>\n",
       "      <td>6.563448</td>\n",
       "      <td>6.348515</td>\n",
       "      <td>6.497834</td>\n",
       "      <td>6.242976</td>\n",
       "      <td>5.796329</td>\n",
       "      <td>5.375151</td>\n",
       "      <td>4.985877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10_3_60_linear_heterogeneous</th>\n",
       "      <td>1.326354</td>\n",
       "      <td>1.456909</td>\n",
       "      <td>1.594475</td>\n",
       "      <td>2.159289</td>\n",
       "      <td>3.119506</td>\n",
       "      <td>3.677154</td>\n",
       "      <td>3.487978</td>\n",
       "      <td>3.143146</td>\n",
       "      <td>2.858583</td>\n",
       "      <td>2.672392</td>\n",
       "      <td>2.159707</td>\n",
       "      <td>1.447951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10_4_60_linear_heterogeneous</th>\n",
       "      <td>4.238426</td>\n",
       "      <td>4.366579</td>\n",
       "      <td>4.770059</td>\n",
       "      <td>4.225180</td>\n",
       "      <td>4.357252</td>\n",
       "      <td>4.460943</td>\n",
       "      <td>4.765281</td>\n",
       "      <td>4.812774</td>\n",
       "      <td>5.467025</td>\n",
       "      <td>5.657111</td>\n",
       "      <td>5.536041</td>\n",
       "      <td>6.042750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10_5_60_linear_heterogeneous</th>\n",
       "      <td>3.305205</td>\n",
       "      <td>3.460611</td>\n",
       "      <td>3.428427</td>\n",
       "      <td>3.619114</td>\n",
       "      <td>3.786007</td>\n",
       "      <td>3.757834</td>\n",
       "      <td>3.931773</td>\n",
       "      <td>4.299621</td>\n",
       "      <td>4.329619</td>\n",
       "      <td>4.414485</td>\n",
       "      <td>4.389320</td>\n",
       "      <td>4.032280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10_6_60_linear_heterogeneous</th>\n",
       "      <td>9.617934</td>\n",
       "      <td>9.426890</td>\n",
       "      <td>9.041199</td>\n",
       "      <td>8.898806</td>\n",
       "      <td>8.518961</td>\n",
       "      <td>8.204192</td>\n",
       "      <td>7.634701</td>\n",
       "      <td>7.409369</td>\n",
       "      <td>7.507990</td>\n",
       "      <td>6.546740</td>\n",
       "      <td>5.857119</td>\n",
       "      <td>5.331190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10_7_60_linear_heterogeneous</th>\n",
       "      <td>2.880725</td>\n",
       "      <td>2.291242</td>\n",
       "      <td>1.940018</td>\n",
       "      <td>1.746893</td>\n",
       "      <td>1.750235</td>\n",
       "      <td>1.905635</td>\n",
       "      <td>2.479341</td>\n",
       "      <td>2.418841</td>\n",
       "      <td>2.676017</td>\n",
       "      <td>2.850685</td>\n",
       "      <td>3.149199</td>\n",
       "      <td>3.879921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10_8_60_linear_heterogeneous</th>\n",
       "      <td>15.099995</td>\n",
       "      <td>15.001862</td>\n",
       "      <td>12.792860</td>\n",
       "      <td>15.069770</td>\n",
       "      <td>15.151043</td>\n",
       "      <td>15.578149</td>\n",
       "      <td>15.005937</td>\n",
       "      <td>12.775508</td>\n",
       "      <td>12.766202</td>\n",
       "      <td>12.455028</td>\n",
       "      <td>12.585050</td>\n",
       "      <td>12.673118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10_9_60_linear_heterogeneous</th>\n",
       "      <td>10.254313</td>\n",
       "      <td>10.363391</td>\n",
       "      <td>10.535353</td>\n",
       "      <td>11.079612</td>\n",
       "      <td>11.118020</td>\n",
       "      <td>11.333678</td>\n",
       "      <td>11.225567</td>\n",
       "      <td>10.941201</td>\n",
       "      <td>11.048612</td>\n",
       "      <td>9.987595</td>\n",
       "      <td>9.345481</td>\n",
       "      <td>8.400910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10_10_60_linear_heterogeneous</th>\n",
       "      <td>6.345929</td>\n",
       "      <td>6.504824</td>\n",
       "      <td>6.825071</td>\n",
       "      <td>6.582064</td>\n",
       "      <td>6.828245</td>\n",
       "      <td>6.541944</td>\n",
       "      <td>6.735053</td>\n",
       "      <td>6.955601</td>\n",
       "      <td>6.737537</td>\n",
       "      <td>6.496371</td>\n",
       "      <td>6.283894</td>\n",
       "      <td>5.751389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "time                                  49         50         51         52  \\\n",
       "series_id                                                                   \n",
       "10_1_60_linear_heterogeneous    6.378587   6.152957   5.852596   6.090985   \n",
       "10_2_60_linear_heterogeneous    6.418603   6.869903   7.254626   7.166583   \n",
       "10_3_60_linear_heterogeneous    1.326354   1.456909   1.594475   2.159289   \n",
       "10_4_60_linear_heterogeneous    4.238426   4.366579   4.770059   4.225180   \n",
       "10_5_60_linear_heterogeneous    3.305205   3.460611   3.428427   3.619114   \n",
       "10_6_60_linear_heterogeneous    9.617934   9.426890   9.041199   8.898806   \n",
       "10_7_60_linear_heterogeneous    2.880725   2.291242   1.940018   1.746893   \n",
       "10_8_60_linear_heterogeneous   15.099995  15.001862  12.792860  15.069770   \n",
       "10_9_60_linear_heterogeneous   10.254313  10.363391  10.535353  11.079612   \n",
       "10_10_60_linear_heterogeneous   6.345929   6.504824   6.825071   6.582064   \n",
       "\n",
       "time                                  53         54         55         56  \\\n",
       "series_id                                                                   \n",
       "10_1_60_linear_heterogeneous    6.367565   6.807419   7.006682   7.163756   \n",
       "10_2_60_linear_heterogeneous    7.134166   6.563448   6.348515   6.497834   \n",
       "10_3_60_linear_heterogeneous    3.119506   3.677154   3.487978   3.143146   \n",
       "10_4_60_linear_heterogeneous    4.357252   4.460943   4.765281   4.812774   \n",
       "10_5_60_linear_heterogeneous    3.786007   3.757834   3.931773   4.299621   \n",
       "10_6_60_linear_heterogeneous    8.518961   8.204192   7.634701   7.409369   \n",
       "10_7_60_linear_heterogeneous    1.750235   1.905635   2.479341   2.418841   \n",
       "10_8_60_linear_heterogeneous   15.151043  15.578149  15.005937  12.775508   \n",
       "10_9_60_linear_heterogeneous   11.118020  11.333678  11.225567  10.941201   \n",
       "10_10_60_linear_heterogeneous   6.828245   6.541944   6.735053   6.955601   \n",
       "\n",
       "time                                  57         58         59         60  \n",
       "series_id                                                                  \n",
       "10_1_60_linear_heterogeneous    6.792528   6.509721   6.274091   5.958255  \n",
       "10_2_60_linear_heterogeneous    6.242976   5.796329   5.375151   4.985877  \n",
       "10_3_60_linear_heterogeneous    2.858583   2.672392   2.159707   1.447951  \n",
       "10_4_60_linear_heterogeneous    5.467025   5.657111   5.536041   6.042750  \n",
       "10_5_60_linear_heterogeneous    4.329619   4.414485   4.389320   4.032280  \n",
       "10_6_60_linear_heterogeneous    7.507990   6.546740   5.857119   5.331190  \n",
       "10_7_60_linear_heterogeneous    2.676017   2.850685   3.149199   3.879921  \n",
       "10_8_60_linear_heterogeneous   12.766202  12.455028  12.585050  12.673118  \n",
       "10_9_60_linear_heterogeneous   11.048612   9.987595   9.345481   8.400910  \n",
       "10_10_60_linear_heterogeneous   6.737537   6.496371   6.283894   5.751389  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Custom sorting function to extract the numeric part\n",
    "def custom_sort_key(s):\n",
    "    parts = s.split('_')\n",
    "    return int(parts[1])\n",
    "\n",
    "# Use the custom sorting function as the key\n",
    "data_row_A.loc[sorted(data_row_A.index, key=custom_sort_key),:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>time</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>series_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10_10_60_linear_heterogeneous</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.853568</td>\n",
       "      <td>4.233268</td>\n",
       "      <td>4.354999</td>\n",
       "      <td>4.388512</td>\n",
       "      <td>5.057418</td>\n",
       "      <td>5.187652</td>\n",
       "      <td>4.928620</td>\n",
       "      <td>5.018153</td>\n",
       "      <td>5.618299</td>\n",
       "      <td>...</td>\n",
       "      <td>6.898193</td>\n",
       "      <td>7.066208</td>\n",
       "      <td>6.920042</td>\n",
       "      <td>6.698136</td>\n",
       "      <td>7.053589</td>\n",
       "      <td>7.045750</td>\n",
       "      <td>7.179429</td>\n",
       "      <td>6.821644</td>\n",
       "      <td>6.380702</td>\n",
       "      <td>6.305574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10_1_60_linear_heterogeneous</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.683460</td>\n",
       "      <td>1.788648</td>\n",
       "      <td>1.832475</td>\n",
       "      <td>1.848978</td>\n",
       "      <td>2.591947</td>\n",
       "      <td>3.307043</td>\n",
       "      <td>3.251489</td>\n",
       "      <td>3.391611</td>\n",
       "      <td>3.149968</td>\n",
       "      <td>...</td>\n",
       "      <td>4.521239</td>\n",
       "      <td>5.181117</td>\n",
       "      <td>5.613029</td>\n",
       "      <td>6.903968</td>\n",
       "      <td>7.841402</td>\n",
       "      <td>7.983405</td>\n",
       "      <td>7.823119</td>\n",
       "      <td>7.068117</td>\n",
       "      <td>7.153280</td>\n",
       "      <td>6.751326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10_2_60_linear_heterogeneous</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.974253</td>\n",
       "      <td>2.948683</td>\n",
       "      <td>3.672360</td>\n",
       "      <td>4.015603</td>\n",
       "      <td>4.512820</td>\n",
       "      <td>4.663110</td>\n",
       "      <td>5.300558</td>\n",
       "      <td>5.740905</td>\n",
       "      <td>6.461263</td>\n",
       "      <td>...</td>\n",
       "      <td>6.841498</td>\n",
       "      <td>6.915100</td>\n",
       "      <td>6.751896</td>\n",
       "      <td>6.746984</td>\n",
       "      <td>6.382577</td>\n",
       "      <td>5.755363</td>\n",
       "      <td>5.446491</td>\n",
       "      <td>5.699152</td>\n",
       "      <td>5.941987</td>\n",
       "      <td>6.121536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10_3_60_linear_heterogeneous</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.470470</td>\n",
       "      <td>3.694064</td>\n",
       "      <td>2.805391</td>\n",
       "      <td>2.767085</td>\n",
       "      <td>3.000050</td>\n",
       "      <td>3.037031</td>\n",
       "      <td>3.788352</td>\n",
       "      <td>3.730273</td>\n",
       "      <td>3.909317</td>\n",
       "      <td>...</td>\n",
       "      <td>4.996539</td>\n",
       "      <td>4.710407</td>\n",
       "      <td>4.450999</td>\n",
       "      <td>3.638493</td>\n",
       "      <td>2.947795</td>\n",
       "      <td>2.671857</td>\n",
       "      <td>2.543497</td>\n",
       "      <td>2.249676</td>\n",
       "      <td>1.955008</td>\n",
       "      <td>1.670262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10_4_60_linear_heterogeneous</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.753893</td>\n",
       "      <td>6.042675</td>\n",
       "      <td>5.531434</td>\n",
       "      <td>5.578234</td>\n",
       "      <td>5.160179</td>\n",
       "      <td>4.827438</td>\n",
       "      <td>4.500598</td>\n",
       "      <td>4.671222</td>\n",
       "      <td>4.272978</td>\n",
       "      <td>...</td>\n",
       "      <td>2.288709</td>\n",
       "      <td>2.805814</td>\n",
       "      <td>3.237261</td>\n",
       "      <td>3.708810</td>\n",
       "      <td>3.612122</td>\n",
       "      <td>4.136078</td>\n",
       "      <td>3.892096</td>\n",
       "      <td>4.028035</td>\n",
       "      <td>3.866400</td>\n",
       "      <td>4.075891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10_5_60_linear_heterogeneous</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.059531</td>\n",
       "      <td>3.446069</td>\n",
       "      <td>3.153067</td>\n",
       "      <td>3.138960</td>\n",
       "      <td>2.827227</td>\n",
       "      <td>2.548592</td>\n",
       "      <td>2.833065</td>\n",
       "      <td>3.293848</td>\n",
       "      <td>3.872409</td>\n",
       "      <td>...</td>\n",
       "      <td>1.037410</td>\n",
       "      <td>0.431445</td>\n",
       "      <td>0.364150</td>\n",
       "      <td>0.333668</td>\n",
       "      <td>1.065808</td>\n",
       "      <td>1.816086</td>\n",
       "      <td>2.167989</td>\n",
       "      <td>2.614635</td>\n",
       "      <td>3.029120</td>\n",
       "      <td>2.841879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10_6_60_linear_heterogeneous</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.274956</td>\n",
       "      <td>2.927431</td>\n",
       "      <td>3.431322</td>\n",
       "      <td>4.344974</td>\n",
       "      <td>4.314408</td>\n",
       "      <td>4.935989</td>\n",
       "      <td>5.225889</td>\n",
       "      <td>5.136426</td>\n",
       "      <td>5.465214</td>\n",
       "      <td>...</td>\n",
       "      <td>8.678591</td>\n",
       "      <td>8.716804</td>\n",
       "      <td>9.017681</td>\n",
       "      <td>8.956676</td>\n",
       "      <td>8.436440</td>\n",
       "      <td>8.168121</td>\n",
       "      <td>8.801920</td>\n",
       "      <td>9.594553</td>\n",
       "      <td>10.126045</td>\n",
       "      <td>9.990315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10_7_60_linear_heterogeneous</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.990011</td>\n",
       "      <td>4.110910</td>\n",
       "      <td>4.712161</td>\n",
       "      <td>4.871238</td>\n",
       "      <td>5.454914</td>\n",
       "      <td>6.421490</td>\n",
       "      <td>6.795303</td>\n",
       "      <td>6.207478</td>\n",
       "      <td>5.692347</td>\n",
       "      <td>...</td>\n",
       "      <td>5.982678</td>\n",
       "      <td>5.790201</td>\n",
       "      <td>6.103956</td>\n",
       "      <td>6.568936</td>\n",
       "      <td>6.067351</td>\n",
       "      <td>5.715336</td>\n",
       "      <td>5.164559</td>\n",
       "      <td>4.391121</td>\n",
       "      <td>3.957237</td>\n",
       "      <td>3.043457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10_8_60_linear_heterogeneous</th>\n",
       "      <td>4.0</td>\n",
       "      <td>5.259868</td>\n",
       "      <td>6.583133</td>\n",
       "      <td>7.222388</td>\n",
       "      <td>7.776269</td>\n",
       "      <td>8.587760</td>\n",
       "      <td>8.457479</td>\n",
       "      <td>8.074153</td>\n",
       "      <td>8.020679</td>\n",
       "      <td>7.699433</td>\n",
       "      <td>...</td>\n",
       "      <td>9.297328</td>\n",
       "      <td>9.760329</td>\n",
       "      <td>9.939821</td>\n",
       "      <td>10.532298</td>\n",
       "      <td>11.095576</td>\n",
       "      <td>10.941753</td>\n",
       "      <td>11.136625</td>\n",
       "      <td>11.057943</td>\n",
       "      <td>12.054023</td>\n",
       "      <td>12.360816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10_9_60_linear_heterogeneous</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.552482</td>\n",
       "      <td>4.870682</td>\n",
       "      <td>6.139203</td>\n",
       "      <td>7.102527</td>\n",
       "      <td>8.939321</td>\n",
       "      <td>10.418614</td>\n",
       "      <td>11.148028</td>\n",
       "      <td>11.754114</td>\n",
       "      <td>11.253545</td>\n",
       "      <td>...</td>\n",
       "      <td>8.984903</td>\n",
       "      <td>8.896401</td>\n",
       "      <td>9.125483</td>\n",
       "      <td>10.214215</td>\n",
       "      <td>10.904494</td>\n",
       "      <td>10.817946</td>\n",
       "      <td>10.829086</td>\n",
       "      <td>10.843918</td>\n",
       "      <td>10.505266</td>\n",
       "      <td>10.435537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "time                            1         2         3         4         5   \\\n",
       "series_id                                                                    \n",
       "10_10_60_linear_heterogeneous  4.0  3.853568  4.233268  4.354999  4.388512   \n",
       "10_1_60_linear_heterogeneous   4.0  2.683460  1.788648  1.832475  1.848978   \n",
       "10_2_60_linear_heterogeneous   4.0  2.974253  2.948683  3.672360  4.015603   \n",
       "10_3_60_linear_heterogeneous   4.0  4.470470  3.694064  2.805391  2.767085   \n",
       "10_4_60_linear_heterogeneous   4.0  4.753893  6.042675  5.531434  5.578234   \n",
       "10_5_60_linear_heterogeneous   4.0  3.059531  3.446069  3.153067  3.138960   \n",
       "10_6_60_linear_heterogeneous   4.0  3.274956  2.927431  3.431322  4.344974   \n",
       "10_7_60_linear_heterogeneous   4.0  3.990011  4.110910  4.712161  4.871238   \n",
       "10_8_60_linear_heterogeneous   4.0  5.259868  6.583133  7.222388  7.776269   \n",
       "10_9_60_linear_heterogeneous   4.0  4.552482  4.870682  6.139203  7.102527   \n",
       "\n",
       "time                                 6          7          8          9   \\\n",
       "series_id                                                                  \n",
       "10_10_60_linear_heterogeneous  5.057418   5.187652   4.928620   5.018153   \n",
       "10_1_60_linear_heterogeneous   2.591947   3.307043   3.251489   3.391611   \n",
       "10_2_60_linear_heterogeneous   4.512820   4.663110   5.300558   5.740905   \n",
       "10_3_60_linear_heterogeneous   3.000050   3.037031   3.788352   3.730273   \n",
       "10_4_60_linear_heterogeneous   5.160179   4.827438   4.500598   4.671222   \n",
       "10_5_60_linear_heterogeneous   2.827227   2.548592   2.833065   3.293848   \n",
       "10_6_60_linear_heterogeneous   4.314408   4.935989   5.225889   5.136426   \n",
       "10_7_60_linear_heterogeneous   5.454914   6.421490   6.795303   6.207478   \n",
       "10_8_60_linear_heterogeneous   8.587760   8.457479   8.074153   8.020679   \n",
       "10_9_60_linear_heterogeneous   8.939321  10.418614  11.148028  11.754114   \n",
       "\n",
       "time                                  10  ...        39        40        41  \\\n",
       "series_id                                 ...                                 \n",
       "10_10_60_linear_heterogeneous   5.618299  ...  6.898193  7.066208  6.920042   \n",
       "10_1_60_linear_heterogeneous    3.149968  ...  4.521239  5.181117  5.613029   \n",
       "10_2_60_linear_heterogeneous    6.461263  ...  6.841498  6.915100  6.751896   \n",
       "10_3_60_linear_heterogeneous    3.909317  ...  4.996539  4.710407  4.450999   \n",
       "10_4_60_linear_heterogeneous    4.272978  ...  2.288709  2.805814  3.237261   \n",
       "10_5_60_linear_heterogeneous    3.872409  ...  1.037410  0.431445  0.364150   \n",
       "10_6_60_linear_heterogeneous    5.465214  ...  8.678591  8.716804  9.017681   \n",
       "10_7_60_linear_heterogeneous    5.692347  ...  5.982678  5.790201  6.103956   \n",
       "10_8_60_linear_heterogeneous    7.699433  ...  9.297328  9.760329  9.939821   \n",
       "10_9_60_linear_heterogeneous   11.253545  ...  8.984903  8.896401  9.125483   \n",
       "\n",
       "time                                  42         43         44         45  \\\n",
       "series_id                                                                   \n",
       "10_10_60_linear_heterogeneous   6.698136   7.053589   7.045750   7.179429   \n",
       "10_1_60_linear_heterogeneous    6.903968   7.841402   7.983405   7.823119   \n",
       "10_2_60_linear_heterogeneous    6.746984   6.382577   5.755363   5.446491   \n",
       "10_3_60_linear_heterogeneous    3.638493   2.947795   2.671857   2.543497   \n",
       "10_4_60_linear_heterogeneous    3.708810   3.612122   4.136078   3.892096   \n",
       "10_5_60_linear_heterogeneous    0.333668   1.065808   1.816086   2.167989   \n",
       "10_6_60_linear_heterogeneous    8.956676   8.436440   8.168121   8.801920   \n",
       "10_7_60_linear_heterogeneous    6.568936   6.067351   5.715336   5.164559   \n",
       "10_8_60_linear_heterogeneous   10.532298  11.095576  10.941753  11.136625   \n",
       "10_9_60_linear_heterogeneous   10.214215  10.904494  10.817946  10.829086   \n",
       "\n",
       "time                                  46         47         48  \n",
       "series_id                                                       \n",
       "10_10_60_linear_heterogeneous   6.821644   6.380702   6.305574  \n",
       "10_1_60_linear_heterogeneous    7.068117   7.153280   6.751326  \n",
       "10_2_60_linear_heterogeneous    5.699152   5.941987   6.121536  \n",
       "10_3_60_linear_heterogeneous    2.249676   1.955008   1.670262  \n",
       "10_4_60_linear_heterogeneous    4.028035   3.866400   4.075891  \n",
       "10_5_60_linear_heterogeneous    2.614635   3.029120   2.841879  \n",
       "10_6_60_linear_heterogeneous    9.594553  10.126045   9.990315  \n",
       "10_7_60_linear_heterogeneous    4.391121   3.957237   3.043457  \n",
       "10_8_60_linear_heterogeneous   11.057943  12.054023  12.360816  \n",
       "10_9_60_linear_heterogeneous   10.843918  10.505266  10.435537  \n",
       "\n",
       "[10 rows x 48 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_row_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_SMAPE:0.5624922298939096\n",
      "mean_MASE:2.839062123749925\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# SMAPE\n",
    "time_series_wise_SMAPE = 2 * np.abs(y_pred_df - np.array(data_row_A)) /\\\n",
    "    (np.abs(y_pred_df) + np.abs(np.array(data_row_A)))\n",
    "SMAPEPerSeries = np.mean(time_series_wise_SMAPE, axis=1)\n",
    "mean_SMAPE = np.mean(SMAPEPerSeries)\n",
    "mean_SMAPE_str = f\"mean_SMAPE:{mean_SMAPE}\"\n",
    "print(mean_SMAPE_str)\n",
    "\n",
    "mase_vector = []\n",
    "for i in range(num_time_series):\n",
    "    lagged_diff = [data_row_B.iloc[i,j] - \\\n",
    "                data_row_B.iloc[i,j - forecast_horizon]\\\n",
    "                    for j in range(forecast_horizon,\\\n",
    "                    len(data_row_B.columns))]\n",
    "    mase_vector.append(np.mean(np.abs(np.array(np.array(data_row_A.iloc[i]))\\\n",
    "                - np.array(y_pred_df.iloc[i])) / np.mean(np.abs(lagged_diff))))\n",
    "\n",
    "mean_MASE = np.mean(mase_vector)\n",
    "mean_MASE_str = f\"mean_MASE:{mean_MASE}\"\n",
    "print(mean_MASE_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 00:20:52.388715: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-08 00:20:52.388873: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-08 00:20:52.391844: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-08 00:20:56.312005: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "############ tsmixer\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import benchmarks\n",
    "import time\n",
    "import glob\n",
    "import os\n",
    "from preprocess_scripts.data_loader import DataLoader\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# first sort, then do placebo test\n",
    "def custom_sort_key(s):\n",
    "    parts = s.split('_')\n",
    "    return int(parts[1])\n",
    "\n",
    "def sMAPE_tf(y_true, y_pred):\n",
    "    y_true = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
    "\n",
    "    smape_values = tf.abs(y_pred - y_true) / (tf.abs(y_pred) + tf.abs(y_true)) * 2\n",
    "    smape_per_series = tf.reduce_mean(smape_values, axis=1)\n",
    "    mean_smape = tf.reduce_mean(smape_per_series)\n",
    "\n",
    "    return mean_smape  # Convert TensorFlow tensor to NumPy array for compatibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'sim_10_60_l_he'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_df_A = pd.read_csv(\"./datasets/text_data/\" + dataset_type +  \\\n",
    "        \"/\" + dataset_name + \"_test_actual.csv\")\n",
    "# Reading the original data to calculate the MASE errors\n",
    "y_true_df_B = pd.read_csv(\"./datasets/text_data/\" + dataset_type +  \\\n",
    "        \"/\" + dataset_name + \"_train.csv\")\n",
    "data_row_A = y_true_df_A.pivot(index='time', columns='series_id', values='value')\n",
    "data_row_B = y_true_df_B.pivot(index='time', columns='series_id', values='value')\n",
    "data_row = pd.concat([data_row_B, data_row_A],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_row.to_csv(\"./datasets/text_data/sim/\"+dataset_name+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>series_id</th>\n",
       "      <th>10_10_60_linear_heterogeneous</th>\n",
       "      <th>10_1_60_linear_heterogeneous</th>\n",
       "      <th>10_2_60_linear_heterogeneous</th>\n",
       "      <th>10_3_60_linear_heterogeneous</th>\n",
       "      <th>10_4_60_linear_heterogeneous</th>\n",
       "      <th>10_5_60_linear_heterogeneous</th>\n",
       "      <th>10_6_60_linear_heterogeneous</th>\n",
       "      <th>10_7_60_linear_heterogeneous</th>\n",
       "      <th>10_8_60_linear_heterogeneous</th>\n",
       "      <th>10_9_60_linear_heterogeneous</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>6.345929</td>\n",
       "      <td>6.378587</td>\n",
       "      <td>6.418603</td>\n",
       "      <td>1.326354</td>\n",
       "      <td>4.238426</td>\n",
       "      <td>3.305205</td>\n",
       "      <td>9.617934</td>\n",
       "      <td>2.880725</td>\n",
       "      <td>15.099995</td>\n",
       "      <td>10.254313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>6.504824</td>\n",
       "      <td>6.152957</td>\n",
       "      <td>6.869903</td>\n",
       "      <td>1.456909</td>\n",
       "      <td>4.366579</td>\n",
       "      <td>3.460611</td>\n",
       "      <td>9.426890</td>\n",
       "      <td>2.291242</td>\n",
       "      <td>15.001862</td>\n",
       "      <td>10.363391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>6.825071</td>\n",
       "      <td>5.852596</td>\n",
       "      <td>7.254626</td>\n",
       "      <td>1.594475</td>\n",
       "      <td>4.770059</td>\n",
       "      <td>3.428427</td>\n",
       "      <td>9.041199</td>\n",
       "      <td>1.940018</td>\n",
       "      <td>12.792860</td>\n",
       "      <td>10.535353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>6.582064</td>\n",
       "      <td>6.090985</td>\n",
       "      <td>7.166583</td>\n",
       "      <td>2.159289</td>\n",
       "      <td>4.225180</td>\n",
       "      <td>3.619114</td>\n",
       "      <td>8.898806</td>\n",
       "      <td>1.746893</td>\n",
       "      <td>15.069770</td>\n",
       "      <td>11.079612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>6.828245</td>\n",
       "      <td>6.367565</td>\n",
       "      <td>7.134166</td>\n",
       "      <td>3.119506</td>\n",
       "      <td>4.357252</td>\n",
       "      <td>3.786007</td>\n",
       "      <td>8.518961</td>\n",
       "      <td>1.750235</td>\n",
       "      <td>15.151043</td>\n",
       "      <td>11.118020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>6.541944</td>\n",
       "      <td>6.807419</td>\n",
       "      <td>6.563448</td>\n",
       "      <td>3.677154</td>\n",
       "      <td>4.460943</td>\n",
       "      <td>3.757834</td>\n",
       "      <td>8.204192</td>\n",
       "      <td>1.905635</td>\n",
       "      <td>15.578149</td>\n",
       "      <td>11.333678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>6.735053</td>\n",
       "      <td>7.006682</td>\n",
       "      <td>6.348515</td>\n",
       "      <td>3.487978</td>\n",
       "      <td>4.765281</td>\n",
       "      <td>3.931773</td>\n",
       "      <td>7.634701</td>\n",
       "      <td>2.479341</td>\n",
       "      <td>15.005937</td>\n",
       "      <td>11.225567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>6.955601</td>\n",
       "      <td>7.163756</td>\n",
       "      <td>6.497834</td>\n",
       "      <td>3.143146</td>\n",
       "      <td>4.812774</td>\n",
       "      <td>4.299621</td>\n",
       "      <td>7.409369</td>\n",
       "      <td>2.418841</td>\n",
       "      <td>12.775508</td>\n",
       "      <td>10.941201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>6.737537</td>\n",
       "      <td>6.792528</td>\n",
       "      <td>6.242976</td>\n",
       "      <td>2.858583</td>\n",
       "      <td>5.467025</td>\n",
       "      <td>4.329619</td>\n",
       "      <td>7.507990</td>\n",
       "      <td>2.676017</td>\n",
       "      <td>12.766202</td>\n",
       "      <td>11.048612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>6.496371</td>\n",
       "      <td>6.509721</td>\n",
       "      <td>5.796329</td>\n",
       "      <td>2.672392</td>\n",
       "      <td>5.657111</td>\n",
       "      <td>4.414485</td>\n",
       "      <td>6.546740</td>\n",
       "      <td>2.850685</td>\n",
       "      <td>12.455028</td>\n",
       "      <td>9.987595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>6.283894</td>\n",
       "      <td>6.274091</td>\n",
       "      <td>5.375151</td>\n",
       "      <td>2.159707</td>\n",
       "      <td>5.536041</td>\n",
       "      <td>4.389320</td>\n",
       "      <td>5.857119</td>\n",
       "      <td>3.149199</td>\n",
       "      <td>12.585050</td>\n",
       "      <td>9.345481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>5.751389</td>\n",
       "      <td>5.958255</td>\n",
       "      <td>4.985877</td>\n",
       "      <td>1.447951</td>\n",
       "      <td>6.042750</td>\n",
       "      <td>4.032280</td>\n",
       "      <td>5.331190</td>\n",
       "      <td>3.879921</td>\n",
       "      <td>12.673118</td>\n",
       "      <td>8.400910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "series_id  10_10_60_linear_heterogeneous  10_1_60_linear_heterogeneous  \\\n",
       "time                                                                     \n",
       "49                              6.345929                      6.378587   \n",
       "50                              6.504824                      6.152957   \n",
       "51                              6.825071                      5.852596   \n",
       "52                              6.582064                      6.090985   \n",
       "53                              6.828245                      6.367565   \n",
       "54                              6.541944                      6.807419   \n",
       "55                              6.735053                      7.006682   \n",
       "56                              6.955601                      7.163756   \n",
       "57                              6.737537                      6.792528   \n",
       "58                              6.496371                      6.509721   \n",
       "59                              6.283894                      6.274091   \n",
       "60                              5.751389                      5.958255   \n",
       "\n",
       "series_id  10_2_60_linear_heterogeneous  10_3_60_linear_heterogeneous  \\\n",
       "time                                                                    \n",
       "49                             6.418603                      1.326354   \n",
       "50                             6.869903                      1.456909   \n",
       "51                             7.254626                      1.594475   \n",
       "52                             7.166583                      2.159289   \n",
       "53                             7.134166                      3.119506   \n",
       "54                             6.563448                      3.677154   \n",
       "55                             6.348515                      3.487978   \n",
       "56                             6.497834                      3.143146   \n",
       "57                             6.242976                      2.858583   \n",
       "58                             5.796329                      2.672392   \n",
       "59                             5.375151                      2.159707   \n",
       "60                             4.985877                      1.447951   \n",
       "\n",
       "series_id  10_4_60_linear_heterogeneous  10_5_60_linear_heterogeneous  \\\n",
       "time                                                                    \n",
       "49                             4.238426                      3.305205   \n",
       "50                             4.366579                      3.460611   \n",
       "51                             4.770059                      3.428427   \n",
       "52                             4.225180                      3.619114   \n",
       "53                             4.357252                      3.786007   \n",
       "54                             4.460943                      3.757834   \n",
       "55                             4.765281                      3.931773   \n",
       "56                             4.812774                      4.299621   \n",
       "57                             5.467025                      4.329619   \n",
       "58                             5.657111                      4.414485   \n",
       "59                             5.536041                      4.389320   \n",
       "60                             6.042750                      4.032280   \n",
       "\n",
       "series_id  10_6_60_linear_heterogeneous  10_7_60_linear_heterogeneous  \\\n",
       "time                                                                    \n",
       "49                             9.617934                      2.880725   \n",
       "50                             9.426890                      2.291242   \n",
       "51                             9.041199                      1.940018   \n",
       "52                             8.898806                      1.746893   \n",
       "53                             8.518961                      1.750235   \n",
       "54                             8.204192                      1.905635   \n",
       "55                             7.634701                      2.479341   \n",
       "56                             7.409369                      2.418841   \n",
       "57                             7.507990                      2.676017   \n",
       "58                             6.546740                      2.850685   \n",
       "59                             5.857119                      3.149199   \n",
       "60                             5.331190                      3.879921   \n",
       "\n",
       "series_id  10_8_60_linear_heterogeneous  10_9_60_linear_heterogeneous  \n",
       "time                                                                   \n",
       "49                            15.099995                     10.254313  \n",
       "50                            15.001862                     10.363391  \n",
       "51                            12.792860                     10.535353  \n",
       "52                            15.069770                     11.079612  \n",
       "53                            15.151043                     11.118020  \n",
       "54                            15.578149                     11.333678  \n",
       "55                            15.005937                     11.225567  \n",
       "56                            12.775508                     10.941201  \n",
       "57                            12.766202                     11.048612  \n",
       "58                            12.455028                      9.987595  \n",
       "59                            12.585050                      9.345481  \n",
       "60                            12.673118                      8.400910  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_row_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def tsmixer_eval(dataset_name,dataset_type, forecast_horizon):\n",
    "    if dataset_type == \"sim\":\n",
    "        y_true_df_A = pd.read_csv(\"./datasets/text_data/\" + dataset_type +  \\\n",
    "                \"/\" + dataset_name + \"_test_actual.csv\")\n",
    "        # Reading the original data to calculate the MASE errors\n",
    "        y_true_df_B = pd.read_csv(\"./datasets/text_data/\" + dataset_type +  \\\n",
    "                \"/\" + dataset_name + \"_train.csv\")\n",
    "        data_row_A = y_true_df_A.pivot(index='time', columns='series_id', values='value')\n",
    "        data_row_B = y_true_df_B.pivot(index='time', columns='series_id', values='value')\n",
    "        data_row_A = data_row_A.loc[:,sorted(data_row_A.columns, key=custom_sort_key)]\n",
    "        data_row_B = data_row_B.loc[:,sorted(data_row_B.columns, key=custom_sort_key)]\n",
    "        data_row = pd.concat([data_row_B, data_row_A],ignore_index=True)\n",
    "        data_row.to_csv(\"./datasets/text_data/sim/\"+dataset_name+\".csv\")\n",
    "        data_row_A = data_row_A.T\n",
    "        data_row_B = data_row_B.T\n",
    "        \n",
    "        \n",
    "    if dataset_type == \"calls911\":\n",
    "        data_row = pd.read_csv('./datasets/text_data/' + dataset_type\\\n",
    "                            + '/'+dataset_name+'.csv')[1:]\n",
    "        y_true_df_A = data_row.iloc[len(data_row['date'])-forecast_horizon:, 1:].T\n",
    "        y_true_df_B = data_row.iloc[:len(data_row['date'])-forecast_horizon, 1:].T\n",
    "        data_row_A = y_true_df_A\n",
    "        # print(data_row_A)\n",
    "        data_row_B = y_true_df_B\n",
    "    \n",
    "    feature_type='M'\n",
    "    norm_type = 'B'\n",
    "    activation = 'relu'\n",
    "    dropout = 0.05\n",
    "    n_block=2\n",
    "    batch_size=31\n",
    "    no_of_series = len(data_row_B.index) \n",
    "    patience = 5\n",
    "    train_epochs = 100\n",
    "    learning_rate = 0.01\n",
    "    seasonality_period = 12\n",
    "    \n",
    "    input_size = int(seasonality_period * 1.25)\n",
    "    checkpoint_dir = './checkpoints/'\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        data=dataset_name,\n",
    "        batch_size=batch_size,\n",
    "        seq_len=input_size,\n",
    "        pred_len=forecast_horizon,\n",
    "        feature_type=feature_type,\n",
    "    )\n",
    "    train_data = data_loader.get_train()\n",
    "    val_data = data_loader.get_val()\n",
    "    test_data = data_loader.get_test()\n",
    "\n",
    "    build_model = getattr(benchmarks, 'tsmixer').build_model\n",
    "    model = build_model(\n",
    "        input_shape=(input_size, data_loader.n_feature),\n",
    "        pred_len=forecast_horizon,\n",
    "        norm_type=norm_type,\n",
    "        activation=activation,\n",
    "        dropout=dropout,\n",
    "        n_block=n_block,\n",
    "        ff_dim=no_of_series,\n",
    "        target_slice=data_loader.target_slice,\n",
    "    )\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=sMAPE_tf, metrics=[sMAPE_tf])\n",
    "\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'{dataset_name}_best')\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "    early_stop_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=patience\n",
    "    )\n",
    "    start_training_time = time.time()\n",
    "    history = model.fit(\n",
    "        train_data,\n",
    "        epochs=train_epochs,\n",
    "        validation_data=val_data,\n",
    "        callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    )\n",
    "    end_training_time = time.time()\n",
    "    elasped_training_time = end_training_time - start_training_time\n",
    "    print(f'Training finished in {elasped_training_time} secconds')\n",
    "\n",
    "    # evaluate best model\n",
    "    best_epoch = np.argmin(history.history['val_loss'])\n",
    "    model.load_weights(checkpoint_path)\n",
    "    test_result = model.evaluate(test_data)\n",
    "    test_smape = test_result[1]\n",
    "    print(test_smape)\n",
    "\n",
    "    for f in glob.glob(checkpoint_path + '*'):\n",
    "        os.remove(f)\n",
    "    \n",
    "    prediction = model.predict(test_data)\n",
    "    y_pred = data_loader.inverse_transform(prediction[0])\n",
    "\n",
    "    output = './results/benchmarks/predicted/' + dataset_name +\\\n",
    "            '_tsmixer.txt'\n",
    "    y_pred_df = pd.DataFrame(y_pred.T)\n",
    "    y_pred_df.to_csv(output, index=False, header=False)\n",
    "    \n",
    "    errors_directory = './results/benchmarks/errors/'\n",
    "\n",
    "    errors_file_name_mean_median = 'mean_median_' + dataset_name + '_tsmixer'\n",
    "    SMAPE_file_name_all_errors = 'all_smape_errors_' + dataset_name + '_tsmixer'\n",
    "    MASE_file_name_all_errors = 'all_mase_errors_' + dataset_name + '_tsmixer'\n",
    "\n",
    "    errors_file_full_name_mean_median = errors_directory + errors_file_name_mean_median+'.txt'\n",
    "    SMAPE_file_full_name_all_errors = errors_directory + SMAPE_file_name_all_errors\n",
    "    MASE_file_full_name_all_errors = errors_directory + MASE_file_name_all_errors\n",
    "    \n",
    "    \n",
    "\n",
    "    # SMAPE\n",
    "    time_series_wise_SMAPE = 2 * np.abs(y_pred_df - np.array(data_row_A)) /\\\n",
    "        (np.abs(y_pred_df) + np.abs(np.array(data_row_A)))\n",
    "    SMAPEPerSeries = np.mean(time_series_wise_SMAPE, axis=1)\n",
    "    mean_SMAPE = np.mean(SMAPEPerSeries)\n",
    "    mean_SMAPE_str = f\"mean_SMAPE:{mean_SMAPE}\"\n",
    "    print(mean_SMAPE_str)\n",
    "    np.savetxt(SMAPE_file_full_name_all_errors+'.txt', SMAPEPerSeries, delimiter=\",\", fmt='%f')\n",
    "    \n",
    "    mase_vector = []\n",
    "    for i in range(no_of_series):\n",
    "        lagged_diff = [data_row_B.iloc[i,j] - \\\n",
    "                   data_row_B.iloc[i,j - forecast_horizon]\\\n",
    "                      for j in range(forecast_horizon,\\\n",
    "                        len(data_row_B.columns))]\n",
    "        mase_vector.append(np.mean(np.abs(np.array(np.array(data_row_A.iloc[i]))\\\n",
    "                 - np.array(y_pred_df.iloc[i])) / np.mean(np.abs(lagged_diff))))\n",
    "\n",
    "    mean_MASE = np.mean(mase_vector)\n",
    "    mean_MASE_str = f\"mean_MASE:{mean_MASE}\"\n",
    "    print(mean_MASE_str)\n",
    "\n",
    "    np.savetxt(MASE_file_full_name_all_errors+'.txt', mase_vector, delimiter=\",\", fmt='%f')\n",
    "\n",
    "    # Writing the SMAPE results to file\n",
    "    with open(errors_file_full_name_mean_median, 'w') as f:\n",
    "        # f.write('\\n'.join([mean_SMAPE_str, median_SMAPE_str, std_SMAPE_str]))\n",
    "        f.write('\\n'.join([mean_SMAPE_str]))\n",
    "\n",
    "    # Writing the MASE results to file\n",
    "    with open(errors_file_full_name_mean_median, 'a') as f:\n",
    "        # f.write('\\n'.join([mean_MASE_str, median_MASE_str, std_MASE_str]))\n",
    "        f.write('\\n'.join([mean_MASE_str]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 22 22\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5195 - sMAPE_tf: 1.5195\n",
      "Epoch 1: val_loss improved from inf to 1.60300, saving model to ./checkpoints/calls911_benchmarks_best\n",
      "1/1 [==============================] - 6s 6s/step - loss: 1.5195 - sMAPE_tf: 1.5195 - val_loss: 1.6030 - val_sMAPE_tf: 1.6030\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4994 - sMAPE_tf: 1.4994\n",
      "Epoch 2: val_loss improved from 1.60300 to 1.55803, saving model to ./checkpoints/calls911_benchmarks_best\n",
      "1/1 [==============================] - 0s 364ms/step - loss: 1.4994 - sMAPE_tf: 1.4994 - val_loss: 1.5580 - val_sMAPE_tf: 1.5580\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4801 - sMAPE_tf: 1.4801\n",
      "Epoch 3: val_loss improved from 1.55803 to 1.55271, saving model to ./checkpoints/calls911_benchmarks_best\n",
      "1/1 [==============================] - 0s 355ms/step - loss: 1.4801 - sMAPE_tf: 1.4801 - val_loss: 1.5527 - val_sMAPE_tf: 1.5527\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4683 - sMAPE_tf: 1.4683\n",
      "Epoch 4: val_loss improved from 1.55271 to 1.55116, saving model to ./checkpoints/calls911_benchmarks_best\n",
      "1/1 [==============================] - 0s 414ms/step - loss: 1.4683 - sMAPE_tf: 1.4683 - val_loss: 1.5512 - val_sMAPE_tf: 1.5512\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4693 - sMAPE_tf: 1.4693\n",
      "Epoch 5: val_loss improved from 1.55116 to 1.53517, saving model to ./checkpoints/calls911_benchmarks_best\n",
      "1/1 [==============================] - 0s 347ms/step - loss: 1.4693 - sMAPE_tf: 1.4693 - val_loss: 1.5352 - val_sMAPE_tf: 1.5352\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4526 - sMAPE_tf: 1.4526\n",
      "Epoch 6: val_loss improved from 1.53517 to 1.51931, saving model to ./checkpoints/calls911_benchmarks_best\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 1.4526 - sMAPE_tf: 1.4526 - val_loss: 1.5193 - val_sMAPE_tf: 1.5193\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4526 - sMAPE_tf: 1.4526\n",
      "Epoch 7: val_loss improved from 1.51931 to 1.50672, saving model to ./checkpoints/calls911_benchmarks_best\n",
      "1/1 [==============================] - 0s 355ms/step - loss: 1.4526 - sMAPE_tf: 1.4526 - val_loss: 1.5067 - val_sMAPE_tf: 1.5067\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4408 - sMAPE_tf: 1.4408\n",
      "Epoch 8: val_loss did not improve from 1.50672\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 1.4408 - sMAPE_tf: 1.4408 - val_loss: 1.5103 - val_sMAPE_tf: 1.5103\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4286 - sMAPE_tf: 1.4286\n",
      "Epoch 9: val_loss did not improve from 1.50672\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 1.4286 - sMAPE_tf: 1.4286 - val_loss: 1.5155 - val_sMAPE_tf: 1.5155\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4224 - sMAPE_tf: 1.4224\n",
      "Epoch 10: val_loss did not improve from 1.50672\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 1.4224 - sMAPE_tf: 1.4224 - val_loss: 1.5225 - val_sMAPE_tf: 1.5225\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4206 - sMAPE_tf: 1.4206\n",
      "Epoch 11: val_loss improved from 1.50672 to 1.49972, saving model to ./checkpoints/calls911_benchmarks_best\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 1.4206 - sMAPE_tf: 1.4206 - val_loss: 1.4997 - val_sMAPE_tf: 1.4997\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4060 - sMAPE_tf: 1.4060\n",
      "Epoch 12: val_loss improved from 1.49972 to 1.49788, saving model to ./checkpoints/calls911_benchmarks_best\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 1.4060 - sMAPE_tf: 1.4060 - val_loss: 1.4979 - val_sMAPE_tf: 1.4979\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3996 - sMAPE_tf: 1.3996\n",
      "Epoch 13: val_loss did not improve from 1.49788\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 1.3996 - sMAPE_tf: 1.3996 - val_loss: 1.5114 - val_sMAPE_tf: 1.5114\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3907 - sMAPE_tf: 1.3907\n",
      "Epoch 14: val_loss did not improve from 1.49788\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 1.3907 - sMAPE_tf: 1.3907 - val_loss: 1.5189 - val_sMAPE_tf: 1.5189\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3827 - sMAPE_tf: 1.3827\n",
      "Epoch 15: val_loss did not improve from 1.49788\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 1.3827 - sMAPE_tf: 1.3827 - val_loss: 1.5129 - val_sMAPE_tf: 1.5129\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3805 - sMAPE_tf: 1.3805\n",
      "Epoch 16: val_loss did not improve from 1.49788\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 1.3805 - sMAPE_tf: 1.3805 - val_loss: 1.5251 - val_sMAPE_tf: 1.5251\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3718 - sMAPE_tf: 1.3718\n",
      "Epoch 17: val_loss did not improve from 1.49788\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 1.3718 - sMAPE_tf: 1.3718 - val_loss: 1.5366 - val_sMAPE_tf: 1.5366\n",
      "Training finished in 10.930486679077148 secconds\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 1.4865 - sMAPE_tf: 1.4865\n",
      "1.4865413904190063\n",
      "1/1 [==============================] - 0s 419ms/step\n",
      "mean_SMAPE:0.3698316963184064\n",
      "mean_MASE:1.9912033489825167\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'calls911_benchmarks'\n",
    "dataset_type = 'calls911'\n",
    "forecast_horizon=7\n",
    "\n",
    "tsmixer_eval(dataset_name,dataset_type,forecast_horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim_10_60_l_he\n",
      "sim_10_60_l_ho\n",
      "sim_10_60_nl_he\n",
      "sim_10_60_nl_ho\n",
      "sim_10_222_l_he\n",
      "sim_10_222_l_ho\n",
      "sim_10_222_nl_he\n",
      "sim_10_222_nl_ho\n",
      "sim_101_60_l_he\n",
      "sim_101_60_l_ho\n",
      "sim_101_60_nl_he\n",
      "sim_101_60_nl_ho\n",
      "sim_101_222_l_he\n",
      "sim_101_222_l_ho\n",
      "sim_101_222_nl_he\n",
      "sim_101_222_nl_ho\n",
      "sim_500_60_l_he\n",
      "sim_500_60_l_ho\n",
      "sim_500_60_nl_he\n",
      "sim_500_60_nl_ho\n",
      "sim_500_222_l_he\n",
      "sim_500_222_l_ho\n",
      "sim_500_222_nl_he\n",
      "sim_500_222_nl_ho\n"
     ]
    }
   ],
   "source": [
    "dataset_name_test = ['sim_10_60_l_he', 'sim_10_60_l_ho',\\\n",
    "                     'sim_10_60_nl_he', 'sim_10_60_nl_ho',\\\n",
    "                     'sim_10_222_l_he', 'sim_10_222_l_ho',\\\n",
    "                     'sim_10_222_nl_he', 'sim_10_222_nl_ho',\\\n",
    "                     'sim_101_60_l_he', 'sim_101_60_l_ho',\\\n",
    "                     'sim_101_60_nl_he', 'sim_101_60_nl_ho',\\\n",
    "                     'sim_101_222_l_he', 'sim_101_222_l_ho',\\\n",
    "                     'sim_101_222_nl_he', 'sim_101_222_nl_ho',\\\n",
    "                     'sim_500_60_l_he', 'sim_500_60_l_ho',\\\n",
    "                     'sim_500_60_nl_he', 'sim_500_60_nl_ho',\\\n",
    "                     'sim_500_222_l_he', 'sim_500_222_l_ho',\\\n",
    "                     'sim_500_222_nl_he', 'sim_500_222_nl_ho']\n",
    "dataset_type = 'sim'\n",
    "forecast_horizon=12\n",
    "\n",
    "for i in dataset_name_test:\n",
    "    print(i)\n",
    "    tsmixer_eval(i,dataset_type,forecast_horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim_10_60_l_he\n",
      "35 27 27\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5557 - sMAPE_tf: 1.5557\n",
      "Epoch 1: val_loss improved from inf to 1.42770, saving model to ./checkpoints/sim_10_60_l_he_best\n",
      "1/1 [==============================] - 5s 5s/step - loss: 1.5557 - sMAPE_tf: 1.5557 - val_loss: 1.4277 - val_sMAPE_tf: 1.4277\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5246 - sMAPE_tf: 1.5246\n",
      "Epoch 2: val_loss did not improve from 1.42770\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 1.5246 - sMAPE_tf: 1.5246 - val_loss: 1.4536 - val_sMAPE_tf: 1.4536\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5050 - sMAPE_tf: 1.5050\n",
      "Epoch 3: val_loss did not improve from 1.42770\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 1.5050 - sMAPE_tf: 1.5050 - val_loss: 1.4632 - val_sMAPE_tf: 1.4632\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4967 - sMAPE_tf: 1.4967\n",
      "Epoch 4: val_loss did not improve from 1.42770\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 1.4967 - sMAPE_tf: 1.4967 - val_loss: 1.4742 - val_sMAPE_tf: 1.4742\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4727 - sMAPE_tf: 1.4727\n",
      "Epoch 5: val_loss did not improve from 1.42770\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 1.4727 - sMAPE_tf: 1.4727 - val_loss: 1.4791 - val_sMAPE_tf: 1.4791\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4628 - sMAPE_tf: 1.4628\n",
      "Epoch 6: val_loss did not improve from 1.42770\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 1.4628 - sMAPE_tf: 1.4628 - val_loss: 1.4834 - val_sMAPE_tf: 1.4834\n",
      "Training finished in 7.042247533798218 secconds\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 1.4715 - sMAPE_tf: 1.4715\n",
      "1.4715226888656616\n",
      "1/1 [==============================] - 1s 899ms/step\n",
      "mean_SMAPE:0.6854110822168675\n",
      "mean_MASE:3.2533837455736916\n",
      "sim_10_60_l_ho\n",
      "35 27 27\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4202 - sMAPE_tf: 1.4202\n",
      "Epoch 1: val_loss improved from inf to 1.35567, saving model to ./checkpoints/sim_10_60_l_ho_best\n",
      "1/1 [==============================] - 9s 9s/step - loss: 1.4202 - sMAPE_tf: 1.4202 - val_loss: 1.3557 - val_sMAPE_tf: 1.3557\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3789 - sMAPE_tf: 1.3789\n",
      "Epoch 2: val_loss did not improve from 1.35567\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 1.3789 - sMAPE_tf: 1.3789 - val_loss: 1.3721 - val_sMAPE_tf: 1.3721\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3658 - sMAPE_tf: 1.3658\n",
      "Epoch 3: val_loss did not improve from 1.35567\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 1.3658 - sMAPE_tf: 1.3658 - val_loss: 1.3813 - val_sMAPE_tf: 1.3813\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3313 - sMAPE_tf: 1.3313\n",
      "Epoch 4: val_loss did not improve from 1.35567\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 1.3313 - sMAPE_tf: 1.3313 - val_loss: 1.3780 - val_sMAPE_tf: 1.3780\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2984 - sMAPE_tf: 1.2984\n",
      "Epoch 5: val_loss did not improve from 1.35567\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 1.2984 - sMAPE_tf: 1.2984 - val_loss: 1.3826 - val_sMAPE_tf: 1.3826\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3095 - sMAPE_tf: 1.3095\n",
      "Epoch 6: val_loss did not improve from 1.35567\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 1.3095 - sMAPE_tf: 1.3095 - val_loss: 1.3818 - val_sMAPE_tf: 1.3818\n",
      "Training finished in 9.980454444885254 secconds\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 1.3628 - sMAPE_tf: 1.3628\n",
      "1.3628411293029785\n",
      "1/1 [==============================] - 1s 572ms/step\n",
      "mean_SMAPE:0.8122283295695116\n",
      "mean_MASE:2.877804015584885\n",
      "sim_10_60_nl_he\n",
      "35 27 27\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4471 - sMAPE_tf: 1.4471\n",
      "Epoch 1: val_loss improved from inf to 1.40001, saving model to ./checkpoints/sim_10_60_nl_he_best\n",
      "1/1 [==============================] - 5s 5s/step - loss: 1.4471 - sMAPE_tf: 1.4471 - val_loss: 1.4000 - val_sMAPE_tf: 1.4000\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4545 - sMAPE_tf: 1.4545\n",
      "Epoch 2: val_loss improved from 1.40001 to 1.39276, saving model to ./checkpoints/sim_10_60_nl_he_best\n",
      "1/1 [==============================] - 0s 417ms/step - loss: 1.4545 - sMAPE_tf: 1.4545 - val_loss: 1.3928 - val_sMAPE_tf: 1.3928\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3934 - sMAPE_tf: 1.3934\n",
      "Epoch 3: val_loss did not improve from 1.39276\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 1.3934 - sMAPE_tf: 1.3934 - val_loss: 1.3982 - val_sMAPE_tf: 1.3982\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3796 - sMAPE_tf: 1.3796\n",
      "Epoch 4: val_loss did not improve from 1.39276\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 1.3796 - sMAPE_tf: 1.3796 - val_loss: 1.3995 - val_sMAPE_tf: 1.3995\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3647 - sMAPE_tf: 1.3647\n",
      "Epoch 5: val_loss did not improve from 1.39276\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 1.3647 - sMAPE_tf: 1.3647 - val_loss: 1.4009 - val_sMAPE_tf: 1.4009\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3637 - sMAPE_tf: 1.3637\n",
      "Epoch 6: val_loss did not improve from 1.39276\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 1.3637 - sMAPE_tf: 1.3637 - val_loss: 1.4020 - val_sMAPE_tf: 1.4020\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3487 - sMAPE_tf: 1.3487\n",
      "Epoch 7: val_loss did not improve from 1.39276\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 1.3487 - sMAPE_tf: 1.3487 - val_loss: 1.4046 - val_sMAPE_tf: 1.4046\n",
      "Training finished in 6.725042819976807 secconds\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 1.4797 - sMAPE_tf: 1.4797\n",
      "1.4797124862670898\n",
      "1/1 [==============================] - 0s 470ms/step\n",
      "mean_SMAPE:0.6721124127817468\n",
      "mean_MASE:2.3636726181939998\n",
      "sim_10_60_nl_ho\n",
      "35 27 27\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4871 - sMAPE_tf: 1.4871\n",
      "Epoch 1: val_loss improved from inf to 1.51675, saving model to ./checkpoints/sim_10_60_nl_ho_best\n",
      "1/1 [==============================] - 5s 5s/step - loss: 1.4871 - sMAPE_tf: 1.4871 - val_loss: 1.5167 - val_sMAPE_tf: 1.5167\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4617 - sMAPE_tf: 1.4617\n",
      "Epoch 2: val_loss improved from 1.51675 to 1.51164, saving model to ./checkpoints/sim_10_60_nl_ho_best\n",
      "1/1 [==============================] - 0s 440ms/step - loss: 1.4617 - sMAPE_tf: 1.4617 - val_loss: 1.5116 - val_sMAPE_tf: 1.5116\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4636 - sMAPE_tf: 1.4636\n",
      "Epoch 3: val_loss improved from 1.51164 to 1.49927, saving model to ./checkpoints/sim_10_60_nl_ho_best\n",
      "1/1 [==============================] - 0s 367ms/step - loss: 1.4636 - sMAPE_tf: 1.4636 - val_loss: 1.4993 - val_sMAPE_tf: 1.4993\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4317 - sMAPE_tf: 1.4317\n",
      "Epoch 4: val_loss improved from 1.49927 to 1.48372, saving model to ./checkpoints/sim_10_60_nl_ho_best\n",
      "1/1 [==============================] - 0s 373ms/step - loss: 1.4317 - sMAPE_tf: 1.4317 - val_loss: 1.4837 - val_sMAPE_tf: 1.4837\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4216 - sMAPE_tf: 1.4216\n",
      "Epoch 5: val_loss improved from 1.48372 to 1.47257, saving model to ./checkpoints/sim_10_60_nl_ho_best\n",
      "1/1 [==============================] - 0s 402ms/step - loss: 1.4216 - sMAPE_tf: 1.4216 - val_loss: 1.4726 - val_sMAPE_tf: 1.4726\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4047 - sMAPE_tf: 1.4047\n",
      "Epoch 6: val_loss did not improve from 1.47257\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 1.4047 - sMAPE_tf: 1.4047 - val_loss: 1.4880 - val_sMAPE_tf: 1.4880\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3767 - sMAPE_tf: 1.3767\n",
      "Epoch 7: val_loss did not improve from 1.47257\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 1.3767 - sMAPE_tf: 1.3767 - val_loss: 1.4931 - val_sMAPE_tf: 1.4931\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3773 - sMAPE_tf: 1.3773\n",
      "Epoch 8: val_loss did not improve from 1.47257\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 1.3773 - sMAPE_tf: 1.3773 - val_loss: 1.4974 - val_sMAPE_tf: 1.4974\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3746 - sMAPE_tf: 1.3746\n",
      "Epoch 9: val_loss did not improve from 1.47257\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 1.3746 - sMAPE_tf: 1.3746 - val_loss: 1.4983 - val_sMAPE_tf: 1.4983\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3549 - sMAPE_tf: 1.3549\n",
      "Epoch 10: val_loss did not improve from 1.47257\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 1.3549 - sMAPE_tf: 1.3549 - val_loss: 1.5006 - val_sMAPE_tf: 1.5006\n",
      "Training finished in 8.036254644393921 secconds\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 1.5820 - sMAPE_tf: 1.5820\n",
      "1.5819876194000244\n",
      "1/1 [==============================] - 1s 524ms/step\n",
      "mean_SMAPE:0.8921816553450246\n",
      "mean_MASE:3.0164671746649194\n",
      "sim_10_222_l_he\n",
      "197 27 27\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.4236 - sMAPE_tf: 1.4236 \n",
      "Epoch 1: val_loss improved from inf to 1.33763, saving model to ./checkpoints/sim_10_222_l_he_best\n",
      "6/6 [==============================] - 8s 282ms/step - loss: 1.4170 - sMAPE_tf: 1.4119 - val_loss: 1.3376 - val_sMAPE_tf: 1.3376\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.3426 - sMAPE_tf: 1.3403\n",
      "Epoch 2: val_loss improved from 1.33763 to 1.21414, saving model to ./checkpoints/sim_10_222_l_he_best\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 1.3426 - sMAPE_tf: 1.3403 - val_loss: 1.2141 - val_sMAPE_tf: 1.2141\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.2875 - sMAPE_tf: 1.2813\n",
      "Epoch 3: val_loss improved from 1.21414 to 1.20093, saving model to ./checkpoints/sim_10_222_l_he_best\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 1.2875 - sMAPE_tf: 1.2813 - val_loss: 1.2009 - val_sMAPE_tf: 1.2009\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.2378 - sMAPE_tf: 1.2347\n",
      "Epoch 4: val_loss improved from 1.20093 to 1.11907, saving model to ./checkpoints/sim_10_222_l_he_best\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 1.2378 - sMAPE_tf: 1.2347 - val_loss: 1.1191 - val_sMAPE_tf: 1.1191\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.1939 - sMAPE_tf: 1.1932\n",
      "Epoch 5: val_loss improved from 1.11907 to 1.02246, saving model to ./checkpoints/sim_10_222_l_he_best\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 1.1939 - sMAPE_tf: 1.1932 - val_loss: 1.0225 - val_sMAPE_tf: 1.0225\n",
      "Epoch 6/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.1428 - sMAPE_tf: 1.1428\n",
      "Epoch 6: val_loss improved from 1.02246 to 0.98483, saving model to ./checkpoints/sim_10_222_l_he_best\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 1.1453 - sMAPE_tf: 1.1472 - val_loss: 0.9848 - val_sMAPE_tf: 0.9848\n",
      "Epoch 7/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.1077 - sMAPE_tf: 1.1077\n",
      "Epoch 7: val_loss improved from 0.98483 to 0.96528, saving model to ./checkpoints/sim_10_222_l_he_best\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 1.1101 - sMAPE_tf: 1.1120 - val_loss: 0.9653 - val_sMAPE_tf: 0.9653\n",
      "Epoch 8/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.0810 - sMAPE_tf: 1.0810\n",
      "Epoch 8: val_loss improved from 0.96528 to 0.95470, saving model to ./checkpoints/sim_10_222_l_he_best\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 1.0849 - sMAPE_tf: 1.0879 - val_loss: 0.9547 - val_sMAPE_tf: 0.9547\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.0640 - sMAPE_tf: 1.0649\n",
      "Epoch 9: val_loss improved from 0.95470 to 0.94386, saving model to ./checkpoints/sim_10_222_l_he_best\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 1.0640 - sMAPE_tf: 1.0649 - val_loss: 0.9439 - val_sMAPE_tf: 0.9439\n",
      "Epoch 10/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.0457 - sMAPE_tf: 1.0457\n",
      "Epoch 10: val_loss did not improve from 0.94386\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.0470 - sMAPE_tf: 1.0479 - val_loss: 0.9503 - val_sMAPE_tf: 0.9503\n",
      "Epoch 11/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.0249 - sMAPE_tf: 1.0249\n",
      "Epoch 11: val_loss improved from 0.94386 to 0.91982, saving model to ./checkpoints/sim_10_222_l_he_best\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 1.0210 - sMAPE_tf: 1.0179 - val_loss: 0.9198 - val_sMAPE_tf: 0.9198\n",
      "Epoch 12/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.0039 - sMAPE_tf: 1.0039\n",
      "Epoch 12: val_loss did not improve from 0.91982\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.0112 - sMAPE_tf: 1.0169 - val_loss: 0.9411 - val_sMAPE_tf: 0.9411\n",
      "Epoch 13/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.9871 - sMAPE_tf: 0.9871\n",
      "Epoch 13: val_loss did not improve from 0.91982\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.9917 - sMAPE_tf: 0.9953 - val_loss: 0.9802 - val_sMAPE_tf: 0.9802\n",
      "Epoch 14/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.9726 - sMAPE_tf: 0.9726\n",
      "Epoch 14: val_loss did not improve from 0.91982\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.9769 - sMAPE_tf: 0.9803 - val_loss: 0.9994 - val_sMAPE_tf: 0.9994\n",
      "Epoch 15/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.9688 - sMAPE_tf: 0.9688\n",
      "Epoch 15: val_loss did not improve from 0.91982\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.9645 - sMAPE_tf: 0.9612 - val_loss: 1.0497 - val_sMAPE_tf: 1.0497\n",
      "Epoch 16/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.9571 - sMAPE_tf: 0.9571\n",
      "Epoch 16: val_loss did not improve from 0.91982\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.9508 - sMAPE_tf: 0.9459 - val_loss: 1.0462 - val_sMAPE_tf: 1.0462\n",
      "Training finished in 14.307444095611572 secconds\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 1.1594 - sMAPE_tf: 1.1594\n",
      "1.1594440937042236\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f16a0122950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 447ms/step\n",
      "mean_SMAPE:0.6226642929277452\n",
      "mean_MASE:2.4266994264975157\n",
      "sim_10_222_l_ho\n",
      "197 27 27\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.4699 - sMAPE_tf: 1.4699 \n",
      "Epoch 1: val_loss improved from inf to 1.44914, saving model to ./checkpoints/sim_10_222_l_ho_best\n",
      "6/6 [==============================] - 5s 163ms/step - loss: 1.4675 - sMAPE_tf: 1.4656 - val_loss: 1.4491 - val_sMAPE_tf: 1.4491\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.3958 - sMAPE_tf: 1.3933\n",
      "Epoch 2: val_loss improved from 1.44914 to 1.30107, saving model to ./checkpoints/sim_10_222_l_ho_best\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 1.3958 - sMAPE_tf: 1.3933 - val_loss: 1.3011 - val_sMAPE_tf: 1.3011\n",
      "Epoch 3/100\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.3751 - sMAPE_tf: 1.3751\n",
      "Epoch 3: val_loss improved from 1.30107 to 1.22483, saving model to ./checkpoints/sim_10_222_l_ho_best\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 1.3542 - sMAPE_tf: 1.3521 - val_loss: 1.2248 - val_sMAPE_tf: 1.2248\n",
      "Epoch 4/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.3057 - sMAPE_tf: 1.3057\n",
      "Epoch 4: val_loss improved from 1.22483 to 1.16695, saving model to ./checkpoints/sim_10_222_l_ho_best\n",
      "6/6 [==============================] - 0s 87ms/step - loss: 1.3034 - sMAPE_tf: 1.3015 - val_loss: 1.1670 - val_sMAPE_tf: 1.1670\n",
      "Epoch 5/100\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.2634 - sMAPE_tf: 1.2634\n",
      "Epoch 5: val_loss improved from 1.16695 to 1.14822, saving model to ./checkpoints/sim_10_222_l_ho_best\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 1.2438 - sMAPE_tf: 1.2446 - val_loss: 1.1482 - val_sMAPE_tf: 1.1482\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.1942 - sMAPE_tf: 1.1889\n",
      "Epoch 6: val_loss improved from 1.14822 to 1.04789, saving model to ./checkpoints/sim_10_222_l_ho_best\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 1.1942 - sMAPE_tf: 1.1889 - val_loss: 1.0479 - val_sMAPE_tf: 1.0479\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.1405 - sMAPE_tf: 1.1333\n",
      "Epoch 7: val_loss improved from 1.04789 to 1.01228, saving model to ./checkpoints/sim_10_222_l_ho_best\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 1.1405 - sMAPE_tf: 1.1333 - val_loss: 1.0123 - val_sMAPE_tf: 1.0123\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.0921 - sMAPE_tf: 1.0941\n",
      "Epoch 8: val_loss improved from 1.01228 to 0.94799, saving model to ./checkpoints/sim_10_222_l_ho_best\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 1.0921 - sMAPE_tf: 1.0941 - val_loss: 0.9480 - val_sMAPE_tf: 0.9480\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.0572 - sMAPE_tf: 1.0489\n",
      "Epoch 9: val_loss improved from 0.94799 to 0.88597, saving model to ./checkpoints/sim_10_222_l_ho_best\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 1.0572 - sMAPE_tf: 1.0489 - val_loss: 0.8860 - val_sMAPE_tf: 0.8860\n",
      "Epoch 10/100\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.0256 - sMAPE_tf: 1.0256\n",
      "Epoch 10: val_loss improved from 0.88597 to 0.85060, saving model to ./checkpoints/sim_10_222_l_ho_best\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 1.0204 - sMAPE_tf: 1.0186 - val_loss: 0.8506 - val_sMAPE_tf: 0.8506\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.9911 - sMAPE_tf: 0.9956\n",
      "Epoch 11: val_loss did not improve from 0.85060\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.9911 - sMAPE_tf: 0.9956 - val_loss: 0.8620 - val_sMAPE_tf: 0.8620\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.9766 - sMAPE_tf: 0.9746\n",
      "Epoch 12: val_loss did not improve from 0.85060\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.9766 - sMAPE_tf: 0.9746 - val_loss: 0.8864 - val_sMAPE_tf: 0.8864\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.9492 - sMAPE_tf: 0.9502\n",
      "Epoch 13: val_loss did not improve from 0.85060\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.9492 - sMAPE_tf: 0.9502 - val_loss: 0.9522 - val_sMAPE_tf: 0.9522\n",
      "Epoch 14/100\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 0.9718 - sMAPE_tf: 0.9718\n",
      "Epoch 14: val_loss did not improve from 0.85060\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.9278 - sMAPE_tf: 0.9283 - val_loss: 0.9461 - val_sMAPE_tf: 0.9461\n",
      "Epoch 15/100\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 0.9039 - sMAPE_tf: 0.9039\n",
      "Epoch 15: val_loss did not improve from 0.85060\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.9173 - sMAPE_tf: 0.9207 - val_loss: 0.9298 - val_sMAPE_tf: 0.9298\n",
      "Training finished in 11.1047523021698 secconds\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 1.0539 - sMAPE_tf: 1.0539\n",
      "1.0538551807403564\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f16a0122440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 493ms/step\n",
      "mean_SMAPE:0.84215414687173\n",
      "mean_MASE:2.8815773287812343\n",
      "sim_10_222_nl_he\n",
      "197 27 27\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.4532 - sMAPE_tf: 1.4500 \n",
      "Epoch 1: val_loss improved from inf to 1.35656, saving model to ./checkpoints/sim_10_222_nl_he_best\n",
      "6/6 [==============================] - 5s 212ms/step - loss: 1.4532 - sMAPE_tf: 1.4500 - val_loss: 1.3566 - val_sMAPE_tf: 1.3566\n",
      "Epoch 2/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.3682 - sMAPE_tf: 1.3682\n",
      "Epoch 2: val_loss improved from 1.35656 to 1.28556, saving model to ./checkpoints/sim_10_222_nl_he_best\n",
      "6/6 [==============================] - 1s 94ms/step - loss: 1.3695 - sMAPE_tf: 1.3705 - val_loss: 1.2856 - val_sMAPE_tf: 1.2856\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.3389 - sMAPE_tf: 1.3365\n",
      "Epoch 3: val_loss improved from 1.28556 to 1.20426, saving model to ./checkpoints/sim_10_222_nl_he_best\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 1.3389 - sMAPE_tf: 1.3365 - val_loss: 1.2043 - val_sMAPE_tf: 1.2043\n",
      "Epoch 4/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.2974 - sMAPE_tf: 1.2974\n",
      "Epoch 4: val_loss did not improve from 1.20426\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.2968 - sMAPE_tf: 1.2963 - val_loss: 1.2099 - val_sMAPE_tf: 1.2099\n",
      "Epoch 5/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.2648 - sMAPE_tf: 1.2648\n",
      "Epoch 5: val_loss improved from 1.20426 to 1.18878, saving model to ./checkpoints/sim_10_222_nl_he_best\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 1.2672 - sMAPE_tf: 1.2690 - val_loss: 1.1888 - val_sMAPE_tf: 1.1888\n",
      "Epoch 6/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.2286 - sMAPE_tf: 1.2286\n",
      "Epoch 6: val_loss improved from 1.18878 to 1.16844, saving model to ./checkpoints/sim_10_222_nl_he_best\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 1.2318 - sMAPE_tf: 1.2343 - val_loss: 1.1684 - val_sMAPE_tf: 1.1684\n",
      "Epoch 7/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.2070 - sMAPE_tf: 1.2070\n",
      "Epoch 7: val_loss improved from 1.16844 to 1.14157, saving model to ./checkpoints/sim_10_222_nl_he_best\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 1.2027 - sMAPE_tf: 1.1994 - val_loss: 1.1416 - val_sMAPE_tf: 1.1416\n",
      "Epoch 8/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.1799 - sMAPE_tf: 1.1799\n",
      "Epoch 8: val_loss improved from 1.14157 to 1.07604, saving model to ./checkpoints/sim_10_222_nl_he_best\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 1.1797 - sMAPE_tf: 1.1796 - val_loss: 1.0760 - val_sMAPE_tf: 1.0760\n",
      "Epoch 9/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.1399 - sMAPE_tf: 1.1399\n",
      "Epoch 9: val_loss improved from 1.07604 to 1.05832, saving model to ./checkpoints/sim_10_222_nl_he_best\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 1.1416 - sMAPE_tf: 1.1430 - val_loss: 1.0583 - val_sMAPE_tf: 1.0583\n",
      "Epoch 10/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.1147 - sMAPE_tf: 1.1147\n",
      "Epoch 10: val_loss did not improve from 1.05832\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.1077 - sMAPE_tf: 1.1022 - val_loss: 1.0765 - val_sMAPE_tf: 1.0765\n",
      "Epoch 11/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.0841 - sMAPE_tf: 1.0841\n",
      "Epoch 11: val_loss did not improve from 1.05832\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.0846 - sMAPE_tf: 1.0851 - val_loss: 1.1489 - val_sMAPE_tf: 1.1489\n",
      "Epoch 12/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.0671 - sMAPE_tf: 1.0671\n",
      "Epoch 12: val_loss did not improve from 1.05832\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.0702 - sMAPE_tf: 1.0726 - val_loss: 1.1411 - val_sMAPE_tf: 1.1411\n",
      "Epoch 13/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.0476 - sMAPE_tf: 1.0476\n",
      "Epoch 13: val_loss did not improve from 1.05832\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.0511 - sMAPE_tf: 1.0539 - val_loss: 1.1531 - val_sMAPE_tf: 1.1531\n",
      "Epoch 14/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.0355 - sMAPE_tf: 1.0355\n",
      "Epoch 14: val_loss did not improve from 1.05832\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.0347 - sMAPE_tf: 1.0341 - val_loss: 1.1611 - val_sMAPE_tf: 1.1611\n",
      "Training finished in 10.291881084442139 secconds\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 1.0204 - sMAPE_tf: 1.0204\n",
      "1.0204492807388306\n",
      "1/1 [==============================] - 0s 485ms/step\n",
      "mean_SMAPE:0.5298340442987806\n",
      "mean_MASE:1.4203011743664837\n",
      "sim_10_222_nl_ho\n",
      "197 27 27\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.4180 - sMAPE_tf: 1.4180 \n",
      "Epoch 1: val_loss improved from inf to 1.44198, saving model to ./checkpoints/sim_10_222_nl_ho_best\n",
      "6/6 [==============================] - 5s 164ms/step - loss: 1.4124 - sMAPE_tf: 1.4081 - val_loss: 1.4420 - val_sMAPE_tf: 1.4420\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.3371 - sMAPE_tf: 1.3347\n",
      "Epoch 2: val_loss improved from 1.44198 to 1.30909, saving model to ./checkpoints/sim_10_222_nl_ho_best\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 1.3371 - sMAPE_tf: 1.3347 - val_loss: 1.3091 - val_sMAPE_tf: 1.3091\n",
      "Epoch 3/100\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.2887 - sMAPE_tf: 1.2887\n",
      "Epoch 3: val_loss did not improve from 1.30909\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.2660 - sMAPE_tf: 1.2710 - val_loss: 1.4140 - val_sMAPE_tf: 1.4140\n",
      "Epoch 4/100\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.2484 - sMAPE_tf: 1.2484\n",
      "Epoch 4: val_loss did not improve from 1.30909\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.2226 - sMAPE_tf: 1.2154 - val_loss: 1.3795 - val_sMAPE_tf: 1.3795\n",
      "Epoch 5/100\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.1860 - sMAPE_tf: 1.1860\n",
      "Epoch 5: val_loss did not improve from 1.30909\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.1568 - sMAPE_tf: 1.1512 - val_loss: 1.3153 - val_sMAPE_tf: 1.3153\n",
      "Epoch 6/100\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.1127 - sMAPE_tf: 1.1127\n",
      "Epoch 6: val_loss improved from 1.30909 to 1.21512, saving model to ./checkpoints/sim_10_222_nl_ho_best\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 1.0996 - sMAPE_tf: 1.0990 - val_loss: 1.2151 - val_sMAPE_tf: 1.2151\n",
      "Epoch 7/100\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.0554 - sMAPE_tf: 1.0554\n",
      "Epoch 7: val_loss improved from 1.21512 to 1.16008, saving model to ./checkpoints/sim_10_222_nl_ho_best\n",
      "6/6 [==============================] - 0s 84ms/step - loss: 1.0699 - sMAPE_tf: 1.0682 - val_loss: 1.1601 - val_sMAPE_tf: 1.1601\n",
      "Epoch 8/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.0315 - sMAPE_tf: 1.0315\n",
      "Epoch 8: val_loss improved from 1.16008 to 1.15952, saving model to ./checkpoints/sim_10_222_nl_ho_best\n",
      "6/6 [==============================] - 0s 87ms/step - loss: 1.0320 - sMAPE_tf: 1.0324 - val_loss: 1.1595 - val_sMAPE_tf: 1.1595\n",
      "Epoch 9/100\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 0.9807 - sMAPE_tf: 0.9807\n",
      "Epoch 9: val_loss did not improve from 1.15952\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.9992 - sMAPE_tf: 0.9971 - val_loss: 1.1709 - val_sMAPE_tf: 1.1709\n",
      "Epoch 10/100\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 0.9782 - sMAPE_tf: 0.9782\n",
      "Epoch 10: val_loss improved from 1.15952 to 1.13860, saving model to ./checkpoints/sim_10_222_nl_ho_best\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.9771 - sMAPE_tf: 0.9818 - val_loss: 1.1386 - val_sMAPE_tf: 1.1386\n",
      "Epoch 11/100\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 0.9115 - sMAPE_tf: 0.9115\n",
      "Epoch 11: val_loss improved from 1.13860 to 1.10401, saving model to ./checkpoints/sim_10_222_nl_ho_best\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 0.9567 - sMAPE_tf: 0.9553 - val_loss: 1.1040 - val_sMAPE_tf: 1.1040\n",
      "Epoch 12/100\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 0.9450 - sMAPE_tf: 0.9450\n",
      "Epoch 12: val_loss improved from 1.10401 to 1.07246, saving model to ./checkpoints/sim_10_222_nl_ho_best\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.9291 - sMAPE_tf: 0.9249 - val_loss: 1.0725 - val_sMAPE_tf: 1.0725\n",
      "Epoch 13/100\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 0.9100 - sMAPE_tf: 0.9100\n",
      "Epoch 13: val_loss improved from 1.07246 to 1.06886, saving model to ./checkpoints/sim_10_222_nl_ho_best\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.9165 - sMAPE_tf: 0.9127 - val_loss: 1.0689 - val_sMAPE_tf: 1.0689\n",
      "Epoch 14/100\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 0.9121 - sMAPE_tf: 0.9121\n",
      "Epoch 14: val_loss did not improve from 1.06886\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.9048 - sMAPE_tf: 0.9074 - val_loss: 1.0836 - val_sMAPE_tf: 1.0836\n",
      "Epoch 15/100\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 0.8805 - sMAPE_tf: 0.8805\n",
      "Epoch 15: val_loss did not improve from 1.06886\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.8896 - sMAPE_tf: 0.8902 - val_loss: 1.0738 - val_sMAPE_tf: 1.0738\n",
      "Epoch 16/100\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 0.8989 - sMAPE_tf: 0.8989\n",
      "Epoch 16: val_loss did not improve from 1.06886\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.8702 - sMAPE_tf: 0.8704 - val_loss: 1.0959 - val_sMAPE_tf: 1.0959\n",
      "Epoch 17/100\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 0.8227 - sMAPE_tf: 0.8227\n",
      "Epoch 17: val_loss did not improve from 1.06886\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.8738 - sMAPE_tf: 0.8767 - val_loss: 1.0839 - val_sMAPE_tf: 1.0839\n",
      "Epoch 18/100\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 0.8536 - sMAPE_tf: 0.8536\n",
      "Epoch 18: val_loss did not improve from 1.06886\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.8491 - sMAPE_tf: 0.8503 - val_loss: 1.0807 - val_sMAPE_tf: 1.0807\n",
      "Training finished in 11.120441913604736 secconds\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.9960 - sMAPE_tf: 0.9960\n",
      "0.996035099029541\n",
      "1/1 [==============================] - 0s 471ms/step\n",
      "mean_SMAPE:0.6321478416439625\n",
      "mean_MASE:1.9400904878296903\n",
      "sim_101_60_l_he\n",
      "35 27 27\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4855 - sMAPE_tf: 1.4855\n",
      "Epoch 1: val_loss improved from inf to 1.45798, saving model to ./checkpoints/sim_101_60_l_he_best\n",
      "1/1 [==============================] - 5s 5s/step - loss: 1.4855 - sMAPE_tf: 1.4855 - val_loss: 1.4580 - val_sMAPE_tf: 1.4580\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4590 - sMAPE_tf: 1.4590\n",
      "Epoch 2: val_loss improved from 1.45798 to 1.44302, saving model to ./checkpoints/sim_101_60_l_he_best\n",
      "1/1 [==============================] - 0s 471ms/step - loss: 1.4590 - sMAPE_tf: 1.4590 - val_loss: 1.4430 - val_sMAPE_tf: 1.4430\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4476 - sMAPE_tf: 1.4476\n",
      "Epoch 3: val_loss did not improve from 1.44302\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 1.4476 - sMAPE_tf: 1.4476 - val_loss: 1.4511 - val_sMAPE_tf: 1.4511\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4209 - sMAPE_tf: 1.4209\n",
      "Epoch 4: val_loss did not improve from 1.44302\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 1.4209 - sMAPE_tf: 1.4209 - val_loss: 1.4638 - val_sMAPE_tf: 1.4638\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4160 - sMAPE_tf: 1.4160\n",
      "Epoch 5: val_loss did not improve from 1.44302\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 1.4160 - sMAPE_tf: 1.4160 - val_loss: 1.4745 - val_sMAPE_tf: 1.4745\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4105 - sMAPE_tf: 1.4105\n",
      "Epoch 6: val_loss did not improve from 1.44302\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 1.4105 - sMAPE_tf: 1.4105 - val_loss: 1.4836 - val_sMAPE_tf: 1.4836\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3990 - sMAPE_tf: 1.3990\n",
      "Epoch 7: val_loss did not improve from 1.44302\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 1.3990 - sMAPE_tf: 1.3990 - val_loss: 1.4914 - val_sMAPE_tf: 1.4914\n",
      "Training finished in 6.87804913520813 secconds\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 1.4794 - sMAPE_tf: 1.4794\n",
      "1.4794002771377563\n",
      "1/1 [==============================] - 1s 545ms/step\n",
      "mean_SMAPE:0.8412116937320018\n",
      "mean_MASE:3.4291749529081863\n",
      "sim_101_60_l_ho\n",
      "35 27 27\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4270 - sMAPE_tf: 1.4270\n",
      "Epoch 1: val_loss improved from inf to 1.46999, saving model to ./checkpoints/sim_101_60_l_ho_best\n",
      "1/1 [==============================] - 5s 5s/step - loss: 1.4270 - sMAPE_tf: 1.4270 - val_loss: 1.4700 - val_sMAPE_tf: 1.4700\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3905 - sMAPE_tf: 1.3905\n",
      "Epoch 2: val_loss improved from 1.46999 to 1.44513, saving model to ./checkpoints/sim_101_60_l_ho_best\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 1.3905 - sMAPE_tf: 1.3905 - val_loss: 1.4451 - val_sMAPE_tf: 1.4451\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3590 - sMAPE_tf: 1.3590\n",
      "Epoch 3: val_loss improved from 1.44513 to 1.43710, saving model to ./checkpoints/sim_101_60_l_ho_best\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 1.3590 - sMAPE_tf: 1.3590 - val_loss: 1.4371 - val_sMAPE_tf: 1.4371\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3442 - sMAPE_tf: 1.3442\n",
      "Epoch 4: val_loss did not improve from 1.43710\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 1.3442 - sMAPE_tf: 1.3442 - val_loss: 1.4486 - val_sMAPE_tf: 1.4486\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3255 - sMAPE_tf: 1.3255\n",
      "Epoch 5: val_loss did not improve from 1.43710\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 1.3255 - sMAPE_tf: 1.3255 - val_loss: 1.4468 - val_sMAPE_tf: 1.4468\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3086 - sMAPE_tf: 1.3086\n",
      "Epoch 6: val_loss did not improve from 1.43710\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 1.3086 - sMAPE_tf: 1.3086 - val_loss: 1.4371 - val_sMAPE_tf: 1.4371\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2768 - sMAPE_tf: 1.2768\n",
      "Epoch 7: val_loss improved from 1.43710 to 1.42493, saving model to ./checkpoints/sim_101_60_l_ho_best\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 1.2768 - sMAPE_tf: 1.2768 - val_loss: 1.4249 - val_sMAPE_tf: 1.4249\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2602 - sMAPE_tf: 1.2602\n",
      "Epoch 8: val_loss improved from 1.42493 to 1.41117, saving model to ./checkpoints/sim_101_60_l_ho_best\n",
      "1/1 [==============================] - 0s 356ms/step - loss: 1.2602 - sMAPE_tf: 1.2602 - val_loss: 1.4112 - val_sMAPE_tf: 1.4112\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2488 - sMAPE_tf: 1.2488\n",
      "Epoch 9: val_loss did not improve from 1.41117\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 1.2488 - sMAPE_tf: 1.2488 - val_loss: 1.4131 - val_sMAPE_tf: 1.4131\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2345 - sMAPE_tf: 1.2345\n",
      "Epoch 10: val_loss did not improve from 1.41117\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 1.2345 - sMAPE_tf: 1.2345 - val_loss: 1.4113 - val_sMAPE_tf: 1.4113\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2274 - sMAPE_tf: 1.2274\n",
      "Epoch 11: val_loss improved from 1.41117 to 1.40594, saving model to ./checkpoints/sim_101_60_l_ho_best\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 1.2274 - sMAPE_tf: 1.2274 - val_loss: 1.4059 - val_sMAPE_tf: 1.4059\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2169 - sMAPE_tf: 1.2169\n",
      "Epoch 12: val_loss improved from 1.40594 to 1.40195, saving model to ./checkpoints/sim_101_60_l_ho_best\n",
      "1/1 [==============================] - 0s 397ms/step - loss: 1.2169 - sMAPE_tf: 1.2169 - val_loss: 1.4020 - val_sMAPE_tf: 1.4020\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1952 - sMAPE_tf: 1.1952\n",
      "Epoch 13: val_loss improved from 1.40195 to 1.39393, saving model to ./checkpoints/sim_101_60_l_ho_best\n",
      "1/1 [==============================] - 0s 421ms/step - loss: 1.1952 - sMAPE_tf: 1.1952 - val_loss: 1.3939 - val_sMAPE_tf: 1.3939\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1803 - sMAPE_tf: 1.1803\n",
      "Epoch 14: val_loss improved from 1.39393 to 1.39320, saving model to ./checkpoints/sim_101_60_l_ho_best\n",
      "1/1 [==============================] - 0s 351ms/step - loss: 1.1803 - sMAPE_tf: 1.1803 - val_loss: 1.3932 - val_sMAPE_tf: 1.3932\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1750 - sMAPE_tf: 1.1750\n",
      "Epoch 15: val_loss improved from 1.39320 to 1.38902, saving model to ./checkpoints/sim_101_60_l_ho_best\n",
      "1/1 [==============================] - 0s 340ms/step - loss: 1.1750 - sMAPE_tf: 1.1750 - val_loss: 1.3890 - val_sMAPE_tf: 1.3890\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1586 - sMAPE_tf: 1.1586\n",
      "Epoch 16: val_loss improved from 1.38902 to 1.38358, saving model to ./checkpoints/sim_101_60_l_ho_best\n",
      "1/1 [==============================] - 0s 376ms/step - loss: 1.1586 - sMAPE_tf: 1.1586 - val_loss: 1.3836 - val_sMAPE_tf: 1.3836\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1345 - sMAPE_tf: 1.1345\n",
      "Epoch 17: val_loss improved from 1.38358 to 1.38016, saving model to ./checkpoints/sim_101_60_l_ho_best\n",
      "1/1 [==============================] - 0s 340ms/step - loss: 1.1345 - sMAPE_tf: 1.1345 - val_loss: 1.3802 - val_sMAPE_tf: 1.3802\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1305 - sMAPE_tf: 1.1305\n",
      "Epoch 18: val_loss improved from 1.38016 to 1.37362, saving model to ./checkpoints/sim_101_60_l_ho_best\n",
      "1/1 [==============================] - 0s 350ms/step - loss: 1.1305 - sMAPE_tf: 1.1305 - val_loss: 1.3736 - val_sMAPE_tf: 1.3736\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1134 - sMAPE_tf: 1.1134\n",
      "Epoch 19: val_loss improved from 1.37362 to 1.37256, saving model to ./checkpoints/sim_101_60_l_ho_best\n",
      "1/1 [==============================] - 0s 396ms/step - loss: 1.1134 - sMAPE_tf: 1.1134 - val_loss: 1.3726 - val_sMAPE_tf: 1.3726\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0954 - sMAPE_tf: 1.0954\n",
      "Epoch 20: val_loss improved from 1.37256 to 1.37104, saving model to ./checkpoints/sim_101_60_l_ho_best\n",
      "1/1 [==============================] - 0s 362ms/step - loss: 1.0954 - sMAPE_tf: 1.0954 - val_loss: 1.3710 - val_sMAPE_tf: 1.3710\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0954 - sMAPE_tf: 1.0954\n",
      "Epoch 21: val_loss improved from 1.37104 to 1.36452, saving model to ./checkpoints/sim_101_60_l_ho_best\n",
      "1/1 [==============================] - 0s 416ms/step - loss: 1.0954 - sMAPE_tf: 1.0954 - val_loss: 1.3645 - val_sMAPE_tf: 1.3645\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0763 - sMAPE_tf: 1.0763\n",
      "Epoch 22: val_loss improved from 1.36452 to 1.36381, saving model to ./checkpoints/sim_101_60_l_ho_best\n",
      "1/1 [==============================] - 0s 416ms/step - loss: 1.0763 - sMAPE_tf: 1.0763 - val_loss: 1.3638 - val_sMAPE_tf: 1.3638\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0640 - sMAPE_tf: 1.0640\n",
      "Epoch 23: val_loss did not improve from 1.36381\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 1.0640 - sMAPE_tf: 1.0640 - val_loss: 1.3695 - val_sMAPE_tf: 1.3695\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0406 - sMAPE_tf: 1.0406\n",
      "Epoch 24: val_loss did not improve from 1.36381\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 1.0406 - sMAPE_tf: 1.0406 - val_loss: 1.3710 - val_sMAPE_tf: 1.3710\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0337 - sMAPE_tf: 1.0337\n",
      "Epoch 25: val_loss did not improve from 1.36381\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 1.0337 - sMAPE_tf: 1.0337 - val_loss: 1.3714 - val_sMAPE_tf: 1.3714\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0141 - sMAPE_tf: 1.0141\n",
      "Epoch 26: val_loss did not improve from 1.36381\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 1.0141 - sMAPE_tf: 1.0141 - val_loss: 1.3666 - val_sMAPE_tf: 1.3666\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0007 - sMAPE_tf: 1.0007\n",
      "Epoch 27: val_loss did not improve from 1.36381\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 1.0007 - sMAPE_tf: 1.0007 - val_loss: 1.3669 - val_sMAPE_tf: 1.3669\n",
      "Training finished in 14.14473295211792 secconds\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 1.5325 - sMAPE_tf: 1.5325\n",
      "1.5324898958206177\n",
      "1/1 [==============================] - 0s 446ms/step\n",
      "mean_SMAPE:0.702899894265084\n",
      "mean_MASE:2.327708007733965\n",
      "sim_101_60_nl_he\n",
      "35 27 27\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4954 - sMAPE_tf: 1.4954\n",
      "Epoch 1: val_loss improved from inf to 1.52755, saving model to ./checkpoints/sim_101_60_nl_he_best\n",
      "1/1 [==============================] - 5s 5s/step - loss: 1.4954 - sMAPE_tf: 1.4954 - val_loss: 1.5275 - val_sMAPE_tf: 1.5275\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4690 - sMAPE_tf: 1.4690\n",
      "Epoch 2: val_loss improved from 1.52755 to 1.51604, saving model to ./checkpoints/sim_101_60_nl_he_best\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 1.4690 - sMAPE_tf: 1.4690 - val_loss: 1.5160 - val_sMAPE_tf: 1.5160\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4552 - sMAPE_tf: 1.4552\n",
      "Epoch 3: val_loss improved from 1.51604 to 1.50166, saving model to ./checkpoints/sim_101_60_nl_he_best\n",
      "1/1 [==============================] - 0s 347ms/step - loss: 1.4552 - sMAPE_tf: 1.4552 - val_loss: 1.5017 - val_sMAPE_tf: 1.5017\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4415 - sMAPE_tf: 1.4415\n",
      "Epoch 4: val_loss improved from 1.50166 to 1.49938, saving model to ./checkpoints/sim_101_60_nl_he_best\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 1.4415 - sMAPE_tf: 1.4415 - val_loss: 1.4994 - val_sMAPE_tf: 1.4994\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4292 - sMAPE_tf: 1.4292\n",
      "Epoch 5: val_loss improved from 1.49938 to 1.48822, saving model to ./checkpoints/sim_101_60_nl_he_best\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 1.4292 - sMAPE_tf: 1.4292 - val_loss: 1.4882 - val_sMAPE_tf: 1.4882\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4221 - sMAPE_tf: 1.4221\n",
      "Epoch 6: val_loss did not improve from 1.48822\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 1.4221 - sMAPE_tf: 1.4221 - val_loss: 1.4916 - val_sMAPE_tf: 1.4916\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4192 - sMAPE_tf: 1.4192\n",
      "Epoch 7: val_loss improved from 1.48822 to 1.48674, saving model to ./checkpoints/sim_101_60_nl_he_best\n",
      "1/1 [==============================] - 0s 358ms/step - loss: 1.4192 - sMAPE_tf: 1.4192 - val_loss: 1.4867 - val_sMAPE_tf: 1.4867\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4128 - sMAPE_tf: 1.4128\n",
      "Epoch 8: val_loss improved from 1.48674 to 1.47456, saving model to ./checkpoints/sim_101_60_nl_he_best\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 1.4128 - sMAPE_tf: 1.4128 - val_loss: 1.4746 - val_sMAPE_tf: 1.4746\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3931 - sMAPE_tf: 1.3931\n",
      "Epoch 9: val_loss did not improve from 1.47456\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 1.3931 - sMAPE_tf: 1.3931 - val_loss: 1.4787 - val_sMAPE_tf: 1.4787\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3782 - sMAPE_tf: 1.3782\n",
      "Epoch 10: val_loss did not improve from 1.47456\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 1.3782 - sMAPE_tf: 1.3782 - val_loss: 1.4774 - val_sMAPE_tf: 1.4774\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3687 - sMAPE_tf: 1.3687\n",
      "Epoch 11: val_loss did not improve from 1.47456\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 1.3687 - sMAPE_tf: 1.3687 - val_loss: 1.4827 - val_sMAPE_tf: 1.4827\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3522 - sMAPE_tf: 1.3522\n",
      "Epoch 12: val_loss did not improve from 1.47456\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 1.3522 - sMAPE_tf: 1.3522 - val_loss: 1.4873 - val_sMAPE_tf: 1.4873\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3442 - sMAPE_tf: 1.3442\n",
      "Epoch 13: val_loss did not improve from 1.47456\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 1.3442 - sMAPE_tf: 1.3442 - val_loss: 1.4945 - val_sMAPE_tf: 1.4945\n",
      "Training finished in 8.165749549865723 secconds\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 1.5284 - sMAPE_tf: 1.5284\n",
      "1.5283660888671875\n",
      "1/1 [==============================] - 0s 408ms/step\n",
      "mean_SMAPE:0.9489899012341311\n",
      "mean_MASE:3.1831638091632444\n",
      "sim_101_60_nl_ho\n",
      "35 27 27\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4447 - sMAPE_tf: 1.4447\n",
      "Epoch 1: val_loss improved from inf to 1.44901, saving model to ./checkpoints/sim_101_60_nl_ho_best\n",
      "1/1 [==============================] - 4s 4s/step - loss: 1.4447 - sMAPE_tf: 1.4447 - val_loss: 1.4490 - val_sMAPE_tf: 1.4490\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4205 - sMAPE_tf: 1.4205\n",
      "Epoch 2: val_loss did not improve from 1.44901\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 1.4205 - sMAPE_tf: 1.4205 - val_loss: 1.4575 - val_sMAPE_tf: 1.4575\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4091 - sMAPE_tf: 1.4091\n",
      "Epoch 3: val_loss did not improve from 1.44901\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 1.4091 - sMAPE_tf: 1.4091 - val_loss: 1.4557 - val_sMAPE_tf: 1.4557\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3944 - sMAPE_tf: 1.3944\n",
      "Epoch 4: val_loss did not improve from 1.44901\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 1.3944 - sMAPE_tf: 1.3944 - val_loss: 1.4710 - val_sMAPE_tf: 1.4710\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3785 - sMAPE_tf: 1.3785\n",
      "Epoch 5: val_loss did not improve from 1.44901\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 1.3785 - sMAPE_tf: 1.3785 - val_loss: 1.4797 - val_sMAPE_tf: 1.4797\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3747 - sMAPE_tf: 1.3747\n",
      "Epoch 6: val_loss did not improve from 1.44901\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 1.3747 - sMAPE_tf: 1.3747 - val_loss: 1.4886 - val_sMAPE_tf: 1.4886\n",
      "Training finished in 5.583810091018677 secconds\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 1.4662 - sMAPE_tf: 1.4662\n",
      "1.4661592245101929\n",
      "1/1 [==============================] - 0s 421ms/step\n",
      "mean_SMAPE:0.8543002343954683\n",
      "mean_MASE:2.8631611985745837\n",
      "sim_101_222_l_he\n",
      "197 27 27\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.4154 - sMAPE_tf: 1.4154\n",
      "Epoch 1: val_loss improved from inf to 1.40899, saving model to ./checkpoints/sim_101_222_l_he_best\n",
      "6/6 [==============================] - 5s 172ms/step - loss: 1.4106 - sMAPE_tf: 1.4068 - val_loss: 1.4090 - val_sMAPE_tf: 1.4090\n",
      "Epoch 2/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.3317 - sMAPE_tf: 1.3317\n",
      "Epoch 2: val_loss improved from 1.40899 to 1.35365, saving model to ./checkpoints/sim_101_222_l_he_best\n",
      "6/6 [==============================] - 1s 97ms/step - loss: 1.3288 - sMAPE_tf: 1.3266 - val_loss: 1.3537 - val_sMAPE_tf: 1.3537\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.2254 - sMAPE_tf: 1.2210\n",
      "Epoch 3: val_loss improved from 1.35365 to 1.19758, saving model to ./checkpoints/sim_101_222_l_he_best\n",
      "6/6 [==============================] - 1s 98ms/step - loss: 1.2254 - sMAPE_tf: 1.2210 - val_loss: 1.1976 - val_sMAPE_tf: 1.1976\n",
      "Epoch 4/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.1406 - sMAPE_tf: 1.1406\n",
      "Epoch 4: val_loss did not improve from 1.19758\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 1.1379 - sMAPE_tf: 1.1358 - val_loss: 1.2232 - val_sMAPE_tf: 1.2232\n",
      "Epoch 5/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.0751 - sMAPE_tf: 1.0751\n",
      "Epoch 5: val_loss did not improve from 1.19758\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 1.0766 - sMAPE_tf: 1.0778 - val_loss: 1.2016 - val_sMAPE_tf: 1.2016\n",
      "Epoch 6/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.0190 - sMAPE_tf: 1.0190\n",
      "Epoch 6: val_loss improved from 1.19758 to 1.14895, saving model to ./checkpoints/sim_101_222_l_he_best\n",
      "6/6 [==============================] - 1s 91ms/step - loss: 1.0168 - sMAPE_tf: 1.0151 - val_loss: 1.1490 - val_sMAPE_tf: 1.1490\n",
      "Epoch 7/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.9533 - sMAPE_tf: 0.9533\n",
      "Epoch 7: val_loss did not improve from 1.14895\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 0.9519 - sMAPE_tf: 0.9508 - val_loss: 1.1551 - val_sMAPE_tf: 1.1551\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.9074 - sMAPE_tf: 0.9086\n",
      "Epoch 8: val_loss improved from 1.14895 to 1.12811, saving model to ./checkpoints/sim_101_222_l_he_best\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 0.9074 - sMAPE_tf: 0.9086 - val_loss: 1.1281 - val_sMAPE_tf: 1.1281\n",
      "Epoch 9/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.8705 - sMAPE_tf: 0.8705\n",
      "Epoch 9: val_loss improved from 1.12811 to 1.09479, saving model to ./checkpoints/sim_101_222_l_he_best\n",
      "6/6 [==============================] - 0s 87ms/step - loss: 0.8688 - sMAPE_tf: 0.8674 - val_loss: 1.0948 - val_sMAPE_tf: 1.0948\n",
      "Epoch 10/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.8186 - sMAPE_tf: 0.8186\n",
      "Epoch 10: val_loss did not improve from 1.09479\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.8149 - sMAPE_tf: 0.8120 - val_loss: 1.0958 - val_sMAPE_tf: 1.0958\n",
      "Epoch 11/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.7737 - sMAPE_tf: 0.7737\n",
      "Epoch 11: val_loss improved from 1.09479 to 1.07489, saving model to ./checkpoints/sim_101_222_l_he_best\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 0.7732 - sMAPE_tf: 0.7729 - val_loss: 1.0749 - val_sMAPE_tf: 1.0749\n",
      "Epoch 12/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.7391 - sMAPE_tf: 0.7391\n",
      "Epoch 12: val_loss improved from 1.07489 to 1.06996, saving model to ./checkpoints/sim_101_222_l_he_best\n",
      "6/6 [==============================] - 0s 85ms/step - loss: 0.7396 - sMAPE_tf: 0.7401 - val_loss: 1.0700 - val_sMAPE_tf: 1.0700\n",
      "Epoch 13/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.7125 - sMAPE_tf: 0.7125\n",
      "Epoch 13: val_loss did not improve from 1.06996\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.7116 - sMAPE_tf: 0.7110 - val_loss: 1.0765 - val_sMAPE_tf: 1.0765\n",
      "Epoch 14/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.6942 - sMAPE_tf: 0.6942\n",
      "Epoch 14: val_loss did not improve from 1.06996\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.6945 - sMAPE_tf: 0.6947 - val_loss: 1.1167 - val_sMAPE_tf: 1.1167\n",
      "Epoch 15/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.6768 - sMAPE_tf: 0.6768\n",
      "Epoch 15: val_loss improved from 1.06996 to 1.06331, saving model to ./checkpoints/sim_101_222_l_he_best\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.6758 - sMAPE_tf: 0.6750 - val_loss: 1.0633 - val_sMAPE_tf: 1.0633\n",
      "Epoch 16/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.6554 - sMAPE_tf: 0.6554\n",
      "Epoch 16: val_loss improved from 1.06331 to 1.04313, saving model to ./checkpoints/sim_101_222_l_he_best\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 0.6574 - sMAPE_tf: 0.6590 - val_loss: 1.0431 - val_sMAPE_tf: 1.0431\n",
      "Epoch 17/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.6396 - sMAPE_tf: 0.6396\n",
      "Epoch 17: val_loss did not improve from 1.04313\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.6393 - sMAPE_tf: 0.6391 - val_loss: 1.0585 - val_sMAPE_tf: 1.0585\n",
      "Epoch 18/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.6198 - sMAPE_tf: 0.6198\n",
      "Epoch 18: val_loss did not improve from 1.04313\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.6188 - sMAPE_tf: 0.6181 - val_loss: 1.0551 - val_sMAPE_tf: 1.0551\n",
      "Epoch 19/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.5902 - sMAPE_tf: 0.5902\n",
      "Epoch 19: val_loss did not improve from 1.04313\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.5892 - sMAPE_tf: 0.5884 - val_loss: 1.0640 - val_sMAPE_tf: 1.0640\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.5738 - sMAPE_tf: 0.5770\n",
      "Epoch 20: val_loss improved from 1.04313 to 1.04040, saving model to ./checkpoints/sim_101_222_l_he_best\n",
      "6/6 [==============================] - 1s 94ms/step - loss: 0.5738 - sMAPE_tf: 0.5770 - val_loss: 1.0404 - val_sMAPE_tf: 1.0404\n",
      "Epoch 21/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.5623 - sMAPE_tf: 0.5623\n",
      "Epoch 21: val_loss did not improve from 1.04040\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.5670 - sMAPE_tf: 0.5707 - val_loss: 1.0534 - val_sMAPE_tf: 1.0534\n",
      "Epoch 22/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.5499 - sMAPE_tf: 0.5499\n",
      "Epoch 22: val_loss did not improve from 1.04040\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.5508 - sMAPE_tf: 0.5514 - val_loss: 1.0424 - val_sMAPE_tf: 1.0424\n",
      "Epoch 23/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.5451 - sMAPE_tf: 0.5451\n",
      "Epoch 23: val_loss did not improve from 1.04040\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.5438 - sMAPE_tf: 0.5428 - val_loss: 1.0510 - val_sMAPE_tf: 1.0510\n",
      "Epoch 24/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.5352 - sMAPE_tf: 0.5352\n",
      "Epoch 24: val_loss did not improve from 1.04040\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.5359 - sMAPE_tf: 0.5364 - val_loss: 1.0504 - val_sMAPE_tf: 1.0504\n",
      "Epoch 25/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.5320 - sMAPE_tf: 0.5320\n",
      "Epoch 25: val_loss did not improve from 1.04040\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 0.5322 - sMAPE_tf: 0.5324 - val_loss: 1.0533 - val_sMAPE_tf: 1.0533\n",
      "Training finished in 15.254820585250854 secconds\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 1.0599 - sMAPE_tf: 1.0599\n",
      "1.0599292516708374\n",
      "1/1 [==============================] - 1s 601ms/step\n",
      "mean_SMAPE:0.46323023057475105\n",
      "mean_MASE:2.2195035320356316\n",
      "sim_101_222_l_ho\n",
      "197 27 27\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.4765 - sMAPE_tf: 1.4765\n",
      "Epoch 1: val_loss improved from inf to 1.39039, saving model to ./checkpoints/sim_101_222_l_ho_best\n",
      "6/6 [==============================] - 5s 169ms/step - loss: 1.4732 - sMAPE_tf: 1.4706 - val_loss: 1.3904 - val_sMAPE_tf: 1.3904\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.4117 - sMAPE_tf: 1.4102\n",
      "Epoch 2: val_loss improved from 1.39039 to 1.32978, saving model to ./checkpoints/sim_101_222_l_ho_best\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 1.4117 - sMAPE_tf: 1.4102 - val_loss: 1.3298 - val_sMAPE_tf: 1.3298\n",
      "Epoch 3/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.3276 - sMAPE_tf: 1.3276\n",
      "Epoch 3: val_loss improved from 1.32978 to 1.24627, saving model to ./checkpoints/sim_101_222_l_ho_best\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 1.3245 - sMAPE_tf: 1.3222 - val_loss: 1.2463 - val_sMAPE_tf: 1.2463\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.2081 - sMAPE_tf: 1.2056\n",
      "Epoch 4: val_loss did not improve from 1.24627\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 1.2081 - sMAPE_tf: 1.2056 - val_loss: 1.2752 - val_sMAPE_tf: 1.2752\n",
      "Epoch 5/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.1357 - sMAPE_tf: 1.1357\n",
      "Epoch 5: val_loss improved from 1.24627 to 1.18752, saving model to ./checkpoints/sim_101_222_l_ho_best\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 1.1329 - sMAPE_tf: 1.1307 - val_loss: 1.1875 - val_sMAPE_tf: 1.1875\n",
      "Epoch 6/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.0661 - sMAPE_tf: 1.0661\n",
      "Epoch 6: val_loss improved from 1.18752 to 1.14677, saving model to ./checkpoints/sim_101_222_l_ho_best\n",
      "6/6 [==============================] - 0s 87ms/step - loss: 1.0635 - sMAPE_tf: 1.0614 - val_loss: 1.1468 - val_sMAPE_tf: 1.1468\n",
      "Epoch 7/100\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 0.9924 - sMAPE_tf: 0.9924\n",
      "Epoch 7: val_loss improved from 1.14677 to 1.12754, saving model to ./checkpoints/sim_101_222_l_ho_best\n",
      "6/6 [==============================] - 1s 95ms/step - loss: 0.9866 - sMAPE_tf: 0.9852 - val_loss: 1.1275 - val_sMAPE_tf: 1.1275\n",
      "Epoch 8/100\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 0.9439 - sMAPE_tf: 0.9439\n",
      "Epoch 8: val_loss improved from 1.12754 to 1.10816, saving model to ./checkpoints/sim_101_222_l_ho_best\n",
      "6/6 [==============================] - 1s 95ms/step - loss: 0.9422 - sMAPE_tf: 0.9440 - val_loss: 1.1082 - val_sMAPE_tf: 1.1082\n",
      "Epoch 9/100\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 0.8852 - sMAPE_tf: 0.8852\n",
      "Epoch 9: val_loss did not improve from 1.10816\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.8830 - sMAPE_tf: 0.8829 - val_loss: 1.1168 - val_sMAPE_tf: 1.1168\n",
      "Epoch 10/100\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 0.8459 - sMAPE_tf: 0.8459\n",
      "Epoch 10: val_loss did not improve from 1.10816\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.8429 - sMAPE_tf: 0.8415 - val_loss: 1.1094 - val_sMAPE_tf: 1.1094\n",
      "Epoch 11/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.8126 - sMAPE_tf: 0.8126\n",
      "Epoch 11: val_loss did not improve from 1.10816\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.8113 - sMAPE_tf: 0.8103 - val_loss: 1.1298 - val_sMAPE_tf: 1.1298\n",
      "Epoch 12/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.7741 - sMAPE_tf: 0.7741\n",
      "Epoch 12: val_loss did not improve from 1.10816\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.7759 - sMAPE_tf: 0.7773 - val_loss: 1.1110 - val_sMAPE_tf: 1.1110\n",
      "Epoch 13/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.7481 - sMAPE_tf: 0.7481\n",
      "Epoch 13: val_loss did not improve from 1.10816\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.7490 - sMAPE_tf: 0.7496 - val_loss: 1.1375 - val_sMAPE_tf: 1.1375\n",
      "Training finished in 10.259819746017456 secconds\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 1.0708 - sMAPE_tf: 1.0708\n",
      "1.0708324909210205\n",
      "1/1 [==============================] - 0s 382ms/step\n",
      "mean_SMAPE:0.5596313251733036\n",
      "mean_MASE:2.389375374656288\n",
      "sim_101_222_nl_he\n",
      "197 27 27\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.4129 - sMAPE_tf: 1.4084\n",
      "Epoch 1: val_loss improved from inf to 1.43349, saving model to ./checkpoints/sim_101_222_nl_he_best\n",
      "6/6 [==============================] - 5s 161ms/step - loss: 1.4129 - sMAPE_tf: 1.4084 - val_loss: 1.4335 - val_sMAPE_tf: 1.4335\n",
      "Epoch 2/100\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 1.3127 - sMAPE_tf: 1.3127\n",
      "Epoch 2: val_loss improved from 1.43349 to 1.36582, saving model to ./checkpoints/sim_101_222_nl_he_best\n",
      "6/6 [==============================] - 0s 89ms/step - loss: 1.2940 - sMAPE_tf: 1.2879 - val_loss: 1.3658 - val_sMAPE_tf: 1.3658\n",
      "Epoch 3/100\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 1.1959 - sMAPE_tf: 1.1959\n",
      "Epoch 3: val_loss improved from 1.36582 to 1.35859, saving model to ./checkpoints/sim_101_222_nl_he_best\n",
      "6/6 [==============================] - 1s 97ms/step - loss: 1.1880 - sMAPE_tf: 1.1859 - val_loss: 1.3586 - val_sMAPE_tf: 1.3586\n",
      "Epoch 4/100\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 1.1222 - sMAPE_tf: 1.1222\n",
      "Epoch 4: val_loss improved from 1.35859 to 1.29209, saving model to ./checkpoints/sim_101_222_nl_he_best\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 1.1181 - sMAPE_tf: 1.1183 - val_loss: 1.2921 - val_sMAPE_tf: 1.2921\n",
      "Epoch 5/100\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 1.0637 - sMAPE_tf: 1.0637\n",
      "Epoch 5: val_loss improved from 1.29209 to 1.24112, saving model to ./checkpoints/sim_101_222_nl_he_best\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 1.0553 - sMAPE_tf: 1.0536 - val_loss: 1.2411 - val_sMAPE_tf: 1.2411\n",
      "Epoch 6/100\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 0.9923 - sMAPE_tf: 0.9923\n",
      "Epoch 6: val_loss improved from 1.24112 to 1.17296, saving model to ./checkpoints/sim_101_222_nl_he_best\n",
      "6/6 [==============================] - 1s 94ms/step - loss: 0.9920 - sMAPE_tf: 0.9897 - val_loss: 1.1730 - val_sMAPE_tf: 1.1730\n",
      "Epoch 7/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.9510 - sMAPE_tf: 0.9510\n",
      "Epoch 7: val_loss did not improve from 1.17296\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.9483 - sMAPE_tf: 0.9461 - val_loss: 1.1785 - val_sMAPE_tf: 1.1785\n",
      "Epoch 8/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.9015 - sMAPE_tf: 0.9015\n",
      "Epoch 8: val_loss did not improve from 1.17296\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.9004 - sMAPE_tf: 0.8995 - val_loss: 1.1809 - val_sMAPE_tf: 1.1809\n",
      "Epoch 9/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.8589 - sMAPE_tf: 0.8589\n",
      "Epoch 9: val_loss improved from 1.17296 to 1.15527, saving model to ./checkpoints/sim_101_222_nl_he_best\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 0.8618 - sMAPE_tf: 0.8641 - val_loss: 1.1553 - val_sMAPE_tf: 1.1553\n",
      "Epoch 10/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.8242 - sMAPE_tf: 0.8242\n",
      "Epoch 10: val_loss did not improve from 1.15527\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.8230 - sMAPE_tf: 0.8221 - val_loss: 1.1709 - val_sMAPE_tf: 1.1709\n",
      "Epoch 11/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.8136 - sMAPE_tf: 0.8136\n",
      "Epoch 11: val_loss did not improve from 1.15527\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.8123 - sMAPE_tf: 0.8112 - val_loss: 1.1663 - val_sMAPE_tf: 1.1663\n",
      "Epoch 12/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.7918 - sMAPE_tf: 0.7918\n",
      "Epoch 12: val_loss improved from 1.15527 to 1.15485, saving model to ./checkpoints/sim_101_222_nl_he_best\n",
      "6/6 [==============================] - 0s 88ms/step - loss: 0.7931 - sMAPE_tf: 0.7940 - val_loss: 1.1548 - val_sMAPE_tf: 1.1548\n",
      "Epoch 13/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.7742 - sMAPE_tf: 0.7742\n",
      "Epoch 13: val_loss improved from 1.15485 to 1.11208, saving model to ./checkpoints/sim_101_222_nl_he_best\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 0.7727 - sMAPE_tf: 0.7714 - val_loss: 1.1121 - val_sMAPE_tf: 1.1121\n",
      "Epoch 14/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.7489 - sMAPE_tf: 0.7489\n",
      "Epoch 14: val_loss did not improve from 1.11208\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.7496 - sMAPE_tf: 0.7502 - val_loss: 1.1398 - val_sMAPE_tf: 1.1398\n",
      "Epoch 15/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.7269 - sMAPE_tf: 0.7269\n",
      "Epoch 15: val_loss did not improve from 1.11208\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 0.7265 - sMAPE_tf: 0.7261 - val_loss: 1.1309 - val_sMAPE_tf: 1.1309\n",
      "Epoch 16/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.7141 - sMAPE_tf: 0.7141\n",
      "Epoch 16: val_loss did not improve from 1.11208\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 0.7159 - sMAPE_tf: 0.7174 - val_loss: 1.1363 - val_sMAPE_tf: 1.1363\n",
      "Epoch 17/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.7076 - sMAPE_tf: 0.7076\n",
      "Epoch 17: val_loss did not improve from 1.11208\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.7098 - sMAPE_tf: 0.7114 - val_loss: 1.1380 - val_sMAPE_tf: 1.1380\n",
      "Epoch 18/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.6913 - sMAPE_tf: 0.6913\n",
      "Epoch 18: val_loss did not improve from 1.11208\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.6940 - sMAPE_tf: 0.6961 - val_loss: 1.1290 - val_sMAPE_tf: 1.1290\n",
      "Training finished in 13.183302402496338 secconds\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 1.2363 - sMAPE_tf: 1.2363\n",
      "1.2362751960754395\n",
      "1/1 [==============================] - 0s 405ms/step\n",
      "mean_SMAPE:0.4850011059481722\n",
      "mean_MASE:1.4687104383662315\n",
      "sim_101_222_nl_ho\n",
      "197 27 27\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.4115 - sMAPE_tf: 1.4096\n",
      "Epoch 1: val_loss improved from inf to 1.42292, saving model to ./checkpoints/sim_101_222_nl_ho_best\n",
      "6/6 [==============================] - 5s 223ms/step - loss: 1.4115 - sMAPE_tf: 1.4096 - val_loss: 1.4229 - val_sMAPE_tf: 1.4229\n",
      "Epoch 2/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.3334 - sMAPE_tf: 1.3334\n",
      "Epoch 2: val_loss improved from 1.42292 to 1.33135, saving model to ./checkpoints/sim_101_222_nl_ho_best\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 1.3300 - sMAPE_tf: 1.3274 - val_loss: 1.3313 - val_sMAPE_tf: 1.3313\n",
      "Epoch 3/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.2366 - sMAPE_tf: 1.2366\n",
      "Epoch 3: val_loss improved from 1.33135 to 1.32713, saving model to ./checkpoints/sim_101_222_nl_ho_best\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 1.2329 - sMAPE_tf: 1.2300 - val_loss: 1.3271 - val_sMAPE_tf: 1.3271\n",
      "Epoch 4/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.1494 - sMAPE_tf: 1.1494\n",
      "Epoch 4: val_loss improved from 1.32713 to 1.22773, saving model to ./checkpoints/sim_101_222_nl_ho_best\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 1.1459 - sMAPE_tf: 1.1432 - val_loss: 1.2277 - val_sMAPE_tf: 1.2277\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.0694 - sMAPE_tf: 1.0666\n",
      "Epoch 5: val_loss improved from 1.22773 to 1.16459, saving model to ./checkpoints/sim_101_222_nl_ho_best\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 1.0694 - sMAPE_tf: 1.0666 - val_loss: 1.1646 - val_sMAPE_tf: 1.1646\n",
      "Epoch 6/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.0032 - sMAPE_tf: 1.0032\n",
      "Epoch 6: val_loss did not improve from 1.16459\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 1.0018 - sMAPE_tf: 1.0008 - val_loss: 1.1758 - val_sMAPE_tf: 1.1758\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.9478 - sMAPE_tf: 0.9475\n",
      "Epoch 7: val_loss did not improve from 1.16459\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.9478 - sMAPE_tf: 0.9475 - val_loss: 1.1772 - val_sMAPE_tf: 1.1772\n",
      "Epoch 8/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.9022 - sMAPE_tf: 0.9022\n",
      "Epoch 8: val_loss did not improve from 1.16459\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.8993 - sMAPE_tf: 0.8970 - val_loss: 1.1683 - val_sMAPE_tf: 1.1683\n",
      "Epoch 9/100\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 0.8563 - sMAPE_tf: 0.8563\n",
      "Epoch 9: val_loss did not improve from 1.16459\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.8513 - sMAPE_tf: 0.8499 - val_loss: 1.1666 - val_sMAPE_tf: 1.1666\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.8156 - sMAPE_tf: 0.8171\n",
      "Epoch 10: val_loss did not improve from 1.16459\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.8156 - sMAPE_tf: 0.8171 - val_loss: 1.1966 - val_sMAPE_tf: 1.1966\n",
      "Training finished in 8.711570501327515 secconds\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 1.2127 - sMAPE_tf: 1.2127\n",
      "1.2127224206924438\n",
      "1/1 [==============================] - 0s 373ms/step\n",
      "mean_SMAPE:0.5384780092645773\n",
      "mean_MASE:1.5164022147348672\n",
      "sim_500_60_l_he\n",
      "35 27 27\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4649 - sMAPE_tf: 1.4649\n",
      "Epoch 1: val_loss improved from inf to 1.54050, saving model to ./checkpoints/sim_500_60_l_he_best\n",
      "1/1 [==============================] - 5s 5s/step - loss: 1.4649 - sMAPE_tf: 1.4649 - val_loss: 1.5405 - val_sMAPE_tf: 1.5405\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4976 - sMAPE_tf: 1.4976\n",
      "Epoch 2: val_loss did not improve from 1.54050\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 1.4976 - sMAPE_tf: 1.4976 - val_loss: 1.6227 - val_sMAPE_tf: 1.6227\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5437 - sMAPE_tf: 1.5437\n",
      "Epoch 3: val_loss did not improve from 1.54050\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 1.5437 - sMAPE_tf: 1.5437 - val_loss: 1.6790 - val_sMAPE_tf: 1.6790\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5740 - sMAPE_tf: 1.5740\n",
      "Epoch 4: val_loss did not improve from 1.54050\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 1.5740 - sMAPE_tf: 1.5740 - val_loss: 1.7145 - val_sMAPE_tf: 1.7145\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5959 - sMAPE_tf: 1.5959\n",
      "Epoch 5: val_loss did not improve from 1.54050\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 1.5959 - sMAPE_tf: 1.5959 - val_loss: 1.7404 - val_sMAPE_tf: 1.7404\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.6145 - sMAPE_tf: 1.6145\n",
      "Epoch 6: val_loss did not improve from 1.54050\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 1.6145 - sMAPE_tf: 1.6145 - val_loss: 1.7389 - val_sMAPE_tf: 1.7389\n",
      "Training finished in 6.485039472579956 secconds\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 1.5492 - sMAPE_tf: 1.5492\n",
      "1.549169898033142\n",
      "1/1 [==============================] - 0s 434ms/step\n",
      "mean_SMAPE:0.9686294707487045\n",
      "mean_MASE:4.986849535240777\n",
      "sim_500_60_l_ho\n",
      "35 27 27\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4417 - sMAPE_tf: 1.4417\n",
      "Epoch 1: val_loss improved from inf to 1.50092, saving model to ./checkpoints/sim_500_60_l_ho_best\n",
      "1/1 [==============================] - 5s 5s/step - loss: 1.4417 - sMAPE_tf: 1.4417 - val_loss: 1.5009 - val_sMAPE_tf: 1.5009\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4511 - sMAPE_tf: 1.4511\n",
      "Epoch 2: val_loss did not improve from 1.50092\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 1.4511 - sMAPE_tf: 1.4511 - val_loss: 1.5315 - val_sMAPE_tf: 1.5315\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4683 - sMAPE_tf: 1.4683\n",
      "Epoch 3: val_loss did not improve from 1.50092\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 1.4683 - sMAPE_tf: 1.4683 - val_loss: 1.5566 - val_sMAPE_tf: 1.5566\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4897 - sMAPE_tf: 1.4897\n",
      "Epoch 4: val_loss did not improve from 1.50092\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 1.4897 - sMAPE_tf: 1.4897 - val_loss: 1.5922 - val_sMAPE_tf: 1.5922\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5051 - sMAPE_tf: 1.5051\n",
      "Epoch 5: val_loss did not improve from 1.50092\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 1.5051 - sMAPE_tf: 1.5051 - val_loss: 1.6169 - val_sMAPE_tf: 1.6169\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5163 - sMAPE_tf: 1.5163\n",
      "Epoch 6: val_loss did not improve from 1.50092\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 1.5163 - sMAPE_tf: 1.5163 - val_loss: 1.6252 - val_sMAPE_tf: 1.6252\n",
      "Training finished in 5.868607759475708 secconds\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 1.5162 - sMAPE_tf: 1.5162\n",
      "1.5162246227264404\n",
      "1/1 [==============================] - 0s 382ms/step\n",
      "mean_SMAPE:0.8197388600236383\n",
      "mean_MASE:3.1084114782214\n",
      "sim_500_60_nl_he\n",
      "35 27 27\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4587 - sMAPE_tf: 1.4587\n",
      "Epoch 1: val_loss improved from inf to 1.51402, saving model to ./checkpoints/sim_500_60_nl_he_best\n",
      "1/1 [==============================] - 4s 4s/step - loss: 1.4587 - sMAPE_tf: 1.4587 - val_loss: 1.5140 - val_sMAPE_tf: 1.5140\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4863 - sMAPE_tf: 1.4863\n",
      "Epoch 2: val_loss did not improve from 1.51402\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 1.4863 - sMAPE_tf: 1.4863 - val_loss: 1.5682 - val_sMAPE_tf: 1.5682\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5208 - sMAPE_tf: 1.5208\n",
      "Epoch 3: val_loss did not improve from 1.51402\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 1.5208 - sMAPE_tf: 1.5208 - val_loss: 1.6043 - val_sMAPE_tf: 1.6043\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5472 - sMAPE_tf: 1.5472\n",
      "Epoch 4: val_loss did not improve from 1.51402\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 1.5472 - sMAPE_tf: 1.5472 - val_loss: 1.6307 - val_sMAPE_tf: 1.6307\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5622 - sMAPE_tf: 1.5622\n",
      "Epoch 5: val_loss did not improve from 1.51402\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 1.5622 - sMAPE_tf: 1.5622 - val_loss: 1.6318 - val_sMAPE_tf: 1.6318\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5765 - sMAPE_tf: 1.5765\n",
      "Epoch 6: val_loss did not improve from 1.51402\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 1.5765 - sMAPE_tf: 1.5765 - val_loss: 1.6395 - val_sMAPE_tf: 1.6395\n",
      "Training finished in 5.485483169555664 secconds\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 1.5273 - sMAPE_tf: 1.5273\n",
      "1.5272643566131592\n",
      "1/1 [==============================] - 0s 411ms/step\n",
      "mean_SMAPE:1.0369361597202724\n",
      "mean_MASE:4.164063294006862\n",
      "sim_500_60_nl_ho\n",
      "35 27 27\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4664 - sMAPE_tf: 1.4664\n",
      "Epoch 1: val_loss improved from inf to 1.49416, saving model to ./checkpoints/sim_500_60_nl_ho_best\n",
      "1/1 [==============================] - 5s 5s/step - loss: 1.4664 - sMAPE_tf: 1.4664 - val_loss: 1.4942 - val_sMAPE_tf: 1.4942\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4572 - sMAPE_tf: 1.4572\n",
      "Epoch 2: val_loss improved from 1.49416 to 1.49192, saving model to ./checkpoints/sim_500_60_nl_ho_best\n",
      "1/1 [==============================] - 1s 540ms/step - loss: 1.4572 - sMAPE_tf: 1.4572 - val_loss: 1.4919 - val_sMAPE_tf: 1.4919\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4667 - sMAPE_tf: 1.4667\n",
      "Epoch 3: val_loss did not improve from 1.49192\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 1.4667 - sMAPE_tf: 1.4667 - val_loss: 1.5151 - val_sMAPE_tf: 1.5151\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4829 - sMAPE_tf: 1.4829\n",
      "Epoch 4: val_loss did not improve from 1.49192\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 1.4829 - sMAPE_tf: 1.4829 - val_loss: 1.5629 - val_sMAPE_tf: 1.5629\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4965 - sMAPE_tf: 1.4965\n",
      "Epoch 5: val_loss did not improve from 1.49192\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 1.4965 - sMAPE_tf: 1.4965 - val_loss: 1.6160 - val_sMAPE_tf: 1.6160\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5112 - sMAPE_tf: 1.5112\n",
      "Epoch 6: val_loss did not improve from 1.49192\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 1.5112 - sMAPE_tf: 1.5112 - val_loss: 1.6498 - val_sMAPE_tf: 1.6498\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5240 - sMAPE_tf: 1.5240\n",
      "Epoch 7: val_loss did not improve from 1.49192\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 1.5240 - sMAPE_tf: 1.5240 - val_loss: 1.6726 - val_sMAPE_tf: 1.6726\n",
      "Training finished in 6.785083293914795 secconds\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 1.5663 - sMAPE_tf: 1.5663\n",
      "1.5662966966629028\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "mean_SMAPE:0.9866999450090704\n",
      "mean_MASE:3.5196685566611903\n",
      "sim_500_222_l_he\n",
      "197 27 27\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.4975 - sMAPE_tf: 1.4984\n",
      "Epoch 1: val_loss improved from inf to 1.67283, saving model to ./checkpoints/sim_500_222_l_he_best\n",
      "6/6 [==============================] - 4s 220ms/step - loss: 1.4975 - sMAPE_tf: 1.4984 - val_loss: 1.6728 - val_sMAPE_tf: 1.6728\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.4949 - sMAPE_tf: 1.4934\n",
      "Epoch 2: val_loss did not improve from 1.67283\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 1.4949 - sMAPE_tf: 1.4934 - val_loss: 1.7171 - val_sMAPE_tf: 1.7171\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.4719 - sMAPE_tf: 1.4696\n",
      "Epoch 3: val_loss did not improve from 1.67283\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 1.4719 - sMAPE_tf: 1.4696 - val_loss: 1.6870 - val_sMAPE_tf: 1.6870\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.4299 - sMAPE_tf: 1.4276\n",
      "Epoch 4: val_loss improved from 1.67283 to 1.66542, saving model to ./checkpoints/sim_500_222_l_he_best\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 1.4299 - sMAPE_tf: 1.4276 - val_loss: 1.6654 - val_sMAPE_tf: 1.6654\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.3704 - sMAPE_tf: 1.3693\n",
      "Epoch 5: val_loss improved from 1.66542 to 1.56142, saving model to ./checkpoints/sim_500_222_l_he_best\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 1.3704 - sMAPE_tf: 1.3693 - val_loss: 1.5614 - val_sMAPE_tf: 1.5614\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.3158 - sMAPE_tf: 1.3150\n",
      "Epoch 6: val_loss improved from 1.56142 to 1.52763, saving model to ./checkpoints/sim_500_222_l_he_best\n",
      "6/6 [==============================] - 1s 126ms/step - loss: 1.3158 - sMAPE_tf: 1.3150 - val_loss: 1.5276 - val_sMAPE_tf: 1.5276\n",
      "Epoch 7/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.2579 - sMAPE_tf: 1.2579\n",
      "Epoch 7: val_loss improved from 1.52763 to 1.47045, saving model to ./checkpoints/sim_500_222_l_he_best\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 1.2521 - sMAPE_tf: 1.2475 - val_loss: 1.4705 - val_sMAPE_tf: 1.4705\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.1935 - sMAPE_tf: 1.1916\n",
      "Epoch 8: val_loss improved from 1.47045 to 1.44463, saving model to ./checkpoints/sim_500_222_l_he_best\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 1.1935 - sMAPE_tf: 1.1916 - val_loss: 1.4446 - val_sMAPE_tf: 1.4446\n",
      "Epoch 9/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.1466 - sMAPE_tf: 1.1466\n",
      "Epoch 9: val_loss improved from 1.44463 to 1.40228, saving model to ./checkpoints/sim_500_222_l_he_best\n",
      "6/6 [==============================] - 1s 99ms/step - loss: 1.1434 - sMAPE_tf: 1.1410 - val_loss: 1.4023 - val_sMAPE_tf: 1.4023\n",
      "Epoch 10/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.0920 - sMAPE_tf: 1.0920\n",
      "Epoch 10: val_loss improved from 1.40228 to 1.35338, saving model to ./checkpoints/sim_500_222_l_he_best\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 1.0894 - sMAPE_tf: 1.0874 - val_loss: 1.3534 - val_sMAPE_tf: 1.3534\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.0390 - sMAPE_tf: 1.0399\n",
      "Epoch 11: val_loss improved from 1.35338 to 1.33807, saving model to ./checkpoints/sim_500_222_l_he_best\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 1.0390 - sMAPE_tf: 1.0399 - val_loss: 1.3381 - val_sMAPE_tf: 1.3381\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.9957 - sMAPE_tf: 0.9918\n",
      "Epoch 12: val_loss improved from 1.33807 to 1.32342, saving model to ./checkpoints/sim_500_222_l_he_best\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 0.9957 - sMAPE_tf: 0.9918 - val_loss: 1.3234 - val_sMAPE_tf: 1.3234\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.9347 - sMAPE_tf: 0.9358\n",
      "Epoch 13: val_loss did not improve from 1.32342\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 0.9347 - sMAPE_tf: 0.9358 - val_loss: 1.3669 - val_sMAPE_tf: 1.3669\n",
      "Epoch 14/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.8978 - sMAPE_tf: 0.8978\n",
      "Epoch 14: val_loss improved from 1.32342 to 1.24250, saving model to ./checkpoints/sim_500_222_l_he_best\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.8960 - sMAPE_tf: 0.8947 - val_loss: 1.2425 - val_sMAPE_tf: 1.2425\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.8349 - sMAPE_tf: 0.8334\n",
      "Epoch 15: val_loss did not improve from 1.24250\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 0.8349 - sMAPE_tf: 0.8334 - val_loss: 1.3284 - val_sMAPE_tf: 1.3284\n",
      "Epoch 16/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.7650 - sMAPE_tf: 0.7650\n",
      "Epoch 16: val_loss improved from 1.24250 to 1.19672, saving model to ./checkpoints/sim_500_222_l_he_best\n",
      "6/6 [==============================] - 1s 122ms/step - loss: 0.7643 - sMAPE_tf: 0.7638 - val_loss: 1.1967 - val_sMAPE_tf: 1.1967\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.7206 - sMAPE_tf: 0.7205\n",
      "Epoch 17: val_loss improved from 1.19672 to 1.15614, saving model to ./checkpoints/sim_500_222_l_he_best\n",
      "6/6 [==============================] - 1s 126ms/step - loss: 0.7206 - sMAPE_tf: 0.7205 - val_loss: 1.1561 - val_sMAPE_tf: 1.1561\n",
      "Epoch 18/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.6746 - sMAPE_tf: 0.6746\n",
      "Epoch 18: val_loss improved from 1.15614 to 1.09339, saving model to ./checkpoints/sim_500_222_l_he_best\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.6765 - sMAPE_tf: 0.6780 - val_loss: 1.0934 - val_sMAPE_tf: 1.0934\n",
      "Epoch 19/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.6324 - sMAPE_tf: 0.6324\n",
      "Epoch 19: val_loss improved from 1.09339 to 1.08639, saving model to ./checkpoints/sim_500_222_l_he_best\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.6323 - sMAPE_tf: 0.6322 - val_loss: 1.0864 - val_sMAPE_tf: 1.0864\n",
      "Epoch 20/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.6070 - sMAPE_tf: 0.6070\n",
      "Epoch 20: val_loss did not improve from 1.08639\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 0.6052 - sMAPE_tf: 0.6038 - val_loss: 1.0993 - val_sMAPE_tf: 1.0993\n",
      "Epoch 21/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.5718 - sMAPE_tf: 0.5718\n",
      "Epoch 21: val_loss did not improve from 1.08639\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 0.5708 - sMAPE_tf: 0.5701 - val_loss: 1.0878 - val_sMAPE_tf: 1.0878\n",
      "Epoch 22/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.5509 - sMAPE_tf: 0.5509\n",
      "Epoch 22: val_loss did not improve from 1.08639\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 0.5505 - sMAPE_tf: 0.5502 - val_loss: 1.1549 - val_sMAPE_tf: 1.1549\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.5474 - sMAPE_tf: 0.5484\n",
      "Epoch 23: val_loss improved from 1.08639 to 1.02462, saving model to ./checkpoints/sim_500_222_l_he_best\n",
      "6/6 [==============================] - 1s 123ms/step - loss: 0.5474 - sMAPE_tf: 0.5484 - val_loss: 1.0246 - val_sMAPE_tf: 1.0246\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.5413 - sMAPE_tf: 0.5424\n",
      "Epoch 24: val_loss did not improve from 1.02462\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 0.5413 - sMAPE_tf: 0.5424 - val_loss: 1.0392 - val_sMAPE_tf: 1.0392\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.5355 - sMAPE_tf: 0.5350\n",
      "Epoch 25: val_loss did not improve from 1.02462\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 0.5355 - sMAPE_tf: 0.5350 - val_loss: 1.2029 - val_sMAPE_tf: 1.2029\n",
      "Epoch 26/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.5162 - sMAPE_tf: 0.5162\n",
      "Epoch 26: val_loss did not improve from 1.02462\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 0.5156 - sMAPE_tf: 0.5152 - val_loss: 1.0307 - val_sMAPE_tf: 1.0307\n",
      "Epoch 27/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.5016 - sMAPE_tf: 0.5016\n",
      "Epoch 27: val_loss did not improve from 1.02462\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 0.5032 - sMAPE_tf: 0.5046 - val_loss: 1.0638 - val_sMAPE_tf: 1.0638\n",
      "Epoch 28/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4927 - sMAPE_tf: 0.4927\n",
      "Epoch 28: val_loss did not improve from 1.02462\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 0.4929 - sMAPE_tf: 0.4930 - val_loss: 1.0405 - val_sMAPE_tf: 1.0405\n",
      "Training finished in 21.90941047668457 secconds\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 1.1016 - sMAPE_tf: 1.1016\n",
      "1.1015836000442505\n",
      "1/1 [==============================] - 0s 410ms/step\n",
      "mean_SMAPE:0.5213437796035771\n",
      "mean_MASE:2.3730076583377095\n",
      "sim_500_222_l_ho\n",
      "197 27 27\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.4721 - sMAPE_tf: 1.4722\n",
      "Epoch 1: val_loss improved from inf to 1.70634, saving model to ./checkpoints/sim_500_222_l_ho_best\n",
      "6/6 [==============================] - 5s 217ms/step - loss: 1.4721 - sMAPE_tf: 1.4722 - val_loss: 1.7063 - val_sMAPE_tf: 1.7063\n",
      "Epoch 2/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.4556 - sMAPE_tf: 1.4556\n",
      "Epoch 2: val_loss improved from 1.70634 to 1.66767, saving model to ./checkpoints/sim_500_222_l_ho_best\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 1.4551 - sMAPE_tf: 1.4547 - val_loss: 1.6677 - val_sMAPE_tf: 1.6677\n",
      "Epoch 3/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.4239 - sMAPE_tf: 1.4239\n",
      "Epoch 3: val_loss improved from 1.66767 to 1.65902, saving model to ./checkpoints/sim_500_222_l_ho_best\n",
      "6/6 [==============================] - 1s 124ms/step - loss: 1.4210 - sMAPE_tf: 1.4188 - val_loss: 1.6590 - val_sMAPE_tf: 1.6590\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.3627 - sMAPE_tf: 1.3609\n",
      "Epoch 4: val_loss improved from 1.65902 to 1.58500, saving model to ./checkpoints/sim_500_222_l_ho_best\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 1.3627 - sMAPE_tf: 1.3609 - val_loss: 1.5850 - val_sMAPE_tf: 1.5850\n",
      "Epoch 5/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.2989 - sMAPE_tf: 1.2989\n",
      "Epoch 5: val_loss improved from 1.58500 to 1.57096, saving model to ./checkpoints/sim_500_222_l_ho_best\n",
      "6/6 [==============================] - 1s 123ms/step - loss: 1.2955 - sMAPE_tf: 1.2928 - val_loss: 1.5710 - val_sMAPE_tf: 1.5710\n",
      "Epoch 6/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.2427 - sMAPE_tf: 1.2427\n",
      "Epoch 6: val_loss did not improve from 1.57096\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 1.2390 - sMAPE_tf: 1.2362 - val_loss: 1.5961 - val_sMAPE_tf: 1.5961\n",
      "Epoch 7/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.2043 - sMAPE_tf: 1.2043\n",
      "Epoch 7: val_loss did not improve from 1.57096\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 1.2029 - sMAPE_tf: 1.2018 - val_loss: 1.6034 - val_sMAPE_tf: 1.6034\n",
      "Epoch 8/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.1538 - sMAPE_tf: 1.1538\n",
      "Epoch 8: val_loss did not improve from 1.57096\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 1.1557 - sMAPE_tf: 1.1572 - val_loss: 1.5952 - val_sMAPE_tf: 1.5952\n",
      "Epoch 9/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.0936 - sMAPE_tf: 1.0936\n",
      "Epoch 9: val_loss improved from 1.57096 to 1.55882, saving model to ./checkpoints/sim_500_222_l_ho_best\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 1.0906 - sMAPE_tf: 1.0883 - val_loss: 1.5588 - val_sMAPE_tf: 1.5588\n",
      "Epoch 10/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.0385 - sMAPE_tf: 1.0385\n",
      "Epoch 10: val_loss did not improve from 1.55882\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 1.0398 - sMAPE_tf: 1.0408 - val_loss: 1.5739 - val_sMAPE_tf: 1.5739\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.9892 - sMAPE_tf: 0.9930\n",
      "Epoch 11: val_loss improved from 1.55882 to 1.48828, saving model to ./checkpoints/sim_500_222_l_ho_best\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 0.9892 - sMAPE_tf: 0.9930 - val_loss: 1.4883 - val_sMAPE_tf: 1.4883\n",
      "Epoch 12/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.9511 - sMAPE_tf: 0.9511\n",
      "Epoch 12: val_loss improved from 1.48828 to 1.40707, saving model to ./checkpoints/sim_500_222_l_ho_best\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 0.9533 - sMAPE_tf: 0.9550 - val_loss: 1.4071 - val_sMAPE_tf: 1.4071\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.9124 - sMAPE_tf: 0.9110\n",
      "Epoch 13: val_loss improved from 1.40707 to 1.37067, saving model to ./checkpoints/sim_500_222_l_ho_best\n",
      "6/6 [==============================] - 1s 121ms/step - loss: 0.9124 - sMAPE_tf: 0.9110 - val_loss: 1.3707 - val_sMAPE_tf: 1.3707\n",
      "Epoch 14/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.8684 - sMAPE_tf: 0.8684\n",
      "Epoch 14: val_loss improved from 1.37067 to 1.29466, saving model to ./checkpoints/sim_500_222_l_ho_best\n",
      "6/6 [==============================] - 1s 122ms/step - loss: 0.8687 - sMAPE_tf: 0.8689 - val_loss: 1.2947 - val_sMAPE_tf: 1.2947\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.8501 - sMAPE_tf: 0.8484\n",
      "Epoch 15: val_loss improved from 1.29466 to 1.28300, saving model to ./checkpoints/sim_500_222_l_ho_best\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 0.8501 - sMAPE_tf: 0.8484 - val_loss: 1.2830 - val_sMAPE_tf: 1.2830\n",
      "Epoch 16/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.8055 - sMAPE_tf: 0.8055\n",
      "Epoch 16: val_loss improved from 1.28300 to 1.19675, saving model to ./checkpoints/sim_500_222_l_ho_best\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 0.8047 - sMAPE_tf: 0.8040 - val_loss: 1.1968 - val_sMAPE_tf: 1.1968\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.7589 - sMAPE_tf: 0.7601\n",
      "Epoch 17: val_loss improved from 1.19675 to 1.11071, saving model to ./checkpoints/sim_500_222_l_ho_best\n",
      "6/6 [==============================] - 1s 125ms/step - loss: 0.7589 - sMAPE_tf: 0.7601 - val_loss: 1.1107 - val_sMAPE_tf: 1.1107\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.7184 - sMAPE_tf: 0.7173\n",
      "Epoch 18: val_loss did not improve from 1.11071\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.7184 - sMAPE_tf: 0.7173 - val_loss: 1.1187 - val_sMAPE_tf: 1.1187\n",
      "Epoch 19/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.6771 - sMAPE_tf: 0.6771\n",
      "Epoch 19: val_loss did not improve from 1.11071\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.6776 - sMAPE_tf: 0.6781 - val_loss: 1.1187 - val_sMAPE_tf: 1.1187\n",
      "Epoch 20/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.6459 - sMAPE_tf: 0.6459\n",
      "Epoch 20: val_loss improved from 1.11071 to 1.09003, saving model to ./checkpoints/sim_500_222_l_ho_best\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.6468 - sMAPE_tf: 0.6476 - val_loss: 1.0900 - val_sMAPE_tf: 1.0900\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.6188 - sMAPE_tf: 0.6185\n",
      "Epoch 21: val_loss did not improve from 1.09003\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 0.6188 - sMAPE_tf: 0.6185 - val_loss: 1.1654 - val_sMAPE_tf: 1.1654\n",
      "Epoch 22/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.5886 - sMAPE_tf: 0.5886\n",
      "Epoch 22: val_loss improved from 1.09003 to 1.06861, saving model to ./checkpoints/sim_500_222_l_ho_best\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.5914 - sMAPE_tf: 0.5936 - val_loss: 1.0686 - val_sMAPE_tf: 1.0686\n",
      "Epoch 23/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.5911 - sMAPE_tf: 0.5911\n",
      "Epoch 23: val_loss did not improve from 1.06861\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 0.5898 - sMAPE_tf: 0.5889 - val_loss: 1.1006 - val_sMAPE_tf: 1.1006\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.5685 - sMAPE_tf: 0.5714\n",
      "Epoch 24: val_loss improved from 1.06861 to 1.06509, saving model to ./checkpoints/sim_500_222_l_ho_best\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.5685 - sMAPE_tf: 0.5714 - val_loss: 1.0651 - val_sMAPE_tf: 1.0651\n",
      "Epoch 25/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.5612 - sMAPE_tf: 0.5612\n",
      "Epoch 25: val_loss did not improve from 1.06509\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.5610 - sMAPE_tf: 0.5608 - val_loss: 1.0832 - val_sMAPE_tf: 1.0832\n",
      "Epoch 26/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.5481 - sMAPE_tf: 0.5481\n",
      "Epoch 26: val_loss did not improve from 1.06509\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 0.5470 - sMAPE_tf: 0.5461 - val_loss: 1.1274 - val_sMAPE_tf: 1.1274\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.5372 - sMAPE_tf: 0.5388\n",
      "Epoch 27: val_loss did not improve from 1.06509\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.5372 - sMAPE_tf: 0.5388 - val_loss: 1.0759 - val_sMAPE_tf: 1.0759\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.5239 - sMAPE_tf: 0.5254\n",
      "Epoch 28: val_loss improved from 1.06509 to 1.04438, saving model to ./checkpoints/sim_500_222_l_ho_best\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.5239 - sMAPE_tf: 0.5254 - val_loss: 1.0444 - val_sMAPE_tf: 1.0444\n",
      "Epoch 29/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.5220 - sMAPE_tf: 0.5220\n",
      "Epoch 29: val_loss did not improve from 1.04438\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.5224 - sMAPE_tf: 0.5227 - val_loss: 1.1097 - val_sMAPE_tf: 1.1097\n",
      "Epoch 30/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.5120 - sMAPE_tf: 0.5120\n",
      "Epoch 30: val_loss improved from 1.04438 to 1.04338, saving model to ./checkpoints/sim_500_222_l_ho_best\n",
      "6/6 [==============================] - 1s 124ms/step - loss: 0.5132 - sMAPE_tf: 0.5142 - val_loss: 1.0434 - val_sMAPE_tf: 1.0434\n",
      "Epoch 31/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.5031 - sMAPE_tf: 0.5031\n",
      "Epoch 31: val_loss did not improve from 1.04338\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.5047 - sMAPE_tf: 0.5060 - val_loss: 1.1393 - val_sMAPE_tf: 1.1393\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4855 - sMAPE_tf: 0.4867\n",
      "Epoch 32: val_loss improved from 1.04338 to 1.02794, saving model to ./checkpoints/sim_500_222_l_ho_best\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4855 - sMAPE_tf: 0.4867 - val_loss: 1.0279 - val_sMAPE_tf: 1.0279\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4813 - sMAPE_tf: 0.4839\n",
      "Epoch 33: val_loss did not improve from 1.02794\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 0.4813 - sMAPE_tf: 0.4839 - val_loss: 1.1515 - val_sMAPE_tf: 1.1515\n",
      "Epoch 34/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4774 - sMAPE_tf: 0.4774\n",
      "Epoch 34: val_loss improved from 1.02794 to 1.02081, saving model to ./checkpoints/sim_500_222_l_ho_best\n",
      "6/6 [==============================] - 1s 125ms/step - loss: 0.4793 - sMAPE_tf: 0.4807 - val_loss: 1.0208 - val_sMAPE_tf: 1.0208\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4761 - sMAPE_tf: 0.4749\n",
      "Epoch 35: val_loss did not improve from 1.02081\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 0.4761 - sMAPE_tf: 0.4749 - val_loss: 1.0717 - val_sMAPE_tf: 1.0717\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4692 - sMAPE_tf: 0.4705\n",
      "Epoch 36: val_loss did not improve from 1.02081\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 0.4692 - sMAPE_tf: 0.4705 - val_loss: 1.0433 - val_sMAPE_tf: 1.0433\n",
      "Epoch 37/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4629 - sMAPE_tf: 0.4629\n",
      "Epoch 37: val_loss did not improve from 1.02081\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.4629 - sMAPE_tf: 0.4629 - val_loss: 1.0638 - val_sMAPE_tf: 1.0638\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4587 - sMAPE_tf: 0.4630\n",
      "Epoch 38: val_loss did not improve from 1.02081\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 0.4587 - sMAPE_tf: 0.4630 - val_loss: 1.0548 - val_sMAPE_tf: 1.0548\n",
      "Epoch 39/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4479 - sMAPE_tf: 0.4479\n",
      "Epoch 39: val_loss did not improve from 1.02081\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 0.4482 - sMAPE_tf: 0.4483 - val_loss: 1.0743 - val_sMAPE_tf: 1.0743\n",
      "Training finished in 28.487102508544922 secconds\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 1.0743 - sMAPE_tf: 1.0743\n",
      "1.0743439197540283\n",
      "1/1 [==============================] - 0s 404ms/step\n",
      "mean_SMAPE:0.5165782768105996\n",
      "mean_MASE:2.277300524745917\n",
      "sim_500_222_nl_he\n",
      "197 27 27\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.4852 - sMAPE_tf: 1.4852\n",
      "Epoch 1: val_loss improved from inf to 1.49165, saving model to ./checkpoints/sim_500_222_nl_he_best\n",
      "6/6 [==============================] - 5s 225ms/step - loss: 1.4834 - sMAPE_tf: 1.4819 - val_loss: 1.4916 - val_sMAPE_tf: 1.4916\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.4750 - sMAPE_tf: 1.4762\n",
      "Epoch 2: val_loss did not improve from 1.49165\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 1.4750 - sMAPE_tf: 1.4762 - val_loss: 1.5459 - val_sMAPE_tf: 1.5459\n",
      "Epoch 3/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.4361 - sMAPE_tf: 1.4361\n",
      "Epoch 3: val_loss improved from 1.49165 to 1.48744, saving model to ./checkpoints/sim_500_222_nl_he_best\n",
      "6/6 [==============================] - 1s 121ms/step - loss: 1.4370 - sMAPE_tf: 1.4377 - val_loss: 1.4874 - val_sMAPE_tf: 1.4874\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.3921 - sMAPE_tf: 1.3914\n",
      "Epoch 4: val_loss improved from 1.48744 to 1.44646, saving model to ./checkpoints/sim_500_222_nl_he_best\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 1.3921 - sMAPE_tf: 1.3914 - val_loss: 1.4465 - val_sMAPE_tf: 1.4465\n",
      "Epoch 5/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.3350 - sMAPE_tf: 1.3350\n",
      "Epoch 5: val_loss improved from 1.44646 to 1.39428, saving model to ./checkpoints/sim_500_222_nl_he_best\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 1.3344 - sMAPE_tf: 1.3340 - val_loss: 1.3943 - val_sMAPE_tf: 1.3943\n",
      "Epoch 6/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.2889 - sMAPE_tf: 1.2889\n",
      "Epoch 6: val_loss improved from 1.39428 to 1.31504, saving model to ./checkpoints/sim_500_222_nl_he_best\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 1.2880 - sMAPE_tf: 1.2872 - val_loss: 1.3150 - val_sMAPE_tf: 1.3150\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.2358 - sMAPE_tf: 1.2383\n",
      "Epoch 7: val_loss improved from 1.31504 to 1.30107, saving model to ./checkpoints/sim_500_222_nl_he_best\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 1.2358 - sMAPE_tf: 1.2383 - val_loss: 1.3011 - val_sMAPE_tf: 1.3011\n",
      "Epoch 8/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.1727 - sMAPE_tf: 1.1727\n",
      "Epoch 8: val_loss improved from 1.30107 to 1.25653, saving model to ./checkpoints/sim_500_222_nl_he_best\n",
      "6/6 [==============================] - 1s 121ms/step - loss: 1.1787 - sMAPE_tf: 1.1833 - val_loss: 1.2565 - val_sMAPE_tf: 1.2565\n",
      "Epoch 9/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.1269 - sMAPE_tf: 1.1269\n",
      "Epoch 9: val_loss improved from 1.25653 to 1.24886, saving model to ./checkpoints/sim_500_222_nl_he_best\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 1.1267 - sMAPE_tf: 1.1265 - val_loss: 1.2489 - val_sMAPE_tf: 1.2489\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.0750 - sMAPE_tf: 1.0814\n",
      "Epoch 10: val_loss improved from 1.24886 to 1.20976, saving model to ./checkpoints/sim_500_222_nl_he_best\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 1.0750 - sMAPE_tf: 1.0814 - val_loss: 1.2098 - val_sMAPE_tf: 1.2098\n",
      "Epoch 11/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.0220 - sMAPE_tf: 1.0220\n",
      "Epoch 11: val_loss improved from 1.20976 to 1.19671, saving model to ./checkpoints/sim_500_222_nl_he_best\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 1.0208 - sMAPE_tf: 1.0199 - val_loss: 1.1967 - val_sMAPE_tf: 1.1967\n",
      "Epoch 12/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.9612 - sMAPE_tf: 0.9612\n",
      "Epoch 12: val_loss improved from 1.19671 to 1.19089, saving model to ./checkpoints/sim_500_222_nl_he_best\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.9714 - sMAPE_tf: 0.9794 - val_loss: 1.1909 - val_sMAPE_tf: 1.1909\n",
      "Epoch 13/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.9282 - sMAPE_tf: 0.9282\n",
      "Epoch 13: val_loss improved from 1.19089 to 1.16017, saving model to ./checkpoints/sim_500_222_nl_he_best\n",
      "6/6 [==============================] - 1s 126ms/step - loss: 0.9277 - sMAPE_tf: 0.9274 - val_loss: 1.1602 - val_sMAPE_tf: 1.1602\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.8705 - sMAPE_tf: 0.8731\n",
      "Epoch 14: val_loss improved from 1.16017 to 1.13873, saving model to ./checkpoints/sim_500_222_nl_he_best\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 0.8705 - sMAPE_tf: 0.8731 - val_loss: 1.1387 - val_sMAPE_tf: 1.1387\n",
      "Epoch 15/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.8266 - sMAPE_tf: 0.8266\n",
      "Epoch 15: val_loss improved from 1.13873 to 1.11455, saving model to ./checkpoints/sim_500_222_nl_he_best\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.8311 - sMAPE_tf: 0.8347 - val_loss: 1.1145 - val_sMAPE_tf: 1.1145\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.7945 - sMAPE_tf: 0.7965\n",
      "Epoch 16: val_loss did not improve from 1.11455\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.7945 - sMAPE_tf: 0.7965 - val_loss: 1.1284 - val_sMAPE_tf: 1.1284\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.7426 - sMAPE_tf: 0.7421\n",
      "Epoch 17: val_loss did not improve from 1.11455\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 0.7426 - sMAPE_tf: 0.7421 - val_loss: 1.1338 - val_sMAPE_tf: 1.1338\n",
      "Epoch 18/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.7118 - sMAPE_tf: 0.7118\n",
      "Epoch 18: val_loss did not improve from 1.11455\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 0.7124 - sMAPE_tf: 0.7129 - val_loss: 1.1906 - val_sMAPE_tf: 1.1906\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.6845 - sMAPE_tf: 0.6853\n",
      "Epoch 19: val_loss did not improve from 1.11455\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 0.6845 - sMAPE_tf: 0.6853 - val_loss: 1.1564 - val_sMAPE_tf: 1.1564\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.6559 - sMAPE_tf: 0.6562\n",
      "Epoch 20: val_loss did not improve from 1.11455\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 0.6559 - sMAPE_tf: 0.6562 - val_loss: 1.1379 - val_sMAPE_tf: 1.1379\n",
      "Training finished in 17.560532093048096 secconds\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 1.2600 - sMAPE_tf: 1.2600\n",
      "1.2600032091140747\n",
      "1/1 [==============================] - 0s 385ms/step\n",
      "mean_SMAPE:0.4436097320721521\n",
      "mean_MASE:1.3027454535118104\n",
      "sim_500_222_nl_ho\n",
      "197 27 27\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.4893 - sMAPE_tf: 1.4895\n",
      "Epoch 1: val_loss improved from inf to 1.55093, saving model to ./checkpoints/sim_500_222_nl_ho_best\n",
      "6/6 [==============================] - 7s 232ms/step - loss: 1.4893 - sMAPE_tf: 1.4895 - val_loss: 1.5509 - val_sMAPE_tf: 1.5509\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.4724 - sMAPE_tf: 1.4708\n",
      "Epoch 2: val_loss improved from 1.55093 to 1.52863, saving model to ./checkpoints/sim_500_222_nl_ho_best\n",
      "6/6 [==============================] - 1s 126ms/step - loss: 1.4724 - sMAPE_tf: 1.4708 - val_loss: 1.5286 - val_sMAPE_tf: 1.5286\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.4286 - sMAPE_tf: 1.4273\n",
      "Epoch 3: val_loss improved from 1.52863 to 1.52775, saving model to ./checkpoints/sim_500_222_nl_ho_best\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 1.4286 - sMAPE_tf: 1.4273 - val_loss: 1.5278 - val_sMAPE_tf: 1.5278\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.3769 - sMAPE_tf: 1.3751\n",
      "Epoch 4: val_loss improved from 1.52775 to 1.47912, saving model to ./checkpoints/sim_500_222_nl_ho_best\n",
      "6/6 [==============================] - 1s 123ms/step - loss: 1.3769 - sMAPE_tf: 1.3751 - val_loss: 1.4791 - val_sMAPE_tf: 1.4791\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.3309 - sMAPE_tf: 1.3283\n",
      "Epoch 5: val_loss improved from 1.47912 to 1.46948, saving model to ./checkpoints/sim_500_222_nl_ho_best\n",
      "6/6 [==============================] - 1s 122ms/step - loss: 1.3309 - sMAPE_tf: 1.3283 - val_loss: 1.4695 - val_sMAPE_tf: 1.4695\n",
      "Epoch 6/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.2798 - sMAPE_tf: 1.2798\n",
      "Epoch 6: val_loss improved from 1.46948 to 1.40218, saving model to ./checkpoints/sim_500_222_nl_ho_best\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 1.2772 - sMAPE_tf: 1.2752 - val_loss: 1.4022 - val_sMAPE_tf: 1.4022\n",
      "Epoch 7/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.2209 - sMAPE_tf: 1.2209\n",
      "Epoch 7: val_loss improved from 1.40218 to 1.33200, saving model to ./checkpoints/sim_500_222_nl_ho_best\n",
      "6/6 [==============================] - 1s 124ms/step - loss: 1.2199 - sMAPE_tf: 1.2191 - val_loss: 1.3320 - val_sMAPE_tf: 1.3320\n",
      "Epoch 8/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.1593 - sMAPE_tf: 1.1593\n",
      "Epoch 8: val_loss improved from 1.33200 to 1.27240, saving model to ./checkpoints/sim_500_222_nl_ho_best\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 1.1634 - sMAPE_tf: 1.1666 - val_loss: 1.2724 - val_sMAPE_tf: 1.2724\n",
      "Epoch 9/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.1076 - sMAPE_tf: 1.1076\n",
      "Epoch 9: val_loss improved from 1.27240 to 1.22500, saving model to ./checkpoints/sim_500_222_nl_ho_best\n",
      "6/6 [==============================] - 1s 122ms/step - loss: 1.1088 - sMAPE_tf: 1.1097 - val_loss: 1.2250 - val_sMAPE_tf: 1.2250\n",
      "Epoch 10/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.0702 - sMAPE_tf: 1.0702\n",
      "Epoch 10: val_loss improved from 1.22500 to 1.21756, saving model to ./checkpoints/sim_500_222_nl_ho_best\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 1.0643 - sMAPE_tf: 1.0596 - val_loss: 1.2176 - val_sMAPE_tf: 1.2176\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.0102 - sMAPE_tf: 1.0108\n",
      "Epoch 11: val_loss improved from 1.21756 to 1.17435, saving model to ./checkpoints/sim_500_222_nl_ho_best\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 1.0102 - sMAPE_tf: 1.0108 - val_loss: 1.1743 - val_sMAPE_tf: 1.1743\n",
      "Epoch 12/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.9735 - sMAPE_tf: 0.9735\n",
      "Epoch 12: val_loss did not improve from 1.17435\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.9708 - sMAPE_tf: 0.9687 - val_loss: 1.2166 - val_sMAPE_tf: 1.2166\n",
      "Epoch 13/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.9321 - sMAPE_tf: 0.9321\n",
      "Epoch 13: val_loss improved from 1.17435 to 1.15782, saving model to ./checkpoints/sim_500_222_nl_ho_best\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.9297 - sMAPE_tf: 0.9277 - val_loss: 1.1578 - val_sMAPE_tf: 1.1578\n",
      "Epoch 14/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.8806 - sMAPE_tf: 0.8806\n",
      "Epoch 14: val_loss improved from 1.15782 to 1.13215, saving model to ./checkpoints/sim_500_222_nl_ho_best\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 0.8762 - sMAPE_tf: 0.8727 - val_loss: 1.1322 - val_sMAPE_tf: 1.1322\n",
      "Epoch 15/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.8260 - sMAPE_tf: 0.8260\n",
      "Epoch 15: val_loss did not improve from 1.13215\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 0.8234 - sMAPE_tf: 0.8214 - val_loss: 1.1510 - val_sMAPE_tf: 1.1510\n",
      "Epoch 16/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.7717 - sMAPE_tf: 0.7717\n",
      "Epoch 16: val_loss did not improve from 1.13215\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 0.7688 - sMAPE_tf: 0.7665 - val_loss: 1.1629 - val_sMAPE_tf: 1.1629\n",
      "Epoch 17/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.7219 - sMAPE_tf: 0.7219\n",
      "Epoch 17: val_loss did not improve from 1.13215\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.7198 - sMAPE_tf: 0.7182 - val_loss: 1.2076 - val_sMAPE_tf: 1.2076\n",
      "Epoch 18/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.7073 - sMAPE_tf: 0.7073\n",
      "Epoch 18: val_loss did not improve from 1.13215\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.7049 - sMAPE_tf: 0.7031 - val_loss: 1.2323 - val_sMAPE_tf: 1.2323\n",
      "Epoch 19/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.6754 - sMAPE_tf: 0.6754\n",
      "Epoch 19: val_loss did not improve from 1.13215\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.6784 - sMAPE_tf: 0.6808 - val_loss: 1.1374 - val_sMAPE_tf: 1.1374\n",
      "Training finished in 18.230535745620728 secconds\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 1.2483 - sMAPE_tf: 1.2483\n",
      "1.2483458518981934\n",
      "1/1 [==============================] - 0s 390ms/step\n",
      "mean_SMAPE:0.4741002461730343\n",
      "mean_MASE:1.4198144737785483\n"
     ]
    }
   ],
   "source": [
    "dataset_name_test = ['sim_10_60_l_he', 'sim_10_60_l_ho',\\\n",
    "                     'sim_10_60_nl_he', 'sim_10_60_nl_ho',\\\n",
    "                     'sim_10_222_l_he', 'sim_10_222_l_ho',\\\n",
    "                     'sim_10_222_nl_he', 'sim_10_222_nl_ho',\\\n",
    "                     'sim_101_60_l_he', 'sim_101_60_l_ho',\\\n",
    "                     'sim_101_60_nl_he', 'sim_101_60_nl_ho',\\\n",
    "                     'sim_101_222_l_he', 'sim_101_222_l_ho',\\\n",
    "                     'sim_101_222_nl_he', 'sim_101_222_nl_ho',\\\n",
    "                     'sim_500_60_l_he', 'sim_500_60_l_ho',\\\n",
    "                     'sim_500_60_nl_he', 'sim_500_60_nl_ho',\\\n",
    "                     'sim_500_222_l_he', 'sim_500_222_l_ho',\\\n",
    "                     'sim_500_222_nl_he', 'sim_500_222_nl_ho']\n",
    "dataset_type = 'sim'\n",
    "forecast_horizon=12\n",
    "\n",
    "for i in dataset_name_test:\n",
    "    print(i)\n",
    "    tsmixer_eval(i,dataset_type,forecast_horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "dataset_name = 'sim_10_60_l_he'\n",
    "data_type = 'sim'\n",
    "data_index = pd.read_csv('./datasets/text_data/'+data_type+'/'+dataset_name+'.csv')\n",
    "tnc = [0] * int((len(data_index.columns)-1)/2) +\\\n",
    "    [1] * int((len(data_index.columns)-1)-int((len(data_index.columns)-1)/2))\n",
    "\n",
    "data_index = data_index.iloc[:,1:]\n",
    "# a boolean list to denote whether the unit is treated or controled\n",
    "tnc_bool = [bool(value) for value in tnc]\n",
    "flipped_tnc_bool = [not value for value in tnc_bool]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10_6_60_linear_heterogeneous</th>\n",
       "      <th>10_7_60_linear_heterogeneous</th>\n",
       "      <th>10_8_60_linear_heterogeneous</th>\n",
       "      <th>10_9_60_linear_heterogeneous</th>\n",
       "      <th>10_10_60_linear_heterogeneous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>9.617934</td>\n",
       "      <td>2.880725</td>\n",
       "      <td>15.099995</td>\n",
       "      <td>10.254313</td>\n",
       "      <td>6.345929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>9.426890</td>\n",
       "      <td>2.291242</td>\n",
       "      <td>15.001862</td>\n",
       "      <td>10.363391</td>\n",
       "      <td>6.504824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>9.041199</td>\n",
       "      <td>1.940018</td>\n",
       "      <td>12.792860</td>\n",
       "      <td>10.535353</td>\n",
       "      <td>6.825071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>8.898806</td>\n",
       "      <td>1.746893</td>\n",
       "      <td>15.069770</td>\n",
       "      <td>11.079612</td>\n",
       "      <td>6.582064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>8.518961</td>\n",
       "      <td>1.750235</td>\n",
       "      <td>15.151043</td>\n",
       "      <td>11.118020</td>\n",
       "      <td>6.828245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>8.204192</td>\n",
       "      <td>1.905635</td>\n",
       "      <td>15.578149</td>\n",
       "      <td>11.333678</td>\n",
       "      <td>6.541944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>7.634701</td>\n",
       "      <td>2.479341</td>\n",
       "      <td>15.005937</td>\n",
       "      <td>11.225567</td>\n",
       "      <td>6.735053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>7.409369</td>\n",
       "      <td>2.418841</td>\n",
       "      <td>12.775508</td>\n",
       "      <td>10.941201</td>\n",
       "      <td>6.955601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>7.507990</td>\n",
       "      <td>2.676017</td>\n",
       "      <td>12.766202</td>\n",
       "      <td>11.048612</td>\n",
       "      <td>6.737537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>6.546740</td>\n",
       "      <td>2.850685</td>\n",
       "      <td>12.455028</td>\n",
       "      <td>9.987595</td>\n",
       "      <td>6.496371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>5.857119</td>\n",
       "      <td>3.149199</td>\n",
       "      <td>12.585050</td>\n",
       "      <td>9.345481</td>\n",
       "      <td>6.283894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>5.331190</td>\n",
       "      <td>3.879921</td>\n",
       "      <td>12.673118</td>\n",
       "      <td>8.400910</td>\n",
       "      <td>5.751389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    10_6_60_linear_heterogeneous  10_7_60_linear_heterogeneous  \\\n",
       "48                      9.617934                      2.880725   \n",
       "49                      9.426890                      2.291242   \n",
       "50                      9.041199                      1.940018   \n",
       "51                      8.898806                      1.746893   \n",
       "52                      8.518961                      1.750235   \n",
       "53                      8.204192                      1.905635   \n",
       "54                      7.634701                      2.479341   \n",
       "55                      7.409369                      2.418841   \n",
       "56                      7.507990                      2.676017   \n",
       "57                      6.546740                      2.850685   \n",
       "58                      5.857119                      3.149199   \n",
       "59                      5.331190                      3.879921   \n",
       "\n",
       "    10_8_60_linear_heterogeneous  10_9_60_linear_heterogeneous  \\\n",
       "48                     15.099995                     10.254313   \n",
       "49                     15.001862                     10.363391   \n",
       "50                     12.792860                     10.535353   \n",
       "51                     15.069770                     11.079612   \n",
       "52                     15.151043                     11.118020   \n",
       "53                     15.578149                     11.333678   \n",
       "54                     15.005937                     11.225567   \n",
       "55                     12.775508                     10.941201   \n",
       "56                     12.766202                     11.048612   \n",
       "57                     12.455028                      9.987595   \n",
       "58                     12.585050                      9.345481   \n",
       "59                     12.673118                      8.400910   \n",
       "\n",
       "    10_10_60_linear_heterogeneous  \n",
       "48                       6.345929  \n",
       "49                       6.504824  \n",
       "50                       6.825071  \n",
       "51                       6.582064  \n",
       "52                       6.828245  \n",
       "53                       6.541944  \n",
       "54                       6.735053  \n",
       "55                       6.955601  \n",
       "56                       6.737537  \n",
       "57                       6.496371  \n",
       "58                       6.283894  \n",
       "59                       5.751389  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_treated_intervened = data_index[data_index.columns[tnc_bool]]\n",
    "actual_treated_intervened_A = actual_treated_intervened.iloc[-12:, :]\n",
    "actual_treated_intervened_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_85964/1948038984.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  actual_treated_intervened_A[actual_treated_intervened_A>quantile_90] = \\\n",
      "/tmp/ipykernel_85964/1948038984.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  actual_treated_intervened_A[actual_treated_intervened_A>quantile_90] = \\\n"
     ]
    }
   ],
   "source": [
    "actual_treated_intervened_A[actual_treated_intervened_A>quantile_90] = \\\n",
    "            actual_treated_intervened_A[actual_treated_intervened_A>quantile_90]-\\\n",
    "              np.std(np.ravel(actual_treated_intervened.iloc[:-11, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.160814242673911"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quantile_90 = actual_treated_intervened.iloc[-12:, :].T.apply(lambda row: row.quantile([0.9])).T\n",
    "quantile_90 = np.quantile(np.ravel(actual_treated_intervened), 0.9)\n",
    "quantile_90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10_6_60_linear_heterogeneous</th>\n",
       "      <th>10_7_60_linear_heterogeneous</th>\n",
       "      <th>10_8_60_linear_heterogeneous</th>\n",
       "      <th>10_9_60_linear_heterogeneous</th>\n",
       "      <th>10_10_60_linear_heterogeneous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>9.617934</td>\n",
       "      <td>2.880725</td>\n",
       "      <td>12.876451</td>\n",
       "      <td>10.254313</td>\n",
       "      <td>6.345929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>9.426890</td>\n",
       "      <td>2.291242</td>\n",
       "      <td>12.778318</td>\n",
       "      <td>10.363391</td>\n",
       "      <td>6.504824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>9.041199</td>\n",
       "      <td>1.940018</td>\n",
       "      <td>10.569317</td>\n",
       "      <td>10.535353</td>\n",
       "      <td>6.825071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>8.898806</td>\n",
       "      <td>1.746893</td>\n",
       "      <td>12.846226</td>\n",
       "      <td>11.079612</td>\n",
       "      <td>6.582064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>8.518961</td>\n",
       "      <td>1.750235</td>\n",
       "      <td>12.927499</td>\n",
       "      <td>11.118020</td>\n",
       "      <td>6.828245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>8.204192</td>\n",
       "      <td>1.905635</td>\n",
       "      <td>13.354605</td>\n",
       "      <td>9.110134</td>\n",
       "      <td>6.541944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>7.634701</td>\n",
       "      <td>2.479341</td>\n",
       "      <td>12.782394</td>\n",
       "      <td>9.002023</td>\n",
       "      <td>6.735053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>7.409369</td>\n",
       "      <td>2.418841</td>\n",
       "      <td>10.551964</td>\n",
       "      <td>10.941201</td>\n",
       "      <td>6.955601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>7.507990</td>\n",
       "      <td>2.676017</td>\n",
       "      <td>10.542658</td>\n",
       "      <td>11.048612</td>\n",
       "      <td>6.737537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>6.546740</td>\n",
       "      <td>2.850685</td>\n",
       "      <td>10.231485</td>\n",
       "      <td>9.987595</td>\n",
       "      <td>6.496371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>5.857119</td>\n",
       "      <td>3.149199</td>\n",
       "      <td>10.361506</td>\n",
       "      <td>9.345481</td>\n",
       "      <td>6.283894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>5.331190</td>\n",
       "      <td>3.879921</td>\n",
       "      <td>10.449574</td>\n",
       "      <td>8.400910</td>\n",
       "      <td>5.751389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    10_6_60_linear_heterogeneous  10_7_60_linear_heterogeneous  \\\n",
       "48                      9.617934                      2.880725   \n",
       "49                      9.426890                      2.291242   \n",
       "50                      9.041199                      1.940018   \n",
       "51                      8.898806                      1.746893   \n",
       "52                      8.518961                      1.750235   \n",
       "53                      8.204192                      1.905635   \n",
       "54                      7.634701                      2.479341   \n",
       "55                      7.409369                      2.418841   \n",
       "56                      7.507990                      2.676017   \n",
       "57                      6.546740                      2.850685   \n",
       "58                      5.857119                      3.149199   \n",
       "59                      5.331190                      3.879921   \n",
       "\n",
       "    10_8_60_linear_heterogeneous  10_9_60_linear_heterogeneous  \\\n",
       "48                     12.876451                     10.254313   \n",
       "49                     12.778318                     10.363391   \n",
       "50                     10.569317                     10.535353   \n",
       "51                     12.846226                     11.079612   \n",
       "52                     12.927499                     11.118020   \n",
       "53                     13.354605                      9.110134   \n",
       "54                     12.782394                      9.002023   \n",
       "55                     10.551964                     10.941201   \n",
       "56                     10.542658                     11.048612   \n",
       "57                     10.231485                      9.987595   \n",
       "58                     10.361506                      9.345481   \n",
       "59                     10.449574                      8.400910   \n",
       "\n",
       "    10_10_60_linear_heterogeneous  \n",
       "48                       6.345929  \n",
       "49                       6.504824  \n",
       "50                       6.825071  \n",
       "51                       6.582064  \n",
       "52                       6.828245  \n",
       "53                       6.541944  \n",
       "54                       6.735053  \n",
       "55                       6.955601  \n",
       "56                       6.737537  \n",
       "57                       6.496371  \n",
       "58                       6.283894  \n",
       "59                       5.751389  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_treated_intervened_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_85964/1948038984.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  actual_treated_intervened_A[actual_treated_intervened_A>quantile_90] = \\\n",
      "/tmp/ipykernel_85964/1948038984.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  actual_treated_intervened_A[actual_treated_intervened_A>quantile_90] = \\\n"
     ]
    }
   ],
   "source": [
    "actual_treated_intervened_A[actual_treated_intervened_A>quantile_90] = \\\n",
    "            actual_treated_intervened_A[actual_treated_intervened_A>quantile_90]-\\\n",
    "              np.std(np.ravel(actual_treated_intervened.iloc[:-11, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_114252/1327967944.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  actual_treated_intervened_A[units_selected] = actual_treated_intervened_A[units_selected]-\\\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10_6_60_linear_heterogeneous</th>\n",
       "      <th>10_7_60_linear_heterogeneous</th>\n",
       "      <th>10_8_60_linear_heterogeneous</th>\n",
       "      <th>10_9_60_linear_heterogeneous</th>\n",
       "      <th>10_10_60_linear_heterogeneous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>9.617934</td>\n",
       "      <td>2.880725</td>\n",
       "      <td>12.876451</td>\n",
       "      <td>10.254313</td>\n",
       "      <td>6.345929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>9.426890</td>\n",
       "      <td>2.291242</td>\n",
       "      <td>12.778318</td>\n",
       "      <td>10.363391</td>\n",
       "      <td>6.504824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>9.041199</td>\n",
       "      <td>1.940018</td>\n",
       "      <td>10.569317</td>\n",
       "      <td>10.535353</td>\n",
       "      <td>6.825071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>8.898806</td>\n",
       "      <td>1.746893</td>\n",
       "      <td>12.846226</td>\n",
       "      <td>11.079612</td>\n",
       "      <td>6.582064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>8.518961</td>\n",
       "      <td>1.750235</td>\n",
       "      <td>12.927499</td>\n",
       "      <td>11.118020</td>\n",
       "      <td>6.828245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>8.204192</td>\n",
       "      <td>1.905635</td>\n",
       "      <td>13.354605</td>\n",
       "      <td>11.333678</td>\n",
       "      <td>6.541944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>7.634701</td>\n",
       "      <td>2.479341</td>\n",
       "      <td>12.782394</td>\n",
       "      <td>11.225567</td>\n",
       "      <td>6.735053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>7.409369</td>\n",
       "      <td>2.418841</td>\n",
       "      <td>10.551964</td>\n",
       "      <td>10.941201</td>\n",
       "      <td>6.955601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>7.507990</td>\n",
       "      <td>2.676017</td>\n",
       "      <td>10.542658</td>\n",
       "      <td>11.048612</td>\n",
       "      <td>6.737537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>6.546740</td>\n",
       "      <td>2.850685</td>\n",
       "      <td>10.231485</td>\n",
       "      <td>9.987595</td>\n",
       "      <td>6.496371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>5.857119</td>\n",
       "      <td>3.149199</td>\n",
       "      <td>10.361506</td>\n",
       "      <td>9.345481</td>\n",
       "      <td>6.283894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>5.331190</td>\n",
       "      <td>3.879921</td>\n",
       "      <td>10.449574</td>\n",
       "      <td>8.400910</td>\n",
       "      <td>5.751389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    10_6_60_linear_heterogeneous  10_7_60_linear_heterogeneous  \\\n",
       "48                      9.617934                      2.880725   \n",
       "49                      9.426890                      2.291242   \n",
       "50                      9.041199                      1.940018   \n",
       "51                      8.898806                      1.746893   \n",
       "52                      8.518961                      1.750235   \n",
       "53                      8.204192                      1.905635   \n",
       "54                      7.634701                      2.479341   \n",
       "55                      7.409369                      2.418841   \n",
       "56                      7.507990                      2.676017   \n",
       "57                      6.546740                      2.850685   \n",
       "58                      5.857119                      3.149199   \n",
       "59                      5.331190                      3.879921   \n",
       "\n",
       "    10_8_60_linear_heterogeneous  10_9_60_linear_heterogeneous  \\\n",
       "48                     12.876451                     10.254313   \n",
       "49                     12.778318                     10.363391   \n",
       "50                     10.569317                     10.535353   \n",
       "51                     12.846226                     11.079612   \n",
       "52                     12.927499                     11.118020   \n",
       "53                     13.354605                     11.333678   \n",
       "54                     12.782394                     11.225567   \n",
       "55                     10.551964                     10.941201   \n",
       "56                     10.542658                     11.048612   \n",
       "57                     10.231485                      9.987595   \n",
       "58                     10.361506                      9.345481   \n",
       "59                     10.449574                      8.400910   \n",
       "\n",
       "    10_10_60_linear_heterogeneous  \n",
       "48                       6.345929  \n",
       "49                       6.504824  \n",
       "50                       6.825071  \n",
       "51                       6.582064  \n",
       "52                       6.828245  \n",
       "53                       6.541944  \n",
       "54                       6.735053  \n",
       "55                       6.955601  \n",
       "56                       6.737537  \n",
       "57                       6.496371  \n",
       "58                       6.283894  \n",
       "59                       5.751389  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_treated_intervened_A = actual_treated_intervened.iloc[-12:, :]\n",
    "quantile_90 = actual_treated_intervened_A.T.apply(lambda row: row.quantile([0.9])).T\n",
    "quantile_90_compare = pd.concat([quantile_90] * \\\n",
    "    actual_treated_intervened_A.shape[1], axis=1, ignore_index=True)\n",
    "quantile_90_compare.columns = actual_treated_intervened_A.columns\n",
    "units_selected = actual_treated_intervened_A.columns[(actual_treated_intervened_A >\\\n",
    "                                quantile_90_compare).all()]\n",
    "actual_treated_intervened_A[units_selected] = actual_treated_intervened_A[units_selected]-\\\n",
    "        np.std(np.ravel(actual_treated_intervened.iloc[:-11, :]))\n",
    "actual_treated_orig_A = actual_treated_intervened_A\n",
    "actual_treated_orig_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 10)\n"
     ]
    }
   ],
   "source": [
    "quantiles = ['0.1', '0.5', '0.9']\n",
    "model = 'DeepProbCP'\n",
    "csv_files = {q: './results/nn_model_results/rnn/processed_ensemble_forecasts/'+dataset_name+'_LSTMcell_cocob_without_stl_decomposition_'\\\n",
    "                 +f'{q}.txt' for q in quantiles}  \n",
    "# Create a dictionary to store the dataframes for each quantile\n",
    "dfs = {q: [] for q in quantiles}\n",
    "\n",
    "# Iterate over each quantile\n",
    "for quantile, files in csv_files.items():\n",
    "    # Iterate over each file for the quantile\n",
    "    # Load the text data from CSV\n",
    "    data = pd.read_csv(files, header=None)\n",
    "\n",
    "    # Transpose the data for better visualization (units as rows, time periods as columns)\n",
    "    transposed_data = data.transpose()\n",
    "\n",
    "    # Append the transposed dataframe to the list for the current quantile\n",
    "    dfs[quantile].append(transposed_data)\n",
    "\n",
    "# Concatenate the dataframes along the columns (units) for each quantile\n",
    "concatenated_data = {q: pd.concat(frames, axis=1) for q, frames in dfs.items()}\n",
    "print(concatenated_data['0.1'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.145553</td>\n",
       "      <td>2.893547</td>\n",
       "      <td>12.484855</td>\n",
       "      <td>10.268354</td>\n",
       "      <td>6.131543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.927241</td>\n",
       "      <td>2.912477</td>\n",
       "      <td>12.661434</td>\n",
       "      <td>10.317207</td>\n",
       "      <td>6.068119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.569663</td>\n",
       "      <td>3.100625</td>\n",
       "      <td>12.745415</td>\n",
       "      <td>10.405247</td>\n",
       "      <td>6.069724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.543782</td>\n",
       "      <td>3.166734</td>\n",
       "      <td>12.699887</td>\n",
       "      <td>10.411573</td>\n",
       "      <td>6.076473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.578660</td>\n",
       "      <td>3.270230</td>\n",
       "      <td>12.557126</td>\n",
       "      <td>10.404446</td>\n",
       "      <td>6.129312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9.499311</td>\n",
       "      <td>3.498094</td>\n",
       "      <td>12.416968</td>\n",
       "      <td>10.431839</td>\n",
       "      <td>6.205075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9.502851</td>\n",
       "      <td>3.709254</td>\n",
       "      <td>12.109759</td>\n",
       "      <td>10.419241</td>\n",
       "      <td>6.323229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9.431769</td>\n",
       "      <td>4.047940</td>\n",
       "      <td>11.730433</td>\n",
       "      <td>10.492127</td>\n",
       "      <td>6.499066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.581570</td>\n",
       "      <td>3.906496</td>\n",
       "      <td>11.514966</td>\n",
       "      <td>10.271623</td>\n",
       "      <td>6.534376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.691827</td>\n",
       "      <td>3.758876</td>\n",
       "      <td>11.395530</td>\n",
       "      <td>10.090722</td>\n",
       "      <td>6.540368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.089239</td>\n",
       "      <td>3.559546</td>\n",
       "      <td>11.130563</td>\n",
       "      <td>9.864840</td>\n",
       "      <td>6.608108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10.348900</td>\n",
       "      <td>3.511258</td>\n",
       "      <td>11.015445</td>\n",
       "      <td>9.701056</td>\n",
       "      <td>6.603352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            5         6          7          8         9\n",
       "0   10.145553  2.893547  12.484855  10.268354  6.131543\n",
       "1    9.927241  2.912477  12.661434  10.317207  6.068119\n",
       "2    9.569663  3.100625  12.745415  10.405247  6.069724\n",
       "3    9.543782  3.166734  12.699887  10.411573  6.076473\n",
       "4    9.578660  3.270230  12.557126  10.404446  6.129312\n",
       "5    9.499311  3.498094  12.416968  10.431839  6.205075\n",
       "6    9.502851  3.709254  12.109759  10.419241  6.323229\n",
       "7    9.431769  4.047940  11.730433  10.492127  6.499066\n",
       "8    9.581570  3.906496  11.514966  10.271623  6.534376\n",
       "9    9.691827  3.758876  11.395530  10.090722  6.540368\n",
       "10  10.089239  3.559546  11.130563   9.864840  6.608108\n",
       "11  10.348900  3.511258  11.015445   9.701056  6.603352"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantile_90_pred = concatenated_data['0.9'][concatenated_data['0.9'].columns[tnc_bool]]\n",
    "quantile_90_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10_6_60_linear_heterogeneous</th>\n",
       "      <th>10_7_60_linear_heterogeneous</th>\n",
       "      <th>10_8_60_linear_heterogeneous</th>\n",
       "      <th>10_9_60_linear_heterogeneous</th>\n",
       "      <th>10_10_60_linear_heterogeneous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>9.617934</td>\n",
       "      <td>2.880725</td>\n",
       "      <td>12.876451</td>\n",
       "      <td>10.254313</td>\n",
       "      <td>6.345929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>9.426890</td>\n",
       "      <td>2.291242</td>\n",
       "      <td>12.778318</td>\n",
       "      <td>10.363391</td>\n",
       "      <td>6.504824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>9.041199</td>\n",
       "      <td>1.940018</td>\n",
       "      <td>10.569317</td>\n",
       "      <td>10.535353</td>\n",
       "      <td>6.825071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>8.898806</td>\n",
       "      <td>1.746893</td>\n",
       "      <td>12.846226</td>\n",
       "      <td>11.079612</td>\n",
       "      <td>6.582064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>8.518961</td>\n",
       "      <td>1.750235</td>\n",
       "      <td>12.927499</td>\n",
       "      <td>11.118020</td>\n",
       "      <td>6.828245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>8.204192</td>\n",
       "      <td>1.905635</td>\n",
       "      <td>13.354605</td>\n",
       "      <td>9.110134</td>\n",
       "      <td>6.541944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>7.634701</td>\n",
       "      <td>2.479341</td>\n",
       "      <td>12.782394</td>\n",
       "      <td>9.002023</td>\n",
       "      <td>6.735053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>7.409369</td>\n",
       "      <td>2.418841</td>\n",
       "      <td>10.551964</td>\n",
       "      <td>10.941201</td>\n",
       "      <td>6.955601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>7.507990</td>\n",
       "      <td>2.676017</td>\n",
       "      <td>10.542658</td>\n",
       "      <td>11.048612</td>\n",
       "      <td>6.737537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>6.546740</td>\n",
       "      <td>2.850685</td>\n",
       "      <td>10.231485</td>\n",
       "      <td>9.987595</td>\n",
       "      <td>6.496371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>5.857119</td>\n",
       "      <td>3.149199</td>\n",
       "      <td>10.361506</td>\n",
       "      <td>9.345481</td>\n",
       "      <td>6.283894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>5.331190</td>\n",
       "      <td>3.879921</td>\n",
       "      <td>10.449574</td>\n",
       "      <td>8.400910</td>\n",
       "      <td>5.751389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    10_6_60_linear_heterogeneous  10_7_60_linear_heterogeneous  \\\n",
       "48                      9.617934                      2.880725   \n",
       "49                      9.426890                      2.291242   \n",
       "50                      9.041199                      1.940018   \n",
       "51                      8.898806                      1.746893   \n",
       "52                      8.518961                      1.750235   \n",
       "53                      8.204192                      1.905635   \n",
       "54                      7.634701                      2.479341   \n",
       "55                      7.409369                      2.418841   \n",
       "56                      7.507990                      2.676017   \n",
       "57                      6.546740                      2.850685   \n",
       "58                      5.857119                      3.149199   \n",
       "59                      5.331190                      3.879921   \n",
       "\n",
       "    10_8_60_linear_heterogeneous  10_9_60_linear_heterogeneous  \\\n",
       "48                     12.876451                     10.254313   \n",
       "49                     12.778318                     10.363391   \n",
       "50                     10.569317                     10.535353   \n",
       "51                     12.846226                     11.079612   \n",
       "52                     12.927499                     11.118020   \n",
       "53                     13.354605                      9.110134   \n",
       "54                     12.782394                      9.002023   \n",
       "55                     10.551964                     10.941201   \n",
       "56                     10.542658                     11.048612   \n",
       "57                     10.231485                      9.987595   \n",
       "58                     10.361506                      9.345481   \n",
       "59                     10.449574                      8.400910   \n",
       "\n",
       "    10_10_60_linear_heterogeneous  \n",
       "48                       6.345929  \n",
       "49                       6.504824  \n",
       "50                       6.825071  \n",
       "51                       6.582064  \n",
       "52                       6.828245  \n",
       "53                       6.541944  \n",
       "54                       6.735053  \n",
       "55                       6.955601  \n",
       "56                       6.737537  \n",
       "57                       6.496371  \n",
       "58                       6.283894  \n",
       "59                       5.751389  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_treated_orig_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average predicted treated units\n",
    "averages_predicted_treated = {q: concatenated_data[q][concatenated_data[q].columns[tnc_bool]].mean(axis=1) for q in quantiles}\n",
    "\n",
    "# average actual treated units\n",
    "actual_treated_intervened = data_index[data_index.columns[tnc_bool]]\n",
    "averages_actual_treated = actual_treated_intervened.mean(axis=1)\n",
    "\n",
    "# average predicted control units\n",
    "averages_predicted_control = {q: concatenated_data[q][concatenated_data[q].columns[flipped_tnc_bool]].mean(axis=1) for q in quantiles}\n",
    "\n",
    "# average actual control units\n",
    "averages_actual_control = data_index[data_index.columns[flipped_tnc_bool]].mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "        actual_treated_intervened_A = actual_treated_intervened.iloc[-7:, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_10_pred = np.quantile(np.ravel(smoothed_pred), 0.1)\n",
    "quantile_10_actual = np.quantile(np.ravel(actual_treated_intervened_A), 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (5,) (10,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# print(quantile_10_pred)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(quantile_10_actual)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m ate_10 \u001b[38;5;241m=\u001b[39m \u001b[43mactual_treated_intervened_A\u001b[49m\u001b[43m[\u001b[49m\u001b[43mactual_treated_intervened_A\u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43mquantile_10_actual\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43msmoothed_pred\u001b[49m\u001b[43m[\u001b[49m\u001b[43msmoothed_pred\u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43mquantile_10_pred\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (5,) (10,) "
     ]
    }
   ],
   "source": [
    "# print(quantile_10_pred)\n",
    "# print(quantile_10_actual)\n",
    "\n",
    "ate_10 = actual_treated_intervened_A[actual_treated_intervened_A>quantile_10_actual].mean().values - \\\n",
    "smoothed_pred[smoothed_pred>quantile_10_pred].mean().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10_6_60_linear_heterogeneous</th>\n",
       "      <th>10_7_60_linear_heterogeneous</th>\n",
       "      <th>10_8_60_linear_heterogeneous</th>\n",
       "      <th>10_9_60_linear_heterogeneous</th>\n",
       "      <th>10_10_60_linear_heterogeneous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>8.204192</td>\n",
       "      <td>1.905635</td>\n",
       "      <td>15.578149</td>\n",
       "      <td>11.333678</td>\n",
       "      <td>6.541944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>7.634701</td>\n",
       "      <td>2.479341</td>\n",
       "      <td>15.005937</td>\n",
       "      <td>11.225567</td>\n",
       "      <td>6.735053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>7.409369</td>\n",
       "      <td>2.418841</td>\n",
       "      <td>12.775508</td>\n",
       "      <td>10.941201</td>\n",
       "      <td>6.955601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>7.507990</td>\n",
       "      <td>2.676017</td>\n",
       "      <td>12.766202</td>\n",
       "      <td>11.048612</td>\n",
       "      <td>6.737537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>6.546740</td>\n",
       "      <td>2.850685</td>\n",
       "      <td>12.455028</td>\n",
       "      <td>9.987595</td>\n",
       "      <td>6.496371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>5.857119</td>\n",
       "      <td>3.149199</td>\n",
       "      <td>12.585050</td>\n",
       "      <td>9.345481</td>\n",
       "      <td>6.283894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>5.331190</td>\n",
       "      <td>3.879921</td>\n",
       "      <td>12.673118</td>\n",
       "      <td>8.400910</td>\n",
       "      <td>5.751389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    10_6_60_linear_heterogeneous  10_7_60_linear_heterogeneous  \\\n",
       "53                      8.204192                      1.905635   \n",
       "54                      7.634701                      2.479341   \n",
       "55                      7.409369                      2.418841   \n",
       "56                      7.507990                      2.676017   \n",
       "57                      6.546740                      2.850685   \n",
       "58                      5.857119                      3.149199   \n",
       "59                      5.331190                      3.879921   \n",
       "\n",
       "    10_8_60_linear_heterogeneous  10_9_60_linear_heterogeneous  \\\n",
       "53                     15.578149                     11.333678   \n",
       "54                     15.005937                     11.225567   \n",
       "55                     12.775508                     10.941201   \n",
       "56                     12.766202                     11.048612   \n",
       "57                     12.455028                      9.987595   \n",
       "58                     12.585050                      9.345481   \n",
       "59                     12.673118                      8.400910   \n",
       "\n",
       "    10_10_60_linear_heterogeneous  \n",
       "53                       6.541944  \n",
       "54                       6.735053  \n",
       "55                       6.955601  \n",
       "56                       6.737537  \n",
       "57                       6.496371  \n",
       "58                       6.283894  \n",
       "59                       5.751389  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_treated_intervened_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.92732874,  3.2932682 , 13.40557026, 10.32614906,  6.50025535])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_treated_intervened_A[actual_treated_intervened_A>quantile_10_actual].mean().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_90_orig = np.quantile(np.ravel(actual_treated_orig_A), 0.9)\n",
    "quantile_90_actual = np.quantile(np.ravel(actual_treated_intervened_A), 0.9)\n",
    "quantile_90_pred = np.quantile(np.ravel(smoothed_pred), 0.9)\n",
    "\n",
    "tte_90_true = actual_treated_intervened_A[actual_treated_intervened_A>quantile_90_actual].sum().sum() - \\\n",
    "                    actual_treated_orig_A[actual_treated_orig_A>quantile_90_orig].sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10_6_60_linear_heterogeneous</th>\n",
       "      <th>10_7_60_linear_heterogeneous</th>\n",
       "      <th>10_8_60_linear_heterogeneous</th>\n",
       "      <th>10_9_60_linear_heterogeneous</th>\n",
       "      <th>10_10_60_linear_heterogeneous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>8.204192</td>\n",
       "      <td>1.905635</td>\n",
       "      <td>15.578149</td>\n",
       "      <td>11.333678</td>\n",
       "      <td>6.541944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>7.634701</td>\n",
       "      <td>2.479341</td>\n",
       "      <td>15.005937</td>\n",
       "      <td>11.225567</td>\n",
       "      <td>6.735053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>7.409369</td>\n",
       "      <td>2.418841</td>\n",
       "      <td>12.775508</td>\n",
       "      <td>10.941201</td>\n",
       "      <td>6.955601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>7.507990</td>\n",
       "      <td>2.676017</td>\n",
       "      <td>12.766202</td>\n",
       "      <td>11.048612</td>\n",
       "      <td>6.737537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>6.546740</td>\n",
       "      <td>2.850685</td>\n",
       "      <td>12.455028</td>\n",
       "      <td>9.987595</td>\n",
       "      <td>6.496371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>5.857119</td>\n",
       "      <td>3.149199</td>\n",
       "      <td>12.585050</td>\n",
       "      <td>9.345481</td>\n",
       "      <td>6.283894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>5.331190</td>\n",
       "      <td>3.879921</td>\n",
       "      <td>12.673118</td>\n",
       "      <td>8.400910</td>\n",
       "      <td>5.751389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    10_6_60_linear_heterogeneous  10_7_60_linear_heterogeneous  \\\n",
       "53                      8.204192                      1.905635   \n",
       "54                      7.634701                      2.479341   \n",
       "55                      7.409369                      2.418841   \n",
       "56                      7.507990                      2.676017   \n",
       "57                      6.546740                      2.850685   \n",
       "58                      5.857119                      3.149199   \n",
       "59                      5.331190                      3.879921   \n",
       "\n",
       "    10_8_60_linear_heterogeneous  10_9_60_linear_heterogeneous  \\\n",
       "53                     15.578149                     11.333678   \n",
       "54                     15.005937                     11.225567   \n",
       "55                     12.775508                     10.941201   \n",
       "56                     12.766202                     11.048612   \n",
       "57                     12.455028                      9.987595   \n",
       "58                     12.585050                      9.345481   \n",
       "59                     12.673118                      8.400910   \n",
       "\n",
       "    10_10_60_linear_heterogeneous  \n",
       "53                       6.541944  \n",
       "54                       6.735053  \n",
       "55                       6.955601  \n",
       "56                       6.737537  \n",
       "57                       6.496371  \n",
       "58                       6.283894  \n",
       "59                       5.751389  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_treated_intervened_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10_6_60_linear_heterogeneous</th>\n",
       "      <th>10_7_60_linear_heterogeneous</th>\n",
       "      <th>10_8_60_linear_heterogeneous</th>\n",
       "      <th>10_9_60_linear_heterogeneous</th>\n",
       "      <th>10_10_60_linear_heterogeneous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>9.617934</td>\n",
       "      <td>2.880725</td>\n",
       "      <td>12.876451</td>\n",
       "      <td>10.254313</td>\n",
       "      <td>6.345929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>9.426890</td>\n",
       "      <td>2.291242</td>\n",
       "      <td>12.778318</td>\n",
       "      <td>10.363391</td>\n",
       "      <td>6.504824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>9.041199</td>\n",
       "      <td>1.940018</td>\n",
       "      <td>10.569317</td>\n",
       "      <td>10.535353</td>\n",
       "      <td>6.825071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>8.898806</td>\n",
       "      <td>1.746893</td>\n",
       "      <td>12.846226</td>\n",
       "      <td>11.079612</td>\n",
       "      <td>6.582064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>8.518961</td>\n",
       "      <td>1.750235</td>\n",
       "      <td>12.927499</td>\n",
       "      <td>11.118020</td>\n",
       "      <td>6.828245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>8.204192</td>\n",
       "      <td>1.905635</td>\n",
       "      <td>13.354605</td>\n",
       "      <td>9.110134</td>\n",
       "      <td>6.541944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>7.634701</td>\n",
       "      <td>2.479341</td>\n",
       "      <td>12.782394</td>\n",
       "      <td>9.002023</td>\n",
       "      <td>6.735053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>7.409369</td>\n",
       "      <td>2.418841</td>\n",
       "      <td>10.551964</td>\n",
       "      <td>10.941201</td>\n",
       "      <td>6.955601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>7.507990</td>\n",
       "      <td>2.676017</td>\n",
       "      <td>10.542658</td>\n",
       "      <td>11.048612</td>\n",
       "      <td>6.737537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>6.546740</td>\n",
       "      <td>2.850685</td>\n",
       "      <td>10.231485</td>\n",
       "      <td>9.987595</td>\n",
       "      <td>6.496371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>5.857119</td>\n",
       "      <td>3.149199</td>\n",
       "      <td>10.361506</td>\n",
       "      <td>9.345481</td>\n",
       "      <td>6.283894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>5.331190</td>\n",
       "      <td>3.879921</td>\n",
       "      <td>10.449574</td>\n",
       "      <td>8.400910</td>\n",
       "      <td>5.751389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    10_6_60_linear_heterogeneous  10_7_60_linear_heterogeneous  \\\n",
       "48                      9.617934                      2.880725   \n",
       "49                      9.426890                      2.291242   \n",
       "50                      9.041199                      1.940018   \n",
       "51                      8.898806                      1.746893   \n",
       "52                      8.518961                      1.750235   \n",
       "53                      8.204192                      1.905635   \n",
       "54                      7.634701                      2.479341   \n",
       "55                      7.409369                      2.418841   \n",
       "56                      7.507990                      2.676017   \n",
       "57                      6.546740                      2.850685   \n",
       "58                      5.857119                      3.149199   \n",
       "59                      5.331190                      3.879921   \n",
       "\n",
       "    10_8_60_linear_heterogeneous  10_9_60_linear_heterogeneous  \\\n",
       "48                     12.876451                     10.254313   \n",
       "49                     12.778318                     10.363391   \n",
       "50                     10.569317                     10.535353   \n",
       "51                     12.846226                     11.079612   \n",
       "52                     12.927499                     11.118020   \n",
       "53                     13.354605                      9.110134   \n",
       "54                     12.782394                      9.002023   \n",
       "55                     10.551964                     10.941201   \n",
       "56                     10.542658                     11.048612   \n",
       "57                     10.231485                      9.987595   \n",
       "58                     10.361506                      9.345481   \n",
       "59                     10.449574                      8.400910   \n",
       "\n",
       "    10_10_60_linear_heterogeneous  \n",
       "48                       6.345929  \n",
       "49                       6.504824  \n",
       "50                       6.825071  \n",
       "51                       6.582064  \n",
       "52                       6.828245  \n",
       "53                       6.541944  \n",
       "54                       6.735053  \n",
       "55                       6.955601  \n",
       "56                       6.737537  \n",
       "57                       6.496371  \n",
       "58                       6.283894  \n",
       "59                       5.751389  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_treated_orig_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.284049843767162"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantile_90_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.7289679883944"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantile_90_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.797173618212657"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantile_90_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_90_pred.index = actual_treated_intervened_A.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-37.564442412332056"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.sum(actual_treated_intervened_A-quantile_90_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-37.564442412332056"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.sum(actual_treated_orig_A-quantile_90_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.56549312715026"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantile_90_true = np.quantile(np.ravel(actual_treated_orig_A), 0.9)\n",
    "\n",
    "actual_treated_orig_A[actual_treated_orig_A>quantile_90_true].sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0.1', '0.5', '0.9'], dtype='<U3')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "np.stack(concatenated_data[0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 6.50622966,  6.26681151,  1.69794442,  4.00662164,\n",
       "          3.25119214,  9.86300406,  3.04319363, 12.68108072,\n",
       "         10.55509685,  6.13268461],\n",
       "        [ 6.62972448,  6.31053431,  1.68897599,  4.13591489,\n",
       "          2.734496  ,  9.94294389,  3.03045079, 12.57681042,\n",
       "         10.44289558,  6.13234696],\n",
       "        [ 6.99112475,  6.30115847,  1.59557893,  4.51417566,\n",
       "          2.20232482, 10.14555327,  2.89354677, 12.48485539,\n",
       "         10.26835366,  6.1315426 ]],\n",
       "\n",
       "       [[ 6.37955061,  6.20668416,  1.87532299,  4.01795517,\n",
       "          3.32110113,  9.44603546,  3.26761269, 12.79331093,\n",
       "         10.6268138 ,  6.11537149],\n",
       "        [ 6.70880607,  6.20051612,  1.69182724,  4.37613917,\n",
       "          2.94636863,  9.7102263 ,  3.03980114, 12.84082984,\n",
       "         10.54059671,  6.08067698],\n",
       "        [ 7.37531658,  6.25487046,  1.57292733,  5.03912264,\n",
       "          1.95414219,  9.92724134,  2.91247724, 12.66143394,\n",
       "         10.31720712,  6.06811914]],\n",
       "\n",
       "       [[ 6.33845164,  6.07668907,  2.09172253,  4.19124164,\n",
       "          3.4369066 ,  8.95815763,  3.58122369, 13.02656808,\n",
       "         10.86179487,  6.05567429],\n",
       "        [ 6.57392655,  6.15670742,  1.81657858,  4.42311659,\n",
       "          3.11913398,  9.40337519,  3.2003724 , 12.97670602,\n",
       "         10.59278968,  6.03159708],\n",
       "        [ 7.3935274 ,  6.17634948,  1.66324354,  5.12501906,\n",
       "          2.13889678,  9.5696629 ,  3.10062539, 12.74541457,\n",
       "         10.40524675,  6.06972419]],\n",
       "\n",
       "       [[ 6.09791041,  5.93017304,  2.38571371,  3.84447373,\n",
       "          3.88304491,  8.70306426,  4.07837237, 12.95397927,\n",
       "         11.1435623 ,  6.16144126],\n",
       "        [ 6.60680486,  6.08332599,  1.90122051,  4.48194349,\n",
       "          3.08081657,  9.21675366,  3.35550548, 13.12971492,\n",
       "         10.79148535,  6.02540692],\n",
       "        [ 7.44508507,  6.1700669 ,  1.69362278,  5.14831663,\n",
       "          2.02285032,  9.54378217,  3.1667337 , 12.69988719,\n",
       "         10.41157329,  6.07647292]],\n",
       "\n",
       "       [[ 5.93420937,  5.92384528,  2.7029084 ,  3.63296747,\n",
       "          3.77011005,  8.3608955 ,  4.52171989, 12.67740159,\n",
       "         11.05598753,  6.27507788],\n",
       "        [ 6.4693395 ,  6.09374731,  2.24235099,  4.15802463,\n",
       "          2.59567466,  8.96463351,  3.88733065, 12.59684613,\n",
       "         10.69610073,  6.19653536],\n",
       "        [ 7.66469601,  6.18799191,  1.74041405,  5.29799116,\n",
       "          1.60385029,  9.57865965,  3.27022954, 12.55712559,\n",
       "         10.40444638,  6.12931188]],\n",
       "\n",
       "       [[ 5.68148863,  5.81080675,  3.06228569,  3.19817842,\n",
       "          3.85912237,  8.3008062 ,  5.17120892, 12.39427293,\n",
       "         11.14299447,  6.47734708],\n",
       "        [ 6.34042203,  6.1033116 ,  2.41269757,  3.853407  ,\n",
       "          2.47229354,  8.85662081,  4.24027258, 12.31483152,\n",
       "         10.62823   ,  6.32455296],\n",
       "        [ 7.39128078,  6.16993896,  1.8607532 ,  4.88990973,\n",
       "          1.72749238,  9.49931132,  3.49809376, 12.41696839,\n",
       "         10.43183891,  6.20507506]],\n",
       "\n",
       "       [[ 5.54840543,  5.81804516,  3.18876446,  2.96748119,\n",
       "          4.35728123,  8.32869202,  5.39685416, 12.12456507,\n",
       "         11.12352471,  6.57755063],\n",
       "        [ 6.22175637,  6.00508935,  2.5424229 ,  3.64665954,\n",
       "          2.78001567,  8.87281991,  4.41050574, 12.06586106,\n",
       "         10.65405649,  6.43940221],\n",
       "        [ 7.41987405,  6.20098341,  1.95026089,  4.6699506 ,\n",
       "          1.54784139,  9.50285088,  3.70925417, 12.10975935,\n",
       "         10.4192411 ,  6.32322892]],\n",
       "\n",
       "       [[ 5.45111408,  5.82884191,  3.42346745,  2.79477513,\n",
       "          4.32917574,  8.24085139,  5.73166419, 11.78343163,\n",
       "         11.09268358,  6.67817077],\n",
       "        [ 6.12186814,  6.0351128 ,  2.65490451,  3.32838815,\n",
       "          3.05913291,  8.87049838,  4.70554055, 11.85923716,\n",
       "         10.76235472,  6.53318585],\n",
       "        [ 7.19828327,  6.16384315,  2.11584446,  4.12611895,\n",
       "          1.7655921 ,  9.43176941,  4.04794028, 11.73043274,\n",
       "         10.49212676,  6.49906565]],\n",
       "\n",
       "       [[ 5.02237193,  5.9974536 ,  3.61435425,  2.50226684,\n",
       "          4.33768661,  8.37002793,  5.55900173, 11.33002054,\n",
       "         10.6219286 ,  6.74700876],\n",
       "        [ 5.72609003,  6.11043132,  2.76843832,  2.98317385,\n",
       "          3.28342742,  8.98410299,  4.62132827, 11.46178629,\n",
       "         10.43630869,  6.63205086],\n",
       "        [ 6.97487923,  6.23705414,  2.1116425 ,  3.93745836,\n",
       "          1.79950182,  9.58156958,  3.90649641, 11.51496612,\n",
       "         10.27162275,  6.53437639]],\n",
       "\n",
       "       [[ 4.59331662,  6.06230697,  4.26485138,  2.32650402,\n",
       "          3.28673236,  8.38270031,  5.8522575 , 11.03474517,\n",
       "         10.41571179,  6.79707119],\n",
       "        [ 5.62629147,  6.2001615 ,  2.76885067,  3.02431411,\n",
       "          2.75007527,  9.16451479,  4.35721858, 11.18977084,\n",
       "         10.14388178,  6.61672201],\n",
       "        [ 6.67722266,  6.25086492,  2.13897778,  3.792397  ,\n",
       "          1.88835132,  9.6918271 ,  3.75887618, 11.39553031,\n",
       "         10.09072178,  6.54036816]],\n",
       "\n",
       "       [[ 4.40705309,  6.19280961,  4.48958812,  2.20695387,\n",
       "          2.63425788,  8.48573691,  5.91548872, 10.83784792,\n",
       "         10.22794295,  6.80871421],\n",
       "        [ 5.35670801,  6.30306201,  2.82859556,  2.79311892,\n",
       "          1.84208691,  9.74150201,  4.17118354, 10.92546684,\n",
       "          9.85284238,  6.75750561],\n",
       "        [ 6.53220199,  6.36955495,  2.09574087,  3.6213964 ,\n",
       "          1.60943355, 10.08923873,  3.55954625, 11.13056324,\n",
       "          9.86484033,  6.60810768]],\n",
       "\n",
       "       [[ 4.47859424,  6.37869191,  4.57253758,  2.11448314,\n",
       "          2.02800423,  8.5809864 ,  6.20737124, 10.7797336 ,\n",
       "         10.17578914,  6.90297756],\n",
       "        [ 5.41504689,  6.60672336,  2.97298717,  2.86733721,\n",
       "          1.2398178 ,  9.68479377,  4.40338454, 10.848368  ,\n",
       "          9.69539654,  6.67455387],\n",
       "        [ 6.59145697,  6.46785806,  2.08744727,  3.70174581,\n",
       "          1.22649069, 10.34890015,  3.51125814, 11.01544484,\n",
       "          9.70105615,  6.6033521 ]]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "\n",
    "cs = CubicSpline(list(concatenated_data.keys()),\\\n",
    "    [concatenated_data['0.1'], concatenated_data['0.5'],\\\n",
    "     concatenated_data['0.9']], bc_type='natural')\n",
    "smoothed_pred = np.transpose(cs(list(concatenated_data.keys())), (1, 0, 2))\n",
    "smoothed_pred = smoothed_pred[:,1,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 10)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crps_y_pred[:,1,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_treated_orig_A.columns[(actual_treated_orig_A >\\\n",
    "                                quantile_90_actual_compare).all()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_10_actual = np.quantile(np.ravel(actual_treated_intervened.iloc[:, -7:-1]), 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.380561110006781"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantile_10_actual = np.quantile(np.ravel(actual_treated_intervened_A.iloc[:, -7:-1]), 0.1)\n",
    "quantile_10_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10_6_60_linear_heterogeneous      7.832924\n",
       "10_7_60_linear_heterogeneous      2.904961\n",
       "10_8_60_linear_heterogeneous     11.689333\n",
       "10_9_60_linear_heterogeneous     10.098887\n",
       "10_10_60_linear_heterogeneous     6.548993\n",
       "dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_treated_intervened_A[actual_treated_intervened_A>quantile_10_actual].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0.1': 0     8.455012\n",
       " 1     8.449829\n",
       " 2     8.496684\n",
       " 3     8.608084\n",
       " 4     8.578216\n",
       " 5     8.697326\n",
       " 6     8.710237\n",
       " 7     8.705360\n",
       " 8     8.525598\n",
       " 9     8.496497\n",
       " 10    8.455146\n",
       " 11    8.529372\n",
       " dtype: float64,\n",
       " '0.5': 0     8.425090\n",
       " 1     8.442426\n",
       " 2     8.440968\n",
       " 3     8.503773\n",
       " 4     8.468289\n",
       " 5     8.472902\n",
       " 6     8.488529\n",
       " 7     8.546163\n",
       " 8     8.427115\n",
       " 9     8.294422\n",
       " 10    8.289700\n",
       " 11    8.261299\n",
       " dtype: float64,\n",
       " '0.9': 0     8.384770\n",
       " 1     8.377296\n",
       " 2     8.378135\n",
       " 3     8.379690\n",
       " 4     8.387955\n",
       " 5     8.410257\n",
       " 6     8.412867\n",
       " 7     8.440267\n",
       " 8     8.361806\n",
       " 9     8.295465\n",
       " 10    8.250459\n",
       " 11    8.236002\n",
       " dtype: float64}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averages_predicted_treated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = CubicSpline(list(concatenated_data.keys()),\\\n",
    "    [concatenated_data['0.1'], concatenated_data['0.5'],\\\n",
    "    concatenated_data['0.9']], bc_type='natural')\n",
    "smoothed_pred = np.transpose(cs(list(concatenated_data.keys())), (1, 0, 2))\n",
    "smoothed_pred = pd.DataFrame(smoothed_pred[:,1,:])\n",
    "quantile_10_pred = np.quantile(np.ravel(smoothed_pred.iloc[:, -7:-1]), 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8005407465750833"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantile_10_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'quantile_90_actual' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m actual_treated_intervened_A[actual_treated_intervened_A\u001b[38;5;241m>\u001b[39m\u001b[43mquantile_90_actual\u001b[49m]\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;241m-\u001b[39m \\\n\u001b[1;32m      2\u001b[0m                          smoothed_pred[smoothed_pred\u001b[38;5;241m>\u001b[39mquantile_90_pred]\u001b[38;5;241m.\u001b[39mmean()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'quantile_90_actual' is not defined"
     ]
    }
   ],
   "source": [
    "actual_treated_intervened_A[actual_treated_intervened_A>quantile_90_actual].mean() - \\\n",
    "                         smoothed_pred[smoothed_pred>quantile_90_pred].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10_6_60_linear_heterogeneous</th>\n",
       "      <th>10_7_60_linear_heterogeneous</th>\n",
       "      <th>10_8_60_linear_heterogeneous</th>\n",
       "      <th>10_9_60_linear_heterogeneous</th>\n",
       "      <th>10_10_60_linear_heterogeneous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.274956</td>\n",
       "      <td>3.990011</td>\n",
       "      <td>5.259868</td>\n",
       "      <td>4.552482</td>\n",
       "      <td>3.853568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.927431</td>\n",
       "      <td>4.110910</td>\n",
       "      <td>6.583133</td>\n",
       "      <td>4.870682</td>\n",
       "      <td>4.233268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.431322</td>\n",
       "      <td>4.712161</td>\n",
       "      <td>7.222388</td>\n",
       "      <td>6.139203</td>\n",
       "      <td>4.354999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.344974</td>\n",
       "      <td>4.871238</td>\n",
       "      <td>7.776269</td>\n",
       "      <td>7.102527</td>\n",
       "      <td>4.388512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.314408</td>\n",
       "      <td>5.454914</td>\n",
       "      <td>8.587760</td>\n",
       "      <td>8.939321</td>\n",
       "      <td>5.057418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.935989</td>\n",
       "      <td>6.421490</td>\n",
       "      <td>8.457479</td>\n",
       "      <td>10.418614</td>\n",
       "      <td>5.187652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.225889</td>\n",
       "      <td>6.795303</td>\n",
       "      <td>8.074153</td>\n",
       "      <td>11.148028</td>\n",
       "      <td>4.928620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.136426</td>\n",
       "      <td>6.207478</td>\n",
       "      <td>8.020679</td>\n",
       "      <td>11.754114</td>\n",
       "      <td>5.018153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.465214</td>\n",
       "      <td>5.692347</td>\n",
       "      <td>7.699433</td>\n",
       "      <td>11.253545</td>\n",
       "      <td>5.618299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.732422</td>\n",
       "      <td>5.320020</td>\n",
       "      <td>8.117523</td>\n",
       "      <td>11.331420</td>\n",
       "      <td>5.947955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.887341</td>\n",
       "      <td>5.568383</td>\n",
       "      <td>8.219767</td>\n",
       "      <td>10.588586</td>\n",
       "      <td>6.815346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6.446839</td>\n",
       "      <td>5.950711</td>\n",
       "      <td>7.933082</td>\n",
       "      <td>10.649765</td>\n",
       "      <td>7.674323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.928076</td>\n",
       "      <td>6.189151</td>\n",
       "      <td>7.969893</td>\n",
       "      <td>9.904755</td>\n",
       "      <td>7.926467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.473912</td>\n",
       "      <td>5.984565</td>\n",
       "      <td>8.265003</td>\n",
       "      <td>9.410221</td>\n",
       "      <td>7.956808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.073103</td>\n",
       "      <td>5.627949</td>\n",
       "      <td>8.542899</td>\n",
       "      <td>9.318537</td>\n",
       "      <td>7.941301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.051531</td>\n",
       "      <td>5.736050</td>\n",
       "      <td>8.195939</td>\n",
       "      <td>8.869008</td>\n",
       "      <td>7.237445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.171579</td>\n",
       "      <td>6.230178</td>\n",
       "      <td>7.911306</td>\n",
       "      <td>8.574907</td>\n",
       "      <td>6.796242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.354026</td>\n",
       "      <td>6.498678</td>\n",
       "      <td>7.170023</td>\n",
       "      <td>8.104262</td>\n",
       "      <td>7.034299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.660604</td>\n",
       "      <td>7.095529</td>\n",
       "      <td>6.862693</td>\n",
       "      <td>7.742397</td>\n",
       "      <td>6.327118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.808407</td>\n",
       "      <td>7.467494</td>\n",
       "      <td>6.929713</td>\n",
       "      <td>7.257976</td>\n",
       "      <td>6.246345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6.172036</td>\n",
       "      <td>6.934992</td>\n",
       "      <td>7.157941</td>\n",
       "      <td>7.698037</td>\n",
       "      <td>6.341199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6.462719</td>\n",
       "      <td>6.369795</td>\n",
       "      <td>6.739595</td>\n",
       "      <td>8.077326</td>\n",
       "      <td>6.605948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6.554098</td>\n",
       "      <td>6.330093</td>\n",
       "      <td>7.123334</td>\n",
       "      <td>7.766870</td>\n",
       "      <td>6.913442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>6.112608</td>\n",
       "      <td>6.033792</td>\n",
       "      <td>7.827843</td>\n",
       "      <td>7.706623</td>\n",
       "      <td>7.112230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6.279516</td>\n",
       "      <td>6.208322</td>\n",
       "      <td>9.133151</td>\n",
       "      <td>7.826821</td>\n",
       "      <td>7.449716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>6.142227</td>\n",
       "      <td>6.831532</td>\n",
       "      <td>10.238272</td>\n",
       "      <td>8.600854</td>\n",
       "      <td>6.695191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6.639708</td>\n",
       "      <td>6.586040</td>\n",
       "      <td>11.432136</td>\n",
       "      <td>9.674974</td>\n",
       "      <td>6.452523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>7.524218</td>\n",
       "      <td>6.639229</td>\n",
       "      <td>11.755966</td>\n",
       "      <td>10.574302</td>\n",
       "      <td>6.541707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7.919083</td>\n",
       "      <td>6.537553</td>\n",
       "      <td>11.702333</td>\n",
       "      <td>11.128135</td>\n",
       "      <td>6.433359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8.232136</td>\n",
       "      <td>6.843351</td>\n",
       "      <td>11.880349</td>\n",
       "      <td>11.786150</td>\n",
       "      <td>6.498769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>8.378958</td>\n",
       "      <td>7.067178</td>\n",
       "      <td>11.765948</td>\n",
       "      <td>12.234508</td>\n",
       "      <td>6.454334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>8.078761</td>\n",
       "      <td>7.084629</td>\n",
       "      <td>11.770562</td>\n",
       "      <td>12.226881</td>\n",
       "      <td>6.952593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>7.983304</td>\n",
       "      <td>7.181661</td>\n",
       "      <td>11.534213</td>\n",
       "      <td>11.613853</td>\n",
       "      <td>6.907234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>8.244075</td>\n",
       "      <td>7.265398</td>\n",
       "      <td>11.153619</td>\n",
       "      <td>10.448785</td>\n",
       "      <td>7.172886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>8.826078</td>\n",
       "      <td>6.829245</td>\n",
       "      <td>10.333148</td>\n",
       "      <td>10.234268</td>\n",
       "      <td>7.346571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>8.557360</td>\n",
       "      <td>6.587077</td>\n",
       "      <td>10.029988</td>\n",
       "      <td>9.857021</td>\n",
       "      <td>7.069538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>8.737783</td>\n",
       "      <td>6.152348</td>\n",
       "      <td>9.760604</td>\n",
       "      <td>9.291463</td>\n",
       "      <td>7.175936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>8.678591</td>\n",
       "      <td>5.982678</td>\n",
       "      <td>9.297328</td>\n",
       "      <td>8.984903</td>\n",
       "      <td>6.898193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>8.716804</td>\n",
       "      <td>5.790201</td>\n",
       "      <td>9.760329</td>\n",
       "      <td>8.896401</td>\n",
       "      <td>7.066208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>9.017681</td>\n",
       "      <td>6.103956</td>\n",
       "      <td>9.939821</td>\n",
       "      <td>9.125483</td>\n",
       "      <td>6.920042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>8.956676</td>\n",
       "      <td>6.568936</td>\n",
       "      <td>10.532298</td>\n",
       "      <td>10.214215</td>\n",
       "      <td>6.698136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>8.436440</td>\n",
       "      <td>6.067351</td>\n",
       "      <td>11.095576</td>\n",
       "      <td>10.904494</td>\n",
       "      <td>7.053589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>8.168121</td>\n",
       "      <td>5.715336</td>\n",
       "      <td>10.941753</td>\n",
       "      <td>10.817946</td>\n",
       "      <td>7.045750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>8.801920</td>\n",
       "      <td>5.164559</td>\n",
       "      <td>11.136625</td>\n",
       "      <td>10.829086</td>\n",
       "      <td>7.179429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>9.594553</td>\n",
       "      <td>4.391121</td>\n",
       "      <td>11.057943</td>\n",
       "      <td>10.843918</td>\n",
       "      <td>6.821644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>10.126045</td>\n",
       "      <td>3.957237</td>\n",
       "      <td>12.054023</td>\n",
       "      <td>10.505266</td>\n",
       "      <td>6.380702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>9.990315</td>\n",
       "      <td>3.043457</td>\n",
       "      <td>12.360816</td>\n",
       "      <td>10.435537</td>\n",
       "      <td>6.305574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>9.617934</td>\n",
       "      <td>2.880725</td>\n",
       "      <td>12.876451</td>\n",
       "      <td>10.254313</td>\n",
       "      <td>6.345929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>9.426890</td>\n",
       "      <td>2.291242</td>\n",
       "      <td>12.778318</td>\n",
       "      <td>10.363391</td>\n",
       "      <td>6.504824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>9.041199</td>\n",
       "      <td>1.940018</td>\n",
       "      <td>10.569317</td>\n",
       "      <td>10.535353</td>\n",
       "      <td>6.825071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>8.898806</td>\n",
       "      <td>1.746893</td>\n",
       "      <td>12.846226</td>\n",
       "      <td>11.079612</td>\n",
       "      <td>6.582064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>8.518961</td>\n",
       "      <td>1.750235</td>\n",
       "      <td>12.927499</td>\n",
       "      <td>11.118020</td>\n",
       "      <td>6.828245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>8.204192</td>\n",
       "      <td>1.905635</td>\n",
       "      <td>13.354605</td>\n",
       "      <td>9.110134</td>\n",
       "      <td>6.541944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>7.634701</td>\n",
       "      <td>2.479341</td>\n",
       "      <td>12.782394</td>\n",
       "      <td>9.002023</td>\n",
       "      <td>6.735053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>7.409369</td>\n",
       "      <td>2.418841</td>\n",
       "      <td>10.551964</td>\n",
       "      <td>10.941201</td>\n",
       "      <td>6.955601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>7.507990</td>\n",
       "      <td>2.676017</td>\n",
       "      <td>10.542658</td>\n",
       "      <td>11.048612</td>\n",
       "      <td>6.737537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>6.546740</td>\n",
       "      <td>2.850685</td>\n",
       "      <td>10.231485</td>\n",
       "      <td>9.987595</td>\n",
       "      <td>6.496371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>5.857119</td>\n",
       "      <td>3.149199</td>\n",
       "      <td>10.361506</td>\n",
       "      <td>9.345481</td>\n",
       "      <td>6.283894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>5.331190</td>\n",
       "      <td>3.879921</td>\n",
       "      <td>10.449574</td>\n",
       "      <td>8.400910</td>\n",
       "      <td>5.751389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    10_6_60_linear_heterogeneous  10_7_60_linear_heterogeneous  \\\n",
       "0                       4.000000                      4.000000   \n",
       "1                       3.274956                      3.990011   \n",
       "2                       2.927431                      4.110910   \n",
       "3                       3.431322                      4.712161   \n",
       "4                       4.344974                      4.871238   \n",
       "5                       4.314408                      5.454914   \n",
       "6                       4.935989                      6.421490   \n",
       "7                       5.225889                      6.795303   \n",
       "8                       5.136426                      6.207478   \n",
       "9                       5.465214                      5.692347   \n",
       "10                      5.732422                      5.320020   \n",
       "11                      5.887341                      5.568383   \n",
       "12                      6.446839                      5.950711   \n",
       "13                      5.928076                      6.189151   \n",
       "14                      5.473912                      5.984565   \n",
       "15                      5.073103                      5.627949   \n",
       "16                      5.051531                      5.736050   \n",
       "17                      5.171579                      6.230178   \n",
       "18                      5.354026                      6.498678   \n",
       "19                      5.660604                      7.095529   \n",
       "20                      5.808407                      7.467494   \n",
       "21                      6.172036                      6.934992   \n",
       "22                      6.462719                      6.369795   \n",
       "23                      6.554098                      6.330093   \n",
       "24                      6.112608                      6.033792   \n",
       "25                      6.279516                      6.208322   \n",
       "26                      6.142227                      6.831532   \n",
       "27                      6.639708                      6.586040   \n",
       "28                      7.524218                      6.639229   \n",
       "29                      7.919083                      6.537553   \n",
       "30                      8.232136                      6.843351   \n",
       "31                      8.378958                      7.067178   \n",
       "32                      8.078761                      7.084629   \n",
       "33                      7.983304                      7.181661   \n",
       "34                      8.244075                      7.265398   \n",
       "35                      8.826078                      6.829245   \n",
       "36                      8.557360                      6.587077   \n",
       "37                      8.737783                      6.152348   \n",
       "38                      8.678591                      5.982678   \n",
       "39                      8.716804                      5.790201   \n",
       "40                      9.017681                      6.103956   \n",
       "41                      8.956676                      6.568936   \n",
       "42                      8.436440                      6.067351   \n",
       "43                      8.168121                      5.715336   \n",
       "44                      8.801920                      5.164559   \n",
       "45                      9.594553                      4.391121   \n",
       "46                     10.126045                      3.957237   \n",
       "47                      9.990315                      3.043457   \n",
       "48                      9.617934                      2.880725   \n",
       "49                      9.426890                      2.291242   \n",
       "50                      9.041199                      1.940018   \n",
       "51                      8.898806                      1.746893   \n",
       "52                      8.518961                      1.750235   \n",
       "53                      8.204192                      1.905635   \n",
       "54                      7.634701                      2.479341   \n",
       "55                      7.409369                      2.418841   \n",
       "56                      7.507990                      2.676017   \n",
       "57                      6.546740                      2.850685   \n",
       "58                      5.857119                      3.149199   \n",
       "59                      5.331190                      3.879921   \n",
       "\n",
       "    10_8_60_linear_heterogeneous  10_9_60_linear_heterogeneous  \\\n",
       "0                       4.000000                      4.000000   \n",
       "1                       5.259868                      4.552482   \n",
       "2                       6.583133                      4.870682   \n",
       "3                       7.222388                      6.139203   \n",
       "4                       7.776269                      7.102527   \n",
       "5                       8.587760                      8.939321   \n",
       "6                       8.457479                     10.418614   \n",
       "7                       8.074153                     11.148028   \n",
       "8                       8.020679                     11.754114   \n",
       "9                       7.699433                     11.253545   \n",
       "10                      8.117523                     11.331420   \n",
       "11                      8.219767                     10.588586   \n",
       "12                      7.933082                     10.649765   \n",
       "13                      7.969893                      9.904755   \n",
       "14                      8.265003                      9.410221   \n",
       "15                      8.542899                      9.318537   \n",
       "16                      8.195939                      8.869008   \n",
       "17                      7.911306                      8.574907   \n",
       "18                      7.170023                      8.104262   \n",
       "19                      6.862693                      7.742397   \n",
       "20                      6.929713                      7.257976   \n",
       "21                      7.157941                      7.698037   \n",
       "22                      6.739595                      8.077326   \n",
       "23                      7.123334                      7.766870   \n",
       "24                      7.827843                      7.706623   \n",
       "25                      9.133151                      7.826821   \n",
       "26                     10.238272                      8.600854   \n",
       "27                     11.432136                      9.674974   \n",
       "28                     11.755966                     10.574302   \n",
       "29                     11.702333                     11.128135   \n",
       "30                     11.880349                     11.786150   \n",
       "31                     11.765948                     12.234508   \n",
       "32                     11.770562                     12.226881   \n",
       "33                     11.534213                     11.613853   \n",
       "34                     11.153619                     10.448785   \n",
       "35                     10.333148                     10.234268   \n",
       "36                     10.029988                      9.857021   \n",
       "37                      9.760604                      9.291463   \n",
       "38                      9.297328                      8.984903   \n",
       "39                      9.760329                      8.896401   \n",
       "40                      9.939821                      9.125483   \n",
       "41                     10.532298                     10.214215   \n",
       "42                     11.095576                     10.904494   \n",
       "43                     10.941753                     10.817946   \n",
       "44                     11.136625                     10.829086   \n",
       "45                     11.057943                     10.843918   \n",
       "46                     12.054023                     10.505266   \n",
       "47                     12.360816                     10.435537   \n",
       "48                     12.876451                     10.254313   \n",
       "49                     12.778318                     10.363391   \n",
       "50                     10.569317                     10.535353   \n",
       "51                     12.846226                     11.079612   \n",
       "52                     12.927499                     11.118020   \n",
       "53                     13.354605                      9.110134   \n",
       "54                     12.782394                      9.002023   \n",
       "55                     10.551964                     10.941201   \n",
       "56                     10.542658                     11.048612   \n",
       "57                     10.231485                      9.987595   \n",
       "58                     10.361506                      9.345481   \n",
       "59                     10.449574                      8.400910   \n",
       "\n",
       "    10_10_60_linear_heterogeneous  \n",
       "0                        4.000000  \n",
       "1                        3.853568  \n",
       "2                        4.233268  \n",
       "3                        4.354999  \n",
       "4                        4.388512  \n",
       "5                        5.057418  \n",
       "6                        5.187652  \n",
       "7                        4.928620  \n",
       "8                        5.018153  \n",
       "9                        5.618299  \n",
       "10                       5.947955  \n",
       "11                       6.815346  \n",
       "12                       7.674323  \n",
       "13                       7.926467  \n",
       "14                       7.956808  \n",
       "15                       7.941301  \n",
       "16                       7.237445  \n",
       "17                       6.796242  \n",
       "18                       7.034299  \n",
       "19                       6.327118  \n",
       "20                       6.246345  \n",
       "21                       6.341199  \n",
       "22                       6.605948  \n",
       "23                       6.913442  \n",
       "24                       7.112230  \n",
       "25                       7.449716  \n",
       "26                       6.695191  \n",
       "27                       6.452523  \n",
       "28                       6.541707  \n",
       "29                       6.433359  \n",
       "30                       6.498769  \n",
       "31                       6.454334  \n",
       "32                       6.952593  \n",
       "33                       6.907234  \n",
       "34                       7.172886  \n",
       "35                       7.346571  \n",
       "36                       7.069538  \n",
       "37                       7.175936  \n",
       "38                       6.898193  \n",
       "39                       7.066208  \n",
       "40                       6.920042  \n",
       "41                       6.698136  \n",
       "42                       7.053589  \n",
       "43                       7.045750  \n",
       "44                       7.179429  \n",
       "45                       6.821644  \n",
       "46                       6.380702  \n",
       "47                       6.305574  \n",
       "48                       6.345929  \n",
       "49                       6.504824  \n",
       "50                       6.825071  \n",
       "51                       6.582064  \n",
       "52                       6.828245  \n",
       "53                       6.541944  \n",
       "54                       6.735053  \n",
       "55                       6.955601  \n",
       "56                       6.737537  \n",
       "57                       6.496371  \n",
       "58                       6.283894  \n",
       "59                       5.751389  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_treated_intervened"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "smoothed_pred[smoothed_pred>quantile_10_pred].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'single_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m smoothed_pred \u001b[38;5;241m=\u001b[39m \u001b[43msingle_data\u001b[49m[single_data\u001b[38;5;241m.\u001b[39mcolumns[tnc_bool]]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'single_data' is not defined"
     ]
    }
   ],
   "source": [
    "smoothed_pred = single_data[single_data.columns[tnc_bool]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_pred = concatenated_data['0.5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.629724</td>\n",
       "      <td>6.310534</td>\n",
       "      <td>1.688976</td>\n",
       "      <td>4.135915</td>\n",
       "      <td>2.734496</td>\n",
       "      <td>9.942944</td>\n",
       "      <td>3.030451</td>\n",
       "      <td>12.576810</td>\n",
       "      <td>10.442896</td>\n",
       "      <td>6.132347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.708806</td>\n",
       "      <td>6.200516</td>\n",
       "      <td>1.691827</td>\n",
       "      <td>4.376139</td>\n",
       "      <td>2.946369</td>\n",
       "      <td>9.710226</td>\n",
       "      <td>3.039801</td>\n",
       "      <td>12.840830</td>\n",
       "      <td>10.540597</td>\n",
       "      <td>6.080677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.573927</td>\n",
       "      <td>6.156707</td>\n",
       "      <td>1.816579</td>\n",
       "      <td>4.423117</td>\n",
       "      <td>3.119134</td>\n",
       "      <td>9.403375</td>\n",
       "      <td>3.200372</td>\n",
       "      <td>12.976706</td>\n",
       "      <td>10.592790</td>\n",
       "      <td>6.031597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.606805</td>\n",
       "      <td>6.083326</td>\n",
       "      <td>1.901221</td>\n",
       "      <td>4.481943</td>\n",
       "      <td>3.080817</td>\n",
       "      <td>9.216754</td>\n",
       "      <td>3.355505</td>\n",
       "      <td>13.129715</td>\n",
       "      <td>10.791485</td>\n",
       "      <td>6.025407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.469340</td>\n",
       "      <td>6.093747</td>\n",
       "      <td>2.242351</td>\n",
       "      <td>4.158025</td>\n",
       "      <td>2.595675</td>\n",
       "      <td>8.964634</td>\n",
       "      <td>3.887331</td>\n",
       "      <td>12.596846</td>\n",
       "      <td>10.696101</td>\n",
       "      <td>6.196535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.340422</td>\n",
       "      <td>6.103312</td>\n",
       "      <td>2.412698</td>\n",
       "      <td>3.853407</td>\n",
       "      <td>2.472294</td>\n",
       "      <td>8.856621</td>\n",
       "      <td>4.240273</td>\n",
       "      <td>12.314832</td>\n",
       "      <td>10.628230</td>\n",
       "      <td>6.324553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.221756</td>\n",
       "      <td>6.005089</td>\n",
       "      <td>2.542423</td>\n",
       "      <td>3.646660</td>\n",
       "      <td>2.780016</td>\n",
       "      <td>8.872820</td>\n",
       "      <td>4.410506</td>\n",
       "      <td>12.065861</td>\n",
       "      <td>10.654056</td>\n",
       "      <td>6.439402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6.121868</td>\n",
       "      <td>6.035113</td>\n",
       "      <td>2.654905</td>\n",
       "      <td>3.328388</td>\n",
       "      <td>3.059133</td>\n",
       "      <td>8.870498</td>\n",
       "      <td>4.705541</td>\n",
       "      <td>11.859237</td>\n",
       "      <td>10.762355</td>\n",
       "      <td>6.533186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.726090</td>\n",
       "      <td>6.110431</td>\n",
       "      <td>2.768438</td>\n",
       "      <td>2.983174</td>\n",
       "      <td>3.283427</td>\n",
       "      <td>8.984103</td>\n",
       "      <td>4.621328</td>\n",
       "      <td>11.461786</td>\n",
       "      <td>10.436309</td>\n",
       "      <td>6.632051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.626291</td>\n",
       "      <td>6.200161</td>\n",
       "      <td>2.768851</td>\n",
       "      <td>3.024314</td>\n",
       "      <td>2.750075</td>\n",
       "      <td>9.164515</td>\n",
       "      <td>4.357219</td>\n",
       "      <td>11.189771</td>\n",
       "      <td>10.143882</td>\n",
       "      <td>6.616722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.356708</td>\n",
       "      <td>6.303062</td>\n",
       "      <td>2.828596</td>\n",
       "      <td>2.793119</td>\n",
       "      <td>1.842087</td>\n",
       "      <td>9.741502</td>\n",
       "      <td>4.171184</td>\n",
       "      <td>10.925467</td>\n",
       "      <td>9.852842</td>\n",
       "      <td>6.757506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.415047</td>\n",
       "      <td>6.606723</td>\n",
       "      <td>2.972987</td>\n",
       "      <td>2.867337</td>\n",
       "      <td>1.239818</td>\n",
       "      <td>9.684794</td>\n",
       "      <td>4.403385</td>\n",
       "      <td>10.848368</td>\n",
       "      <td>9.695397</td>\n",
       "      <td>6.674554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "0   6.629724  6.310534  1.688976  4.135915  2.734496  9.942944  3.030451   \n",
       "1   6.708806  6.200516  1.691827  4.376139  2.946369  9.710226  3.039801   \n",
       "2   6.573927  6.156707  1.816579  4.423117  3.119134  9.403375  3.200372   \n",
       "3   6.606805  6.083326  1.901221  4.481943  3.080817  9.216754  3.355505   \n",
       "4   6.469340  6.093747  2.242351  4.158025  2.595675  8.964634  3.887331   \n",
       "5   6.340422  6.103312  2.412698  3.853407  2.472294  8.856621  4.240273   \n",
       "6   6.221756  6.005089  2.542423  3.646660  2.780016  8.872820  4.410506   \n",
       "7   6.121868  6.035113  2.654905  3.328388  3.059133  8.870498  4.705541   \n",
       "8   5.726090  6.110431  2.768438  2.983174  3.283427  8.984103  4.621328   \n",
       "9   5.626291  6.200161  2.768851  3.024314  2.750075  9.164515  4.357219   \n",
       "10  5.356708  6.303062  2.828596  2.793119  1.842087  9.741502  4.171184   \n",
       "11  5.415047  6.606723  2.972987  2.867337  1.239818  9.684794  4.403385   \n",
       "\n",
       "            7          8         9  \n",
       "0   12.576810  10.442896  6.132347  \n",
       "1   12.840830  10.540597  6.080677  \n",
       "2   12.976706  10.592790  6.031597  \n",
       "3   13.129715  10.791485  6.025407  \n",
       "4   12.596846  10.696101  6.196535  \n",
       "5   12.314832  10.628230  6.324553  \n",
       "6   12.065861  10.654056  6.439402  \n",
       "7   11.859237  10.762355  6.533186  \n",
       "8   11.461786  10.436309  6.632051  \n",
       "9   11.189771  10.143882  6.616722  \n",
       "10  10.925467   9.852842  6.757506  \n",
       "11  10.848368   9.695397  6.674554  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smoothed_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        actual_treated_intervened = data_index[data_index.columns[tnc_bool]]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
