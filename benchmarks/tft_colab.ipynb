{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==2.0.1 pytorch-lightning==2.0.2 pytorch_forecasting==1.0.0 torchaudio==2.0.2 torchdata==0.6.1 torchtext==0.15.2 torchvision==0.15.2 optuna==3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import CubicSpline\n",
    "# imports for training\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "# import dataset, network to train and metric to optimize\n",
    "from pytorch_forecasting import Baseline, TimeSeriesDataSet, TemporalFusionTransformer, QuantileLoss\n",
    "from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "import pandas as pd\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "def mean_weighted_quantile_loss(\n",
    "    y_pred: np.ndarray, y_true: np.ndarray, quantiles: list\n",
    ") -> float:\n",
    "    y_true_rep = y_true[:, None].repeat(len(quantiles), axis=1)\n",
    "    quantiles = np.array([float(q) for q in quantiles])\n",
    "    # print(quantiles_repeated.shape, y_pred.shape, y_true.shape)\n",
    "    quantile_losses = 2 * np.sum(\n",
    "        np.abs(\n",
    "            (y_pred - y_true_rep)\n",
    "            * ((y_true_rep <= y_pred) - quantiles[:, None])\n",
    "        ),\n",
    "        axis=-1,\n",
    "    )  # shape [num_time_series, num_quantiles]\n",
    "    denom = np.sum(np.abs(y_true_rep))  # shape [1]\n",
    "    weighted_losses = quantile_losses.sum(0) / denom  # shape [num_quantiles]\n",
    "    return weighted_losses\n",
    "\n",
    "def custom_sort_key(s):\n",
    "    parts = s.split('_')\n",
    "    return int(parts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function\n",
    "\n",
    "def tft_eval(dataset_name,dataset_type,forecast_horizon, batch_size):\n",
    "  pl.seed_everything(42)\n",
    "\n",
    "  data = pd.read_csv(dataset_name+'.csv')\n",
    "  data[\"time_idx\"] = data.index.to_list()\n",
    "\n",
    "  if dataset_type == \"sim\":\n",
    "    d_t = 'time'\n",
    "    data = data.rename(columns={data.columns[0]: 'time'})\n",
    "    data_melted = pd.melt(data, id_vars=[d_t,'time_idx'])\n",
    "    print(len(data_melted.index))\n",
    "    print(len(data.index))\n",
    "    print(len(data.columns))\n",
    "    print(int((len(data.columns)-2)/2))\n",
    "    print(int((len(data.columns)-2)-int((len(data.columns)-2)/2)))\n",
    "    data_melted['tnc'] = ['0'] * int(len(data.index)) * int((len(data.columns)-2)/2) +\\\n",
    "     ['1'] * int(len(data.index)) * int((len(data.columns)-2)-int((len(data.columns)-2)/2))\n",
    "  if dataset_type == \"calls911\":\n",
    "    d_t = 'date'\n",
    "    data_melted = pd.melt(data, id_vars=[d_t,'time_idx'])\n",
    "    treated = [\"ABINGTON\",  \"AMBLER\",  \"CHELTENHAM\",  \"COLLEGEVILLE\",  \"CONSHOHOCKEN\",\n",
    "                  \"EAST GREENVILLE\",  \"EAST NORRITON\",  \"FRANCONIA\" , \"GREEN LANE\", \"HATFIELD TOWNSHIP\",\n",
    "                  \"HORSHAM\" , \"JENKINTOWN\",  \"LANSDALE\",  \"LIMERICK\",  \"LOWER GWYNEDD\",\n",
    "                  \"LOWER MERION\",  \"LOWER MORELAND\",  \"LOWER POTTSGROVE\",  \"LOWER PROVIDENCE\",  \"LOWER SALFORD\",\n",
    "                  \"MARLBOROUGH\",  \"MONTGOMERY\",  \"NARBERTH\",  \"PENNSBURG\",  \"PERKIOMEN\",\n",
    "                  \"PLYMOUTH\",  \"POTTSTOWN\",  \"RED HILL\",  \"ROCKLEDGE\",  \"ROYERSFORD\",\n",
    "                  \"SCHWENKSVILLE\",  \"SKIPPACK\",  \"SOUDERTON\",  \"TELFORD\",  \"TOWAMENCIN\",\n",
    "                  \"UPPER DUBLIN\",  \"UPPER FREDERICK\",  \"UPPER GWYNEDD\",  \"UPPER HANOVER\",  \"UPPER MERION\",\n",
    "                  \"UPPER MORELAND\",  \"UPPER POTTSGROVE\",  \"UPPER PROVIDENCE\",  \"UPPER SALFORD\",  \"WEST CONSHOHOCKEN\",\n",
    "                  \"WEST NORRITON\",  \"WEST POTTSGROVE\",  \"WHITEMARSH\",  \"WHITPAIN\",  \"WORCESTER\"]\n",
    "    control = [\"BRIDGEPORT\", \"BRYN ATHYN\", \"DOUGLASS\", \"HATBORO\", \"HATFIELD BORO\",\n",
    "                      \"LOWER FREDERICK\", \"NEW HANOVER\", \"NORRISTOWN\", \"NORTH WALES\", \"SALFORD\",\n",
    "                      \"SPRINGFIELD\", \"TRAPPE\"]\n",
    "    # Create a binary mask indicating whether each column is treated (1) or control (0)\n",
    "    data_melted['tnc'] = ['1' if col in treated else '0' for col in data_melted.variable]\n",
    "\n",
    "  # define the dataset, i.e. add metadata to pandas dataframe for the model to understand it\n",
    "  max_encoder_length = 15\n",
    "  max_prediction_length = forecast_horizon\n",
    "  # training_cutoff = \"2020-01-01\"  # day for cutoff\n",
    "  training_cutoff = data_melted[\"time_idx\"].max() - max_prediction_length*2\n",
    "\n",
    "\n",
    "  training = TimeSeriesDataSet(\n",
    "      # data[lambda x: x.date <= training_cutoff],\n",
    "      data_melted[lambda x: x.time_idx <= training_cutoff],\n",
    "      group_ids=[\"tnc\",\"variable\"],  # column name(s) for timeseries IDs\n",
    "      target= \"value\",  # column name of target to predict\n",
    "      time_idx= \"time_idx\",  # column name of time of observation\n",
    "      max_encoder_length=max_encoder_length,  # how much history to use\n",
    "      max_prediction_length=max_prediction_length,  # how far to predict into future\n",
    "      static_categoricals=[\"tnc\",\"variable\"],\n",
    "      time_varying_known_reals=[\"time_idx\"],\n",
    "      time_varying_unknown_reals=[\"value\"],\n",
    "      add_relative_time_idx=True,\n",
    "      add_target_scales=True,\n",
    "      add_encoder_length=True,\n",
    "      target_normalizer=GroupNormalizer(\n",
    "          groups=[\"tnc\",\"variable\"], transformation=\"relu\"),\n",
    "  )\n",
    "\n",
    "  # create validation dataset using the same normalization techniques as for the training dataset\n",
    "  validation = TimeSeriesDataSet.from_dataset(training, data_melted[lambda x: x.time_idx <=\\\n",
    "              data_melted[\"time_idx\"].max() - max_prediction_length], stop_randomization=True)\n",
    "  # convert datasets to dataloaders for training\n",
    "  train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=2)\n",
    "  val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=2)\n",
    "\n",
    "\n",
    "  # Early Stopping\n",
    "  MIN_DELTA  = 1e-4\n",
    "  PATIENCE = 10\n",
    "\n",
    "  #PL Trainer\n",
    "  MAX_EPOCHS = 15\n",
    "  # GPUS = 1\n",
    "  LIMIT_TRAIN_BATCHES = batch_size\n",
    "\n",
    "  # OUTPUT_SIZE=3\n",
    "  REDUCE_ON_PLATEAU_PATIENCE=5\n",
    "\n",
    "  if dataset_name == 'sim_101_60_nl_he':\n",
    "    best_params = {'gradient_clip_val': 0.4225449658914148,\n",
    "                   'hidden_size': 38,\n",
    "                   'dropout': 0.14244586345854338,\n",
    "                   'hidden_continuous_size': 35,\n",
    "                   'attention_head_size': 3,\n",
    "                   'learning_rate': 0.032221938240178535}\n",
    "  elif dataset_name == 'sim_101_222_nl_ho':\n",
    "    best_params =  {'gradient_clip_val': 0.11974316071920273,\n",
    "                   'hidden_size': 45,\n",
    "                   'dropout': 0.15032399710839242,\n",
    "                   'hidden_continuous_size': 32,\n",
    "                   'attention_head_size': 4,\n",
    "                   'learning_rate': 0.01918316565499633}    \n",
    "  elif dataset_name == 'sim_500_222_l_he':\n",
    "    best_params = {'gradient_clip_val': 0.749550431700117,\n",
    "                   'hidden_size': 55,\n",
    "                   'dropout': 0.3499898055165465,\n",
    "                   'hidden_continuous_size': 52,\n",
    "                   'attention_head_size': 4,\n",
    "                   'learning_rate': 0.03637234644558992}\n",
    "  else:\n",
    "    # Load the study from the saved file\n",
    "    with open(\"pytorch_lightning_optuna_\"+dataset_name+\".pkl\", \"rb\") as fin:\n",
    "        study = pickle.load(fin)\n",
    "    # show best hyperparameters\n",
    "    print(study.best_trial.params)\n",
    "    best_params = study.best_trial.params\n",
    "\n",
    "  #PL Trainer\n",
    "  GRADIENT_CLIP_VAL=best_params['gradient_clip_val']\n",
    "  #Fusion Transformer\n",
    "  LR = best_params['learning_rate']\n",
    "  HIDDEN_SIZE = best_params['hidden_size']\n",
    "  DROPOUT = best_params['dropout']\n",
    "  ATTENTION_HEAD_SIZE = best_params['attention_head_size']\n",
    "  HIDDEN_CONTINUOUS_SIZE = best_params['hidden_continuous_size']\n",
    "\n",
    "\n",
    "  early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=MIN_DELTA, patience=PATIENCE, verbose=False, mode=\"min\")\n",
    "  lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "  logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "  trainer = pl.Trainer(\n",
    "      max_epochs=MAX_EPOCHS,\n",
    "      accelerator=\"cpu\",\n",
    "      enable_model_summary=True,\n",
    "      gradient_clip_val=GRADIENT_CLIP_VAL,\n",
    "      limit_train_batches=LIMIT_TRAIN_BATCHES,  # coment in for training, running valiation every 30 batches\n",
    "      # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "      callbacks=[lr_logger, early_stop_callback],\n",
    "      logger=logger,\n",
    "      # log_every_n_steps=10\n",
    "  )\n",
    "\n",
    "  tft = TemporalFusionTransformer.from_dataset(\n",
    "      training,\n",
    "      learning_rate=LR,\n",
    "      hidden_size=HIDDEN_SIZE,\n",
    "      attention_head_size=ATTENTION_HEAD_SIZE,\n",
    "      dropout=DROPOUT,\n",
    "      hidden_continuous_size=HIDDEN_CONTINUOUS_SIZE,\n",
    "      # output_size=OUTPUT_SIZE,\n",
    "      loss=QuantileLoss(),\n",
    "      # log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "      optimizer=\"Ranger\",\n",
    "      reduce_on_plateau_patience=REDUCE_ON_PLATEAU_PATIENCE,\n",
    "  )\n",
    "  print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "\n",
    "  # fit network\n",
    "  trainer.fit(\n",
    "      tft,\n",
    "      train_dataloaders=train_dataloader,\n",
    "      val_dataloaders=val_dataloader,\n",
    "  )\n",
    "\n",
    "  # load the best model according to the validation loss\n",
    "  # (given that we use early stopping, this is not necessarily the last epoch)\n",
    "  best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "  print(best_model_path)\n",
    "  best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "\n",
    "  # predict the treated and control\n",
    "  predicted_result_1 = {}\n",
    "  predicted_result_5 = {}\n",
    "  predicted_result_9 = {}\n",
    "  predicted_result = {}\n",
    "  data = data.drop(\"time_idx\", axis=1)\n",
    "\n",
    "  for i in data.columns[1:]:\n",
    "      raw_prediction_treated = best_tft.predict(\n",
    "          data_melted[lambda x: (training_cutoff - max_encoder_length <\\\n",
    "                          x.time_idx)][data_melted.variable==i],\n",
    "          mode=\"quantiles\"\n",
    "          # return_x=True,\n",
    "          # return_y=True,\n",
    "      )\n",
    "      predicted_result_1[i]=raw_prediction_treated[0][:,1].tolist()\n",
    "      predicted_result_5[i]=raw_prediction_treated[0][:,3].tolist()\n",
    "      predicted_result_9[i]=raw_prediction_treated[0][:,5].tolist()\n",
    "      predicted_result[i] = raw_prediction_treated[0].mean(axis=1).tolist()\n",
    "  # best_tft.plot_prediction(raw_prediction_treated.index, raw_prediction_treated.output, idx=0)\n",
    "\n",
    "  y_pred_1 = pd.DataFrame(predicted_result_1).T\n",
    "  y_pred_1.to_csv(\"predicted/\"+dataset_name+\"_tft_0.1.csv\", header=False, index=False)\n",
    "  y_pred_5 = pd.DataFrame(predicted_result_5).T\n",
    "  y_pred_5.to_csv(\"predicted/\"+dataset_name+\"_tft_0.5.csv\", header=False, index=False)\n",
    "  y_pred_9 = pd.DataFrame(predicted_result_9).T\n",
    "  y_pred_9.to_csv(\"predicted/\"+dataset_name+\"_tft_0.9.csv\", header=False, index=False)\n",
    "  y_pred_mean = pd.DataFrame(predicted_result).T\n",
    "  y_pred_mean.to_csv(\"predicted/\"+dataset_name+\"_tft_mean.csv\", header=False, index=False)\n",
    "\n",
    "  # # doesn't need to train again, just read in\n",
    "  # y_pred_1 =pd.read_csv(\"predicted/\"+dataset_name+\"_tft_0.1.csv\", header=None)\n",
    "  # y_pred_5 =pd.read_csv(\"predicted/\"+dataset_name+\"_tft_0.5.csv\", header=None)\n",
    "  # y_pred_9 =pd.read_csv(\"predicted/\"+dataset_name+\"_tft_0.9.csv\", header=None)\n",
    "\n",
    "  # predicted treated units compared to actual counterfactual; predicted control units compared to actual control units\n",
    "  length_of_series = len(data.index)\n",
    "  no_of_series = len(data.columns[1:-1])\n",
    "  if dataset_type == \"calls911\":\n",
    "    y_pred_1['names'] = data.columns[1:-1]\n",
    "    y_pred_1.set_index('names', inplace=True)\n",
    "    y_pred_1_for_errors = y_pred_1.loc[control,:]\n",
    "    y_pred_5['names'] = data.columns[1:-1]\n",
    "    y_pred_5.set_index('names', inplace=True)\n",
    "    y_pred_5_for_errors = y_pred_5.loc[control,:]\n",
    "    y_pred_9['names'] = data.columns[1:-1]\n",
    "    y_pred_9.set_index('names', inplace=True)\n",
    "    y_pred_9_for_errors = y_pred_9.loc[control,:]\n",
    "\n",
    "    data_row = data.loc[:,control]\n",
    "    no_of_series_errors = len(control)\n",
    "  if dataset_type == \"sim\":\n",
    "    y_pred_1_for_errors = y_pred_1\n",
    "    y_pred_5_for_errors = y_pred_5\n",
    "    y_pred_9_for_errors = y_pred_9\n",
    "    data_row = data.iloc[:,1:-1]\n",
    "    no_of_series_errors = no_of_series\n",
    "    # The columns of data_row need to be rename\n",
    "    # Define the pattern to replace\n",
    "    linear_to_l = r'_linear'\n",
    "    nonlinear_to_nl = r'_nonlinear'\n",
    "    heterogeneous_to_he = r'_heterogeneous'\n",
    "    homogeneous_to_ho = r'_homogeneous'\n",
    "\n",
    "    # Use regular expression to replace the part in column names\n",
    "    data_row.columns = data_row.columns.str.replace(linear_to_l, '_l')\n",
    "    data_row.columns = data_row.columns.str.replace(nonlinear_to_nl, '_nl')\n",
    "    data_row.columns = data_row.columns.str.replace(heterogeneous_to_he, '_he')\n",
    "    data_row.columns = data_row.columns.str.replace(homogeneous_to_ho, '_ho')\n",
    "\n",
    "    data_true_counterfactual = pd.read_csv(dataset_name+'_true_counterfactual.csv')\n",
    "    data_true_counterfactual['time'] = data_true_counterfactual['time']+length_of_series-forecast_horizon-1\n",
    "    data_true_counterfactual = data_true_counterfactual.pivot(index='time', columns='series_id')['value']\n",
    "    data_true_counterfactual = data_true_counterfactual.loc[:,sorted(data_true_counterfactual.columns, key=custom_sort_key)]\n",
    "    # Replace values in data_row using the mapping\n",
    "    data_row.loc[data_true_counterfactual.index, data_true_counterfactual.columns] = data_true_counterfactual\n",
    "\n",
    "  data_row_A = data_row.iloc[length_of_series-forecast_horizon:, :].T\n",
    "  data_row_B = data_row.iloc[:length_of_series-forecast_horizon, :].T\n",
    "\n",
    "  errors_directory = 'errors/'\n",
    "\n",
    "  errors_file_name_mean_median = 'mean_median_' + dataset_name + '_tft'\n",
    "  SMAPE_file_name_all_errors = 'all_smape_errors_' + dataset_name + '_tft'\n",
    "  MASE_file_name_all_errors = 'all_mase_errors_' + dataset_name + '_tft'\n",
    "\n",
    "  errors_file_full_name_mean_median = errors_directory + errors_file_name_mean_median+'.txt'\n",
    "  SMAPE_file_full_name_all_errors = errors_directory + SMAPE_file_name_all_errors\n",
    "  MASE_file_full_name_all_errors = errors_directory + MASE_file_name_all_errors\n",
    "\n",
    "  # SMAPE\n",
    "  time_series_wise_SMAPE = 2 * np.abs(y_pred_5_for_errors - np.array(data_row_A)) /\\\n",
    "      (np.abs(y_pred_5_for_errors) + np.abs(np.array(data_row_A)))\n",
    "  SMAPEPerSeries = np.mean(time_series_wise_SMAPE, axis=1)\n",
    "  mean_SMAPE = np.mean(SMAPEPerSeries)\n",
    "  mean_SMAPE_str = f\"mean_SMAPE:{mean_SMAPE}\"\n",
    "  print(mean_SMAPE_str)\n",
    "  np.savetxt(SMAPE_file_full_name_all_errors+'.txt', SMAPEPerSeries, delimiter=\",\", fmt='%f')\n",
    "\n",
    "  mase_vector = []\n",
    "  for i in range(no_of_series_errors):\n",
    "      lagged_diff = [data_row_B.iloc[i,j] - \\\n",
    "                  data_row_B.iloc[i,j - forecast_horizon]\\\n",
    "                    for j in range(forecast_horizon,\\\n",
    "                      len(data_row_B.columns))]\n",
    "      mase_vector.append(np.mean(np.abs(np.array(np.array(data_row_A.iloc[i]))\\\n",
    "                - np.array(y_pred_5_for_errors.iloc[i])) / np.mean(np.abs(lagged_diff))))\n",
    "\n",
    "  mean_MASE = np.mean(mase_vector)\n",
    "  mean_MASE_str = f\"mean_MASE:{mean_MASE}\"\n",
    "  print(mean_MASE_str)\n",
    "\n",
    "  np.savetxt(MASE_file_full_name_all_errors+'.txt', mase_vector, delimiter=\",\", fmt='%f')\n",
    "\n",
    "  # Writing the SMAPE results to file\n",
    "  with open(errors_file_full_name_mean_median, 'w') as f:\n",
    "      # f.write('\\n'.join([mean_SMAPE_str, median_SMAPE_str, std_SMAPE_str]))\n",
    "      f.write('\\n'.join([mean_SMAPE_str]))\n",
    "\n",
    "  # Writing the MASE results to file\n",
    "  with open(errors_file_full_name_mean_median, 'a') as f:\n",
    "      # f.write('\\n'.join([mean_MASE_str, median_MASE_str, std_MASE_str]))\n",
    "      f.write('\\n'.join([mean_MASE_str]))\n",
    "\n",
    "  # CRPS\n",
    "  quantiles = ['0.1', '0.5', '0.9']\n",
    "  cs = CubicSpline(quantiles, [y_pred_1_for_errors,y_pred_5_for_errors,y_pred_9_for_errors], bc_type='natural')\n",
    "  crps_y_pred = np.transpose(cs(quantiles), (1, 0, 2))\n",
    "\n",
    "  # Calculating the CRPS\n",
    "  crps_qs = mean_weighted_quantile_loss(crps_y_pred, np.array(data_row_A), quantiles)\n",
    "\n",
    "  mean_CRPS = np.mean(crps_qs)\n",
    "\n",
    "  mean_CRPS_str = f\"mean_CRPS:{mean_CRPS}\"\n",
    "  all_CRPS_qs = f\"CRPS for different quantiles:{crps_qs}\"\n",
    "  # std_CRPS_str = f\"std_CRPS:{std_CRPS}\"\n",
    "\n",
    "  print(mean_CRPS_str)\n",
    "  print(all_CRPS_qs)\n",
    "\n",
    "  CRPS_file_name_cs = dataset_name + '_tft'+ '_cs'\n",
    "  CRPS_file_cs = 'predicted/' + CRPS_file_name_cs\n",
    "\n",
    "\n",
    "  # Writing the CRPS results to file\n",
    "  with open(errors_file_full_name_mean_median, 'a') as f:\n",
    "      # f.write('\\n'.join([mean_CRPS_str, median_CRPS_str, std_CRPS_str]))\n",
    "      f.write('\\n'.join([mean_CRPS_str]))\n",
    "  with open(CRPS_file_cs+'.pickle', 'wb') as f:\n",
    "      pickle.dump(crps_y_pred, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### before loop\n",
    "dataset_name = 'calls911_benchmarks'\n",
    "dataset_type = 'calls911'\n",
    "forecast_horizon=7\n",
    "tft_eval(dataset_name,dataset_type,forecast_horizon, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'sim_10_60_l_he', 'sim_10_60_l_ho',\\\n",
    "#                      'sim_10_60_nl_he', 'sim_10_60_nl_ho',\\\n",
    "#                      'sim_10_222_l_he', 'sim_10_222_l_ho',\\\n",
    "#                      'sim_10_222_nl_he', 'sim_10_222_nl_ho',\\\n",
    "#                      'sim_101_60_l_he', 'sim_101_60_l_ho',\\\n",
    "#                      'sim_101_60_nl_he', 'sim_101_60_nl_ho',\\\n",
    "dataset_name_test = ['sim_101_222_l_he', 'sim_101_222_l_ho',\\\n",
    "                     'sim_101_222_nl_he', 'sim_101_222_nl_ho',\\\n",
    "                     'sim_500_60_l_he', 'sim_500_60_l_ho',\\\n",
    "                     'sim_500_60_nl_he', 'sim_500_60_nl_ho',\\\n",
    "                     'sim_500_222_l_he', 'sim_500_222_l_ho',\\\n",
    "                     'sim_500_222_nl_he', 'sim_500_222_nl_ho'] # \n",
    "# [31]*12 + \n",
    "batch_size_test = [124]*12\n",
    "dataset_type = 'sim'\n",
    "forecast_horizon=12\n",
    "for i in range(len(dataset_name_test)):\n",
    "  print(dataset_name_test[i], batch_size_test[i])\n",
    "  tft_eval(dataset_name_test[i],dataset_type,forecast_horizon, batch_size_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(42)\n",
    "# early_stop = EarlyStopping(monitor=\"val_acc\", mode=\"max\")\n",
    "# checkpoint = ModelCheckpoint(monitor=\"val_loss\")\n",
    "# # early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "# # lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "# logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "# trainer = pl.Trainer(\n",
    "#     # max_epochs=1000,\n",
    "#     accelerator=\"cpu\",\n",
    "#     enable_model_summary=True,\n",
    "#     # gradient_clip_val=0.1,\n",
    "#     limit_train_batches=batch_size,  # coment in for training, running valiation every 30 batches\n",
    "#     # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "#     callbacks=[early_stop, checkpoint],\n",
    "#     logger=logger,\n",
    "# )\n",
    "\n",
    "# create study\n",
    "study1 = optimize_hyperparameters(\n",
    "    train_dataloader1,\n",
    "    val_dataloader1,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=15,\n",
    "    max_epochs=30,\n",
    "    gradient_clip_val_range=(0.01, 0.5),\n",
    "    hidden_size_range=(12, 32),\n",
    "    hidden_continuous_size_range=(12, 32),\n",
    "    attention_head_size_range=(1, 3),\n",
    "    learning_rate_range=(0.01, 0.5),\n",
    "    dropout_range=(0.1, 0.3),\n",
    "    trainer_kwargs=dict(limit_train_batches=batch_size),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    ")\n",
    "\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"pytorch_lightning_optuna_sim_10_60_l_he.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study1, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study1.best_trial.params)\n",
    "\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"/drive/MyDrive/pytorch_lightning_optuna_sim_10_60_l_he.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study1, fout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('sim_10_60_l_he_train.csv')\n",
    "data2 = data2.sort_values(by=['series_id','time'], ignore_index=True)\n",
    "data2[\"time_idx\"] = data2.index.to_list()\n",
    "data2 = data2[['time', 'time_idx', 'series_id', 'value', 'c_t']]\n",
    "data2 = data2.rename(columns={'series_id': 'variable'})\n",
    "data2['tnc'] = ['1' if i == 'treated' else '0' for i in data2['c_t']]\n",
    "data2_processed = data2[['time', 'time_idx', 'variable', 'value','tnc']]\n",
    "# define the dataset, i.e. add metadata to pandas dataframe for the model to understand it\n",
    "max_encoder_length = 15\n",
    "max_prediction_length = 12\n",
    "\n",
    "training2 = TimeSeriesDataSet(\n",
    "    # data[lambda x: x.date <= training_cutoff],\n",
    "    data2_processed[lambda x: x.time_idx <= data2_processed[\"time_idx\"].max() - max_prediction_length*2],\n",
    "    group_ids=[\"tnc\",\"variable\"],  # column name(s) for timeseries IDs\n",
    "    target= \"value\",  # column name of target to predict\n",
    "    time_idx= \"time_idx\",  # column name of time of observation\n",
    "    max_encoder_length=max_encoder_length,  # how much history to use\n",
    "    max_prediction_length=max_prediction_length,  # how far to predict into future\n",
    "    static_categoricals=[\"tnc\",\"variable\"],\n",
    "    time_varying_unknown_reals=[\"value\"],\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"tnc\",\"variable\"], transformation=\"relu\"),\n",
    ")\n",
    "\n",
    "\n",
    "# create validation dataset using the same normalization techniques as for the training dataset\n",
    "validation2 = TimeSeriesDataSet.from_dataset(training2, data2_processed[lambda x: x.time_idx <= \\\n",
    "              data2_processed[\"time_idx\"].max() - max_prediction_length], stop_randomization=True)\n",
    "\n",
    "# convert datasets to dataloaders for training\n",
    "batch_size = 31\n",
    "train_dataloader2 = training2.to_dataloader(train=True, batch_size=batch_size, num_workers=2)\n",
    "val_dataloader2 = validation2.to_dataloader(train=False, batch_size=batch_size, num_workers=2)\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# create study\n",
    "study1 = optimize_hyperparameters(\n",
    "    train_dataloader2,\n",
    "    val_dataloader2,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=15,\n",
    "    max_epochs=30,\n",
    "    gradient_clip_val_range=(0.01, 0.5),\n",
    "    hidden_size_range=(12, 32),\n",
    "    hidden_continuous_size_range=(12, 32),\n",
    "    attention_head_size_range=(1, 3),\n",
    "    learning_rate_range=(0.01, 0.5),\n",
    "    dropout_range=(0.1, 0.3),\n",
    "    trainer_kwargs=dict(limit_train_batches=batch_size),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    ")\n",
    "\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"pytorch_lightning_optuna_sim_10_60_l_he.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study1, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study1.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('sim_10_60_l_ho_train.csv')\n",
    "data2 = data2.sort_values(by=['series_id','time'], ignore_index=True)\n",
    "data2[\"time_idx\"] = data2.index.to_list()\n",
    "data2 = data2[['time', 'time_idx', 'series_id', 'value', 'c_t']]\n",
    "data2 = data2.rename(columns={'series_id': 'variable'})\n",
    "data2['tnc'] = ['1' if i == 'treated' else '0' for i in data2['c_t']]\n",
    "data2_processed = data2[['time', 'time_idx', 'variable', 'value','tnc']]\n",
    "# define the dataset, i.e. add metadata to pandas dataframe for the model to understand it\n",
    "max_encoder_length = 15\n",
    "max_prediction_length = 12\n",
    "\n",
    "training2 = TimeSeriesDataSet(\n",
    "    # data[lambda x: x.date <= training_cutoff],\n",
    "    data2_processed[lambda x: x.time_idx <= data2_processed[\"time_idx\"].max() - max_prediction_length*2],\n",
    "    group_ids=[\"tnc\",\"variable\"],  # column name(s) for timeseries IDs\n",
    "    target= \"value\",  # column name of target to predict\n",
    "    time_idx= \"time_idx\",  # column name of time of observation\n",
    "    max_encoder_length=max_encoder_length,  # how much history to use\n",
    "    max_prediction_length=max_prediction_length,  # how far to predict into future\n",
    "    static_categoricals=[\"tnc\",\"variable\"],\n",
    "    time_varying_unknown_reals=[\"value\"],\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"tnc\",\"variable\"], transformation=\"relu\"),\n",
    ")\n",
    "\n",
    "\n",
    "# create validation dataset using the same normalization techniques as for the training dataset\n",
    "validation2 = TimeSeriesDataSet.from_dataset(training2, data2_processed[lambda x: x.time_idx <= \\\n",
    "              data2_processed[\"time_idx\"].max() - max_prediction_length], stop_randomization=True)\n",
    "\n",
    "# convert datasets to dataloaders for training\n",
    "batch_size = 31\n",
    "train_dataloader2 = training2.to_dataloader(train=True, batch_size=batch_size, num_workers=2)\n",
    "val_dataloader2 = validation2.to_dataloader(train=False, batch_size=batch_size, num_workers=2)\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# create study\n",
    "study2 = optimize_hyperparameters(\n",
    "    train_dataloader2,\n",
    "    val_dataloader2,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=15,\n",
    "    max_epochs=30,\n",
    "    gradient_clip_val_range=(0.01, 0.5),\n",
    "    hidden_size_range=(12, 32),\n",
    "    hidden_continuous_size_range=(12, 32),\n",
    "    attention_head_size_range=(1, 3),\n",
    "    learning_rate_range=(0.01, 0.5),\n",
    "    dropout_range=(0.1, 0.3),\n",
    "    trainer_kwargs=dict(limit_train_batches=batch_size),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    ")\n",
    "\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"pytorch_lightning_optuna_sim_10_60_l_ho.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study2, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study2.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('sim_10_60_nl_he_train.csv')\n",
    "data2 = data2.sort_values(by=['series_id','time'], ignore_index=True)\n",
    "data2[\"time_idx\"] = data2.index.to_list()\n",
    "data2 = data2[['time', 'time_idx', 'series_id', 'value', 'c_t']]\n",
    "data2 = data2.rename(columns={'series_id': 'variable'})\n",
    "data2['tnc'] = ['1' if i == 'treated' else '0' for i in data2['c_t']]\n",
    "data2_processed = data2[['time', 'time_idx', 'variable', 'value','tnc']]\n",
    "# define the dataset, i.e. add metadata to pandas dataframe for the model to understand it\n",
    "max_encoder_length = 15\n",
    "max_prediction_length = 12\n",
    "\n",
    "training2 = TimeSeriesDataSet(\n",
    "    # data[lambda x: x.date <= training_cutoff],\n",
    "    data2_processed[lambda x: x.time_idx <= data2_processed[\"time_idx\"].max() - max_prediction_length*2],\n",
    "    group_ids=[\"tnc\",\"variable\"],  # column name(s) for timeseries IDs\n",
    "    target= \"value\",  # column name of target to predict\n",
    "    time_idx= \"time_idx\",  # column name of time of observation\n",
    "    max_encoder_length=max_encoder_length,  # how much history to use\n",
    "    max_prediction_length=max_prediction_length,  # how far to predict into future\n",
    "    static_categoricals=[\"tnc\",\"variable\"],\n",
    "    time_varying_unknown_reals=[\"value\"],\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"tnc\",\"variable\"], transformation=\"relu\"),\n",
    ")\n",
    "\n",
    "\n",
    "# create validation dataset using the same normalization techniques as for the training dataset\n",
    "validation2 = TimeSeriesDataSet.from_dataset(training2, data2_processed[lambda x: x.time_idx <= \\\n",
    "              data2_processed[\"time_idx\"].max() - max_prediction_length], stop_randomization=True)\n",
    "\n",
    "# convert datasets to dataloaders for training\n",
    "batch_size = 31\n",
    "train_dataloader2 = training2.to_dataloader(train=True, batch_size=batch_size, num_workers=2)\n",
    "val_dataloader2 = validation2.to_dataloader(train=False, batch_size=batch_size, num_workers=2)\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# create study\n",
    "study3 = optimize_hyperparameters(\n",
    "    train_dataloader2,\n",
    "    val_dataloader2,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=15,\n",
    "    max_epochs=30,\n",
    "    gradient_clip_val_range=(0.01, 0.5),\n",
    "    hidden_size_range=(12, 32),\n",
    "    hidden_continuous_size_range=(12, 32),\n",
    "    attention_head_size_range=(1, 3),\n",
    "    learning_rate_range=(0.01, 0.5),\n",
    "    dropout_range=(0.1, 0.3),\n",
    "    trainer_kwargs=dict(limit_train_batches=batch_size),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    ")\n",
    "\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"pytorch_lightning_optuna_sim_10_60_nl_he.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study3, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study3.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data2 = pd.read_csv('sim_10_60_nl_ho_train.csv')\n",
    "data2 = data2.sort_values(by=['series_id','time'], ignore_index=True)\n",
    "data2[\"time_idx\"] = data2.index.to_list()\n",
    "data2 = data2[['time', 'time_idx', 'series_id', 'value', 'c_t']]\n",
    "data2 = data2.rename(columns={'series_id': 'variable'})\n",
    "data2['tnc'] = ['1' if i == 'treated' else '0' for i in data2['c_t']]\n",
    "data2_processed = data2[['time', 'time_idx', 'variable', 'value','tnc']]\n",
    "# define the dataset, i.e. add metadata to pandas dataframe for the model to understand it\n",
    "max_encoder_length = 15\n",
    "max_prediction_length = 12\n",
    "\n",
    "training2 = TimeSeriesDataSet(\n",
    "    # data[lambda x: x.date <= training_cutoff],\n",
    "    data2_processed[lambda x: x.time_idx <= data2_processed[\"time_idx\"].max() - max_prediction_length*2],\n",
    "    group_ids=[\"tnc\",\"variable\"],  # column name(s) for timeseries IDs\n",
    "    target= \"value\",  # column name of target to predict\n",
    "    time_idx= \"time_idx\",  # column name of time of observation\n",
    "    max_encoder_length=max_encoder_length,  # how much history to use\n",
    "    max_prediction_length=max_prediction_length,  # how far to predict into future\n",
    "    static_categoricals=[\"tnc\",\"variable\"],\n",
    "    time_varying_unknown_reals=[\"value\"],\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"tnc\",\"variable\"], transformation=\"relu\"),\n",
    ")\n",
    "\n",
    "\n",
    "# create validation dataset using the same normalization techniques as for the training dataset\n",
    "validation2 = TimeSeriesDataSet.from_dataset(training2, data2_processed[lambda x: x.time_idx <= \\\n",
    "              data2_processed[\"time_idx\"].max() - max_prediction_length], stop_randomization=True)\n",
    "\n",
    "# convert datasets to dataloaders for training\n",
    "batch_size = 31\n",
    "train_dataloader2 = training2.to_dataloader(train=True, batch_size=batch_size, num_workers=2)\n",
    "val_dataloader2 = validation2.to_dataloader(train=False, batch_size=batch_size, num_workers=2)\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# create study\n",
    "study4 = optimize_hyperparameters(\n",
    "    train_dataloader2,\n",
    "    val_dataloader2,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=15,\n",
    "    max_epochs=30,\n",
    "    gradient_clip_val_range=(0.01, 0.5),\n",
    "    hidden_size_range=(12, 32),\n",
    "    hidden_continuous_size_range=(12, 32),\n",
    "    attention_head_size_range=(1, 3),\n",
    "    learning_rate_range=(0.01, 0.5),\n",
    "    dropout_range=(0.1, 0.3),\n",
    "    trainer_kwargs=dict(limit_train_batches=batch_size),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    ")\n",
    "\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"pytorch_lightning_optuna_sim_10_60_nl_ho.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study4, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study4.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "import pickle\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "data2 = pd.read_csv('sim_10_222_l_he_train.csv')\n",
    "data2 = data2.sort_values(by=['series_id','time'], ignore_index=True)\n",
    "data2[\"time_idx\"] = data2.index.to_list()\n",
    "data2 = data2[['time', 'time_idx', 'series_id', 'value', 'c_t']]\n",
    "data2 = data2.rename(columns={'series_id': 'variable'})\n",
    "data2['tnc'] = ['1' if i == 'treated' else '0' for i in data2['c_t']]\n",
    "data2_processed = data2[['time', 'time_idx', 'variable', 'value','tnc']]\n",
    "# define the dataset, i.e. add metadata to pandas dataframe for the model to understand it\n",
    "max_encoder_length = 15\n",
    "max_prediction_length = 12\n",
    "\n",
    "training2 = TimeSeriesDataSet(\n",
    "    # data[lambda x: x.date <= training_cutoff],\n",
    "    data2_processed[lambda x: x.time_idx <= data2_processed[\"time_idx\"].max() - max_prediction_length*2],\n",
    "    group_ids=[\"tnc\",\"variable\"],  # column name(s) for timeseries IDs\n",
    "    target= \"value\",  # column name of target to predict\n",
    "    time_idx= \"time_idx\",  # column name of time of observation\n",
    "    max_encoder_length=max_encoder_length,  # how much history to use\n",
    "    max_prediction_length=max_prediction_length,  # how far to predict into future\n",
    "    static_categoricals=[\"tnc\",\"variable\"],\n",
    "    time_varying_unknown_reals=[\"value\"],\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"tnc\",\"variable\"], transformation=\"relu\"),\n",
    ")\n",
    "\n",
    "\n",
    "# create validation dataset using the same normalization techniques as for the training dataset\n",
    "validation2 = TimeSeriesDataSet.from_dataset(training2, data2_processed[lambda x: x.time_idx <= \\\n",
    "              data2_processed[\"time_idx\"].max() - max_prediction_length], stop_randomization=True)\n",
    "\n",
    "# convert datasets to dataloaders for training\n",
    "batch_size = 31\n",
    "train_dataloader2 = training2.to_dataloader(train=True, batch_size=batch_size, num_workers=2)\n",
    "val_dataloader2 = validation2.to_dataloader(train=False, batch_size=batch_size, num_workers=2)\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# create study\n",
    "study5 = optimize_hyperparameters(\n",
    "    train_dataloader2,\n",
    "    val_dataloader2,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=15,\n",
    "    max_epochs=30,\n",
    "    gradient_clip_val_range=(0.01, 0.5),\n",
    "    hidden_size_range=(30, 128),\n",
    "    hidden_continuous_size_range=(30, 128),\n",
    "    attention_head_size_range=(1, 3),\n",
    "    learning_rate_range=(0.01, 0.5),\n",
    "    dropout_range=(0.1, 0.3),\n",
    "    trainer_kwargs=dict(limit_train_batches=batch_size),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    ")\n",
    "\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"pytorch_lightning_optuna_sim_10_222_l_he.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study5, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study5.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('sim_10_222_l_ho_train.csv')\n",
    "data2 = data2.sort_values(by=['series_id','time'], ignore_index=True)\n",
    "data2[\"time_idx\"] = data2.index.to_list()\n",
    "data2 = data2[['time', 'time_idx', 'series_id', 'value', 'c_t']]\n",
    "data2 = data2.rename(columns={'series_id': 'variable'})\n",
    "data2['tnc'] = ['1' if i == 'treated' else '0' for i in data2['c_t']]\n",
    "data2_processed = data2[['time', 'time_idx', 'variable', 'value','tnc']]\n",
    "# define the dataset, i.e. add metadata to pandas dataframe for the model to understand it\n",
    "max_encoder_length = 15\n",
    "max_prediction_length = 12\n",
    "\n",
    "training2 = TimeSeriesDataSet(\n",
    "    # data[lambda x: x.date <= training_cutoff],\n",
    "    data2_processed[lambda x: x.time_idx <= data2_processed[\"time_idx\"].max() - max_prediction_length*2],\n",
    "    group_ids=[\"tnc\",\"variable\"],  # column name(s) for timeseries IDs\n",
    "    target= \"value\",  # column name of target to predict\n",
    "    time_idx= \"time_idx\",  # column name of time of observation\n",
    "    max_encoder_length=max_encoder_length,  # how much history to use\n",
    "    max_prediction_length=max_prediction_length,  # how far to predict into future\n",
    "    static_categoricals=[\"tnc\",\"variable\"],\n",
    "    time_varying_unknown_reals=[\"value\"],\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"tnc\",\"variable\"], transformation=\"relu\"),\n",
    ")\n",
    "\n",
    "\n",
    "# create validation dataset using the same normalization techniques as for the training dataset\n",
    "validation2 = TimeSeriesDataSet.from_dataset(training2, data2_processed[lambda x: x.time_idx <= \\\n",
    "              data2_processed[\"time_idx\"].max() - max_prediction_length], stop_randomization=True)\n",
    "\n",
    "# convert datasets to dataloaders for training\n",
    "batch_size = 31\n",
    "train_dataloader2 = training2.to_dataloader(train=True, batch_size=batch_size, num_workers=2)\n",
    "val_dataloader2 = validation2.to_dataloader(train=False, batch_size=batch_size, num_workers=2)\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# create study\n",
    "study6 = optimize_hyperparameters(\n",
    "    train_dataloader2,\n",
    "    val_dataloader2,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=15,\n",
    "    max_epochs=30,\n",
    "    gradient_clip_val_range=(0.01, 0.5),\n",
    "    hidden_size_range=(30, 64),\n",
    "    hidden_continuous_size_range=(30, 64),\n",
    "    attention_head_size_range=(1, 3),\n",
    "    learning_rate_range=(0.01, 0.5),\n",
    "    dropout_range=(0.1, 0.3),\n",
    "    trainer_kwargs=dict(limit_train_batches=batch_size),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    ")\n",
    "\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"pytorch_lightning_optuna_sim_10_222_l_ho.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study6, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study6.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('sim_10_222_nl_he_train.csv')\n",
    "data2 = data2.sort_values(by=['series_id','time'], ignore_index=True)\n",
    "data2[\"time_idx\"] = data2.index.to_list()\n",
    "data2 = data2[['time', 'time_idx', 'series_id', 'value', 'c_t']]\n",
    "data2 = data2.rename(columns={'series_id': 'variable'})\n",
    "data2['tnc'] = ['1' if i == 'treated' else '0' for i in data2['c_t']]\n",
    "data2_processed = data2[['time', 'time_idx', 'variable', 'value','tnc']]\n",
    "# define the dataset, i.e. add metadata to pandas dataframe for the model to understand it\n",
    "max_encoder_length = 15\n",
    "max_prediction_length = 12\n",
    "\n",
    "training2 = TimeSeriesDataSet(\n",
    "    # data[lambda x: x.date <= training_cutoff],\n",
    "    data2_processed[lambda x: x.time_idx <= data2_processed[\"time_idx\"].max() - max_prediction_length*2],\n",
    "    group_ids=[\"tnc\",\"variable\"],  # column name(s) for timeseries IDs\n",
    "    target= \"value\",  # column name of target to predict\n",
    "    time_idx= \"time_idx\",  # column name of time of observation\n",
    "    max_encoder_length=max_encoder_length,  # how much history to use\n",
    "    max_prediction_length=max_prediction_length,  # how far to predict into future\n",
    "    static_categoricals=[\"tnc\",\"variable\"],\n",
    "    time_varying_unknown_reals=[\"value\"],\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"tnc\",\"variable\"], transformation=\"relu\"),\n",
    ")\n",
    "\n",
    "\n",
    "# create validation dataset using the same normalization techniques as for the training dataset\n",
    "validation2 = TimeSeriesDataSet.from_dataset(training2, data2_processed[lambda x: x.time_idx <= \\\n",
    "              data2_processed[\"time_idx\"].max() - max_prediction_length], stop_randomization=True)\n",
    "\n",
    "# convert datasets to dataloaders for training\n",
    "batch_size = 31\n",
    "train_dataloader2 = training2.to_dataloader(train=True, batch_size=batch_size, num_workers=2)\n",
    "val_dataloader2 = validation2.to_dataloader(train=False, batch_size=batch_size, num_workers=2)\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# create study\n",
    "study7 = optimize_hyperparameters(\n",
    "    train_dataloader2,\n",
    "    val_dataloader2,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=15,\n",
    "    max_epochs=30,\n",
    "    gradient_clip_val_range=(0.01, 0.5),\n",
    "    hidden_size_range=(30, 64),\n",
    "    hidden_continuous_size_range=(30, 64),\n",
    "    attention_head_size_range=(1, 3),\n",
    "    learning_rate_range=(0.01, 0.5),\n",
    "    dropout_range=(0.1, 0.3),\n",
    "    trainer_kwargs=dict(limit_train_batches=batch_size),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    ")\n",
    "\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"pytorch_lightning_optuna_sim_10_222_nl_he.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study7, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study7.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('sim_10_222_nl_ho_train.csv')\n",
    "data2 = data2.sort_values(by=['series_id','time'], ignore_index=True)\n",
    "data2[\"time_idx\"] = data2.index.to_list()\n",
    "data2 = data2[['time', 'time_idx', 'series_id', 'value', 'c_t']]\n",
    "data2 = data2.rename(columns={'series_id': 'variable'})\n",
    "data2['tnc'] = ['1' if i == 'treated' else '0' for i in data2['c_t']]\n",
    "data2_processed = data2[['time', 'time_idx', 'variable', 'value','tnc']]\n",
    "# define the dataset, i.e. add metadata to pandas dataframe for the model to understand it\n",
    "max_encoder_length = 15\n",
    "max_prediction_length = 12\n",
    "\n",
    "training2 = TimeSeriesDataSet(\n",
    "    # data[lambda x: x.date <= training_cutoff],\n",
    "    data2_processed[lambda x: x.time_idx <= data2_processed[\"time_idx\"].max() - max_prediction_length*2],\n",
    "    group_ids=[\"tnc\",\"variable\"],  # column name(s) for timeseries IDs\n",
    "    target= \"value\",  # column name of target to predict\n",
    "    time_idx= \"time_idx\",  # column name of time of observation\n",
    "    max_encoder_length=max_encoder_length,  # how much history to use\n",
    "    max_prediction_length=max_prediction_length,  # how far to predict into future\n",
    "    static_categoricals=[\"tnc\",\"variable\"],\n",
    "    time_varying_unknown_reals=[\"value\"],\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"tnc\",\"variable\"], transformation=\"relu\"),\n",
    ")\n",
    "\n",
    "\n",
    "# create validation dataset using the same normalization techniques as for the training dataset\n",
    "validation2 = TimeSeriesDataSet.from_dataset(training2, data2_processed[lambda x: x.time_idx <= \\\n",
    "              data2_processed[\"time_idx\"].max() - max_prediction_length], stop_randomization=True)\n",
    "\n",
    "# convert datasets to dataloaders for training\n",
    "batch_size = 31\n",
    "train_dataloader2 = training2.to_dataloader(train=True, batch_size=batch_size, num_workers=2)\n",
    "val_dataloader2 = validation2.to_dataloader(train=False, batch_size=batch_size, num_workers=2)\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# create study\n",
    "study8 = optimize_hyperparameters(\n",
    "    train_dataloader2,\n",
    "    val_dataloader2,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=15,\n",
    "    max_epochs=30,\n",
    "    gradient_clip_val_range=(0.01, 0.5),\n",
    "    hidden_size_range=(30, 64),\n",
    "    hidden_continuous_size_range=(30, 64),\n",
    "    attention_head_size_range=(1, 3),\n",
    "    learning_rate_range=(0.01, 0.5),\n",
    "    dropout_range=(0.1, 0.3),\n",
    "    trainer_kwargs=dict(limit_train_batches=batch_size),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    ")\n",
    "\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"pytorch_lightning_optuna_sim_10_222_nl_ho.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study8, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study8.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('sim_101_60_l_he_train.csv')\n",
    "data2 = data2.sort_values(by=['series_id','time'], ignore_index=True)\n",
    "data2[\"time_idx\"] = data2.index.to_list()\n",
    "data2 = data2[['time', 'time_idx', 'series_id', 'value', 'c_t']]\n",
    "data2 = data2.rename(columns={'series_id': 'variable'})\n",
    "data2['tnc'] = ['1' if i == 'treated' else '0' for i in data2['c_t']]\n",
    "data2_processed = data2[['time', 'time_idx', 'variable', 'value','tnc']]\n",
    "# define the dataset, i.e. add metadata to pandas dataframe for the model to understand it\n",
    "max_encoder_length = 15\n",
    "max_prediction_length = 12\n",
    "\n",
    "training2 = TimeSeriesDataSet(\n",
    "    # data[lambda x: x.date <= training_cutoff],\n",
    "    data2_processed[lambda x: x.time_idx <= data2_processed[\"time_idx\"].max() - max_prediction_length*2],\n",
    "    group_ids=[\"tnc\",\"variable\"],  # column name(s) for timeseries IDs\n",
    "    target= \"value\",  # column name of target to predict\n",
    "    time_idx= \"time_idx\",  # column name of time of observation\n",
    "    max_encoder_length=max_encoder_length,  # how much history to use\n",
    "    max_prediction_length=max_prediction_length,  # how far to predict into future\n",
    "    static_categoricals=[\"tnc\",\"variable\"],\n",
    "    time_varying_unknown_reals=[\"value\"],\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"tnc\",\"variable\"], transformation=\"relu\"),\n",
    ")\n",
    "\n",
    "\n",
    "# create validation dataset using the same normalization techniques as for the training dataset\n",
    "validation2 = TimeSeriesDataSet.from_dataset(training2, data2_processed[lambda x: x.time_idx <= \\\n",
    "              data2_processed[\"time_idx\"].max() - max_prediction_length], stop_randomization=True)\n",
    "\n",
    "# convert datasets to dataloaders for training\n",
    "batch_size = 31\n",
    "train_dataloader2 = training2.to_dataloader(train=True, batch_size=batch_size, num_workers=2)\n",
    "val_dataloader2 = validation2.to_dataloader(train=False, batch_size=batch_size, num_workers=2)\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# create study\n",
    "study9 = optimize_hyperparameters(\n",
    "    train_dataloader2,\n",
    "    val_dataloader2,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=15,\n",
    "    max_epochs=30,\n",
    "    gradient_clip_val_range=(0.01, 0.5),\n",
    "    hidden_size_range=(12, 64),\n",
    "    hidden_continuous_size_range=(12, 64),\n",
    "    attention_head_size_range=(1, 3),\n",
    "    learning_rate_range=(0.01, 0.5),\n",
    "    dropout_range=(0.1, 0.3),\n",
    "    trainer_kwargs=dict(limit_train_batches=batch_size),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    ")\n",
    "\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"pytorch_lightning_optuna_sim_101_60_l_he.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study9, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study9.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('sim_101_60_l_ho_train.csv')\n",
    "data2 = data2.sort_values(by=['series_id','time'], ignore_index=True)\n",
    "data2[\"time_idx\"] = data2.index.to_list()\n",
    "data2 = data2[['time', 'time_idx', 'series_id', 'value', 'c_t']]\n",
    "data2 = data2.rename(columns={'series_id': 'variable'})\n",
    "data2['tnc'] = ['1' if i == 'treated' else '0' for i in data2['c_t']]\n",
    "data2_processed = data2[['time', 'time_idx', 'variable', 'value','tnc']]\n",
    "# define the dataset, i.e. add metadata to pandas dataframe for the model to understand it\n",
    "max_encoder_length = 15\n",
    "max_prediction_length = 12\n",
    "\n",
    "training2 = TimeSeriesDataSet(\n",
    "    # data[lambda x: x.date <= training_cutoff],\n",
    "    data2_processed[lambda x: x.time_idx <= data2_processed[\"time_idx\"].max() - max_prediction_length*2],\n",
    "    group_ids=[\"tnc\",\"variable\"],  # column name(s) for timeseries IDs\n",
    "    target= \"value\",  # column name of target to predict\n",
    "    time_idx= \"time_idx\",  # column name of time of observation\n",
    "    max_encoder_length=max_encoder_length,  # how much history to use\n",
    "    max_prediction_length=max_prediction_length,  # how far to predict into future\n",
    "    static_categoricals=[\"tnc\",\"variable\"],\n",
    "    time_varying_unknown_reals=[\"value\"],\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"tnc\",\"variable\"], transformation=\"relu\"),\n",
    ")\n",
    "\n",
    "\n",
    "# create validation dataset using the same normalization techniques as for the training dataset\n",
    "validation2 = TimeSeriesDataSet.from_dataset(training2, data2_processed[lambda x: x.time_idx <= \\\n",
    "              data2_processed[\"time_idx\"].max() - max_prediction_length], stop_randomization=True)\n",
    "\n",
    "# convert datasets to dataloaders for training\n",
    "batch_size = 31\n",
    "train_dataloader2 = training2.to_dataloader(train=True, batch_size=batch_size, num_workers=2)\n",
    "val_dataloader2 = validation2.to_dataloader(train=False, batch_size=batch_size, num_workers=2)\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# create study\n",
    "study10 = optimize_hyperparameters(\n",
    "    train_dataloader2,\n",
    "    val_dataloader2,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=15,\n",
    "    max_epochs=30,\n",
    "    gradient_clip_val_range=(0.01, 0.5),\n",
    "    hidden_size_range=(30, 40),\n",
    "    hidden_continuous_size_range=(30, 40),\n",
    "    attention_head_size_range=(1, 3),\n",
    "    learning_rate_range=(0.01, 0.5),\n",
    "    dropout_range=(0.1, 0.3),\n",
    "    trainer_kwargs=dict(limit_train_batches=batch_size),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    ")\n",
    "\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"pytorch_lightning_optuna_sim_101_60_l_ho.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study10, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study10.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('sim_101_60_nl_he_train.csv')\n",
    "data2 = data2.sort_values(by=['series_id','time'], ignore_index=True)\n",
    "data2[\"time_idx\"] = data2.index.to_list()\n",
    "data2 = data2[['time', 'time_idx', 'series_id', 'value', 'c_t']]\n",
    "data2 = data2.rename(columns={'series_id': 'variable'})\n",
    "data2['tnc'] = ['1' if i == 'treated' else '0' for i in data2['c_t']]\n",
    "data2_processed = data2[['time', 'time_idx', 'variable', 'value','tnc']]\n",
    "# define the dataset, i.e. add metadata to pandas dataframe for the model to understand it\n",
    "max_encoder_length = 15\n",
    "max_prediction_length = 12\n",
    "\n",
    "training2 = TimeSeriesDataSet(\n",
    "    # data[lambda x: x.date <= training_cutoff],\n",
    "    data2_processed[lambda x: x.time_idx <= data2_processed[\"time_idx\"].max() - max_prediction_length*2],\n",
    "    group_ids=[\"tnc\",\"variable\"],  # column name(s) for timeseries IDs\n",
    "    target= \"value\",  # column name of target to predict\n",
    "    time_idx= \"time_idx\",  # column name of time of observation\n",
    "    max_encoder_length=max_encoder_length,  # how much history to use\n",
    "    max_prediction_length=max_prediction_length,  # how far to predict into future\n",
    "    static_categoricals=[\"tnc\",\"variable\"],\n",
    "    time_varying_unknown_reals=[\"value\"],\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"tnc\",\"variable\"], transformation=\"relu\"),\n",
    ")\n",
    "\n",
    "\n",
    "# create validation dataset using the same normalization techniques as for the training dataset\n",
    "validation2 = TimeSeriesDataSet.from_dataset(training2, data2_processed[lambda x: x.time_idx <= \\\n",
    "              data2_processed[\"time_idx\"].max() - max_prediction_length], stop_randomization=True)\n",
    "\n",
    "# convert datasets to dataloaders for training\n",
    "batch_size = 31\n",
    "train_dataloader2 = training2.to_dataloader(train=True, batch_size=batch_size, num_workers=2)\n",
    "val_dataloader2 = validation2.to_dataloader(train=False, batch_size=batch_size, num_workers=2)\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# create study\n",
    "study11 = optimize_hyperparameters(\n",
    "    train_dataloader2,\n",
    "    val_dataloader2,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=15,\n",
    "    max_epochs=30,\n",
    "    gradient_clip_val_range=(0.01, 0.5),\n",
    "    hidden_size_range=(30, 40),\n",
    "    hidden_continuous_size_range=(30, 40),\n",
    "    attention_head_size_range=(1, 3),\n",
    "    learning_rate_range=(0.01, 0.5),\n",
    "    dropout_range=(0.1, 0.3),\n",
    "    trainer_kwargs=dict(limit_train_batches=batch_size),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    ")\n",
    "\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"pytorch_lightning_optuna_sim_101_60_nl_he.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study11, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study11.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('sim_101_60_nl_ho_train.csv')\n",
    "data2 = data2.sort_values(by=['series_id','time'], ignore_index=True)\n",
    "data2[\"time_idx\"] = data2.index.to_list()\n",
    "data2 = data2[['time', 'time_idx', 'series_id', 'value', 'c_t']]\n",
    "data2 = data2.rename(columns={'series_id': 'variable'})\n",
    "data2['tnc'] = ['1' if i == 'treated' else '0' for i in data2['c_t']]\n",
    "data2_processed = data2[['time', 'time_idx', 'variable', 'value','tnc']]\n",
    "# define the dataset, i.e. add metadata to pandas dataframe for the model to understand it\n",
    "max_encoder_length = 15\n",
    "max_prediction_length = 12\n",
    "\n",
    "training2 = TimeSeriesDataSet(\n",
    "    # data[lambda x: x.date <= training_cutoff],\n",
    "    data2_processed[lambda x: x.time_idx <= data2_processed[\"time_idx\"].max() - max_prediction_length*2],\n",
    "    group_ids=[\"tnc\",\"variable\"],  # column name(s) for timeseries IDs\n",
    "    target= \"value\",  # column name of target to predict\n",
    "    time_idx= \"time_idx\",  # column name of time of observation\n",
    "    max_encoder_length=max_encoder_length,  # how much history to use\n",
    "    max_prediction_length=max_prediction_length,  # how far to predict into future\n",
    "    static_categoricals=[\"tnc\",\"variable\"],\n",
    "    time_varying_unknown_reals=[\"value\"],\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"tnc\",\"variable\"], transformation=\"relu\"),\n",
    ")\n",
    "\n",
    "\n",
    "# create validation dataset using the same normalization techniques as for the training dataset\n",
    "validation2 = TimeSeriesDataSet.from_dataset(training2, data2_processed[lambda x: x.time_idx <= \\\n",
    "              data2_processed[\"time_idx\"].max() - max_prediction_length], stop_randomization=True)\n",
    "\n",
    "# convert datasets to dataloaders for training\n",
    "batch_size = 31\n",
    "train_dataloader2 = training2.to_dataloader(train=True, batch_size=batch_size, num_workers=2)\n",
    "val_dataloader2 = validation2.to_dataloader(train=False, batch_size=batch_size, num_workers=2)\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# create study\n",
    "study12 = optimize_hyperparameters(\n",
    "    train_dataloader2,\n",
    "    val_dataloader2,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=15,\n",
    "    max_epochs=30,\n",
    "    gradient_clip_val_range=(0.1, 0.7),\n",
    "    hidden_size_range=(30, 40),\n",
    "    hidden_continuous_size_range=(30, 40),\n",
    "    attention_head_size_range=(2, 4),\n",
    "    learning_rate_range=(0.01, 0.5),\n",
    "    dropout_range=(0.1, 0.5),\n",
    "    trainer_kwargs=dict(limit_train_batches=batch_size),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    ")\n",
    "\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"pytorch_lightning_optuna_sim_101_60_nl_ho.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study12, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study12.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('sim_101_222_l_he_train.csv')\n",
    "data2 = data2.sort_values(by=['series_id','time'], ignore_index=True)\n",
    "data2[\"time_idx\"] = data2.index.to_list()\n",
    "data2 = data2[['time', 'time_idx', 'series_id', 'value', 'c_t']]\n",
    "data2 = data2.rename(columns={'series_id': 'variable'})\n",
    "data2['tnc'] = ['1' if i == 'treated' else '0' for i in data2['c_t']]\n",
    "data2_processed = data2[['time', 'time_idx', 'variable', 'value','tnc']]\n",
    "# define the dataset, i.e. add metadata to pandas dataframe for the model to understand it\n",
    "max_encoder_length = 15\n",
    "max_prediction_length = 12\n",
    "\n",
    "training2 = TimeSeriesDataSet(\n",
    "    # data[lambda x: x.date <= training_cutoff],\n",
    "    data2_processed[lambda x: x.time_idx <= data2_processed[\"time_idx\"].max() - max_prediction_length*2],\n",
    "    group_ids=[\"tnc\",\"variable\"],  # column name(s) for timeseries IDs\n",
    "    target= \"value\",  # column name of target to predict\n",
    "    time_idx= \"time_idx\",  # column name of time of observation\n",
    "    max_encoder_length=max_encoder_length,  # how much history to use\n",
    "    max_prediction_length=max_prediction_length,  # how far to predict into future\n",
    "    static_categoricals=[\"tnc\",\"variable\"],\n",
    "    time_varying_unknown_reals=[\"value\"],\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"tnc\",\"variable\"], transformation=\"relu\"),\n",
    ")\n",
    "\n",
    "\n",
    "# create validation dataset using the same normalization techniques as for the training dataset\n",
    "validation2 = TimeSeriesDataSet.from_dataset(training2, data2_processed[lambda x: x.time_idx <= \\\n",
    "              data2_processed[\"time_idx\"].max() - max_prediction_length], stop_randomization=True)\n",
    "\n",
    "# convert datasets to dataloaders for training\n",
    "batch_size = 124\n",
    "train_dataloader2 = training2.to_dataloader(train=True, batch_size=batch_size, num_workers=4)\n",
    "val_dataloader2 = validation2.to_dataloader(train=False, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# create study\n",
    "study13 = optimize_hyperparameters(\n",
    "    train_dataloader2,\n",
    "    val_dataloader2,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=8,\n",
    "    max_epochs=15,\n",
    "    gradient_clip_val_range=(0.1, 0.7),\n",
    "    hidden_size_range=(30, 64),\n",
    "    hidden_continuous_size_range=(30, 64),\n",
    "    attention_head_size_range=(2, 4),\n",
    "    learning_rate_range=(0.01, 0.5),\n",
    "    dropout_range=(0.1, 0.5),\n",
    "    trainer_kwargs=dict(limit_train_batches=batch_size),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    ")\n",
    "\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"pytorch_lightning_optuna_sim_101_222_l_he.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study13, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study13.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('sim_101_222_l_ho_train.csv')\n",
    "data2 = data2.sort_values(by=['series_id','time'], ignore_index=True)\n",
    "data2[\"time_idx\"] = data2.index.to_list()\n",
    "data2 = data2[['time', 'time_idx', 'series_id', 'value', 'c_t']]\n",
    "data2 = data2.rename(columns={'series_id': 'variable'})\n",
    "data2['tnc'] = ['1' if i == 'treated' else '0' for i in data2['c_t']]\n",
    "data2_processed = data2[['time', 'time_idx', 'variable', 'value','tnc']]\n",
    "# define the dataset, i.e. add metadata to pandas dataframe for the model to understand it\n",
    "max_encoder_length = 15\n",
    "max_prediction_length = 12\n",
    "\n",
    "training2 = TimeSeriesDataSet(\n",
    "    # data[lambda x: x.date <= training_cutoff],\n",
    "    data2_processed[lambda x: x.time_idx <= data2_processed[\"time_idx\"].max() - max_prediction_length*2],\n",
    "    group_ids=[\"tnc\",\"variable\"],  # column name(s) for timeseries IDs\n",
    "    target= \"value\",  # column name of target to predict\n",
    "    time_idx= \"time_idx\",  # column name of time of observation\n",
    "    max_encoder_length=max_encoder_length,  # how much history to use\n",
    "    max_prediction_length=max_prediction_length,  # how far to predict into future\n",
    "    static_categoricals=[\"tnc\",\"variable\"],\n",
    "    time_varying_unknown_reals=[\"value\"],\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"tnc\",\"variable\"], transformation=\"relu\"),\n",
    ")\n",
    "\n",
    "\n",
    "# create validation dataset using the same normalization techniques as for the training dataset\n",
    "validation2 = TimeSeriesDataSet.from_dataset(training2, data2_processed[lambda x: x.time_idx <= \\\n",
    "              data2_processed[\"time_idx\"].max() - max_prediction_length], stop_randomization=True)\n",
    "\n",
    "# convert datasets to dataloaders for training\n",
    "batch_size = 124\n",
    "train_dataloader2 = training2.to_dataloader(train=True, batch_size=batch_size, num_workers=4)\n",
    "val_dataloader2 = validation2.to_dataloader(train=False, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# create study\n",
    "study14 = optimize_hyperparameters(\n",
    "    train_dataloader2,\n",
    "    val_dataloader2,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=8,\n",
    "    max_epochs=15,\n",
    "    gradient_clip_val_range=(0.1, 0.7),\n",
    "    hidden_size_range=(30, 64),\n",
    "    hidden_continuous_size_range=(30, 64),\n",
    "    attention_head_size_range=(2, 4),\n",
    "    learning_rate_range=(0.01, 0.5),\n",
    "    dropout_range=(0.1, 0.5),\n",
    "    trainer_kwargs=dict(limit_train_batches=batch_size),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    ")\n",
    "\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"pytorch_lightning_optuna_sim_101_222_l_ho.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study14, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study14.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('sim_101_222_nl_he_train.csv')\n",
    "data2 = data2.sort_values(by=['series_id','time'], ignore_index=True)\n",
    "data2[\"time_idx\"] = data2.index.to_list()\n",
    "data2 = data2[['time', 'time_idx', 'series_id', 'value', 'c_t']]\n",
    "data2 = data2.rename(columns={'series_id': 'variable'})\n",
    "data2['tnc'] = ['1' if i == 'treated' else '0' for i in data2['c_t']]\n",
    "data2_processed = data2[['time', 'time_idx', 'variable', 'value','tnc']]\n",
    "# define the dataset, i.e. add metadata to pandas dataframe for the model to understand it\n",
    "max_encoder_length = 15\n",
    "max_prediction_length = 12\n",
    "\n",
    "training2 = TimeSeriesDataSet(\n",
    "    # data[lambda x: x.date <= training_cutoff],\n",
    "    data2_processed[lambda x: x.time_idx <= data2_processed[\"time_idx\"].max() - max_prediction_length*2],\n",
    "    group_ids=[\"tnc\",\"variable\"],  # column name(s) for timeseries IDs\n",
    "    target= \"value\",  # column name of target to predict\n",
    "    time_idx= \"time_idx\",  # column name of time of observation\n",
    "    max_encoder_length=max_encoder_length,  # how much history to use\n",
    "    max_prediction_length=max_prediction_length,  # how far to predict into future\n",
    "    static_categoricals=[\"tnc\",\"variable\"],\n",
    "    time_varying_unknown_reals=[\"value\"],\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"tnc\",\"variable\"], transformation=\"relu\"),\n",
    ")\n",
    "\n",
    "\n",
    "# create validation dataset using the same normalization techniques as for the training dataset\n",
    "validation2 = TimeSeriesDataSet.from_dataset(training2, data2_processed[lambda x: x.time_idx <= \\\n",
    "              data2_processed[\"time_idx\"].max() - max_prediction_length], stop_randomization=True)\n",
    "\n",
    "# convert datasets to dataloaders for training\n",
    "batch_size = 124\n",
    "train_dataloader2 = training2.to_dataloader(train=True, batch_size=batch_size, num_workers=4)\n",
    "val_dataloader2 = validation2.to_dataloader(train=False, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# create study\n",
    "study15 = optimize_hyperparameters(\n",
    "    train_dataloader2,\n",
    "    val_dataloader2,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=8,\n",
    "    max_epochs=15,\n",
    "    gradient_clip_val_range=(0.1, 0.7),\n",
    "    hidden_size_range=(30, 64),\n",
    "    hidden_continuous_size_range=(30, 64),\n",
    "    attention_head_size_range=(2, 4),\n",
    "    learning_rate_range=(0.01, 0.5),\n",
    "    dropout_range=(0.1, 0.5),\n",
    "    trainer_kwargs=dict(limit_train_batches=batch_size),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    ")\n",
    "\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"pytorch_lightning_optuna_sim_101_222_nl_he.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study15, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study15.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('sim_101_222_nl_ho_train.csv')\n",
    "data2 = data2.sort_values(by=['series_id','time'], ignore_index=True)\n",
    "data2[\"time_idx\"] = data2.index.to_list()\n",
    "data2 = data2[['time', 'time_idx', 'series_id', 'value', 'c_t']]\n",
    "data2 = data2.rename(columns={'series_id': 'variable'})\n",
    "data2['tnc'] = ['1' if i == 'treated' else '0' for i in data2['c_t']]\n",
    "data2_processed = data2[['time', 'time_idx', 'variable', 'value','tnc']]\n",
    "# define the dataset, i.e. add metadata to pandas dataframe for the model to understand it\n",
    "max_encoder_length = 15\n",
    "max_prediction_length = 12\n",
    "\n",
    "training2 = TimeSeriesDataSet(\n",
    "    # data[lambda x: x.date <= training_cutoff],\n",
    "    data2_processed[lambda x: x.time_idx <= data2_processed[\"time_idx\"].max() - max_prediction_length*2],\n",
    "    group_ids=[\"tnc\",\"variable\"],  # column name(s) for timeseries IDs\n",
    "    target= \"value\",  # column name of target to predict\n",
    "    time_idx= \"time_idx\",  # column name of time of observation\n",
    "    max_encoder_length=max_encoder_length,  # how much history to use\n",
    "    max_prediction_length=max_prediction_length,  # how far to predict into future\n",
    "    static_categoricals=[\"tnc\",\"variable\"],\n",
    "    time_varying_unknown_reals=[\"value\"],\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"tnc\",\"variable\"], transformation=\"relu\"),\n",
    ")\n",
    "\n",
    "\n",
    "# create validation dataset using the same normalization techniques as for the training dataset\n",
    "validation2 = TimeSeriesDataSet.from_dataset(training2, data2_processed[lambda x: x.time_idx <= \\\n",
    "              data2_processed[\"time_idx\"].max() - max_prediction_length], stop_randomization=True)\n",
    "\n",
    "# convert datasets to dataloaders for training\n",
    "batch_size = 124\n",
    "train_dataloader2 = training2.to_dataloader(train=True, batch_size=batch_size, num_workers=4)\n",
    "val_dataloader2 = validation2.to_dataloader(train=False, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# create study\n",
    "study16 = optimize_hyperparameters(\n",
    "    train_dataloader2,\n",
    "    val_dataloader2,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=8,\n",
    "    max_epochs=15,\n",
    "    gradient_clip_val_range=(0.1, 0.7),\n",
    "    hidden_size_range=(30, 64),\n",
    "    hidden_continuous_size_range=(30, 64),\n",
    "    attention_head_size_range=(2, 4),\n",
    "    learning_rate_range=(0.01, 0.3),\n",
    "    dropout_range=(0.1, 0.5),\n",
    "    trainer_kwargs=dict(limit_train_batches=batch_size),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    ")\n",
    "\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"pytorch_lightning_optuna_sim_101_222_nl_ho.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study16, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study16.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('sim_500_222_l_he_train.csv')\n",
    "data2 = data2.sort_values(by=['series_id','time'], ignore_index=True)\n",
    "data2[\"time_idx\"] = data2.index.to_list()\n",
    "data2 = data2[['time', 'time_idx', 'series_id', 'value', 'c_t']]\n",
    "data2 = data2.rename(columns={'series_id': 'variable'})\n",
    "data2['tnc'] = ['1' if i == 'treated' else '0' for i in data2['c_t']]\n",
    "data2_processed = data2[['time', 'time_idx', 'variable', 'value','tnc']]\n",
    "# define the dataset, i.e. add metadata to pandas dataframe for the model to understand it\n",
    "max_encoder_length = 15\n",
    "max_prediction_length = 12\n",
    "\n",
    "training2 = TimeSeriesDataSet(\n",
    "    # data[lambda x: x.date <= training_cutoff],\n",
    "    data2_processed[lambda x: x.time_idx <= data2_processed[\"time_idx\"].max() - max_prediction_length*2],\n",
    "    group_ids=[\"tnc\",\"variable\"],  # column name(s) for timeseries IDs\n",
    "    target= \"value\",  # column name of target to predict\n",
    "    time_idx= \"time_idx\",  # column name of time of observation\n",
    "    max_encoder_length=max_encoder_length,  # how much history to use\n",
    "    max_prediction_length=max_prediction_length,  # how far to predict into future\n",
    "    static_categoricals=[\"tnc\",\"variable\"],\n",
    "    time_varying_unknown_reals=[\"value\"],\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"tnc\",\"variable\"], transformation=\"relu\"),\n",
    ")\n",
    "\n",
    "\n",
    "# create validation dataset using the same normalization techniques as for the training dataset\n",
    "validation2 = TimeSeriesDataSet.from_dataset(training2, data2_processed[lambda x: x.time_idx <= \\\n",
    "              data2_processed[\"time_idx\"].max() - max_prediction_length], stop_randomization=True)\n",
    "\n",
    "# convert datasets to dataloaders for training\n",
    "batch_size = 124\n",
    "train_dataloader2 = training2.to_dataloader(train=True, batch_size=batch_size, num_workers=4)\n",
    "val_dataloader2 = validation2.to_dataloader(train=False, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# create study\n",
    "study17 = optimize_hyperparameters(\n",
    "    train_dataloader2,\n",
    "    val_dataloader2,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=8,\n",
    "    max_epochs=15,\n",
    "    gradient_clip_val_range=(0.1, 0.9),\n",
    "    hidden_size_range=(30, 64),\n",
    "    hidden_continuous_size_range=(30, 64),\n",
    "    attention_head_size_range=(3, 5),\n",
    "    learning_rate_range=(0.01, 0.3),\n",
    "    dropout_range=(0.1, 0.9),\n",
    "    trainer_kwargs=dict(limit_train_batches=batch_size),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    ")\n",
    "\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"pytorch_lightning_optuna_sim_500_222_l_he.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study17, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study17.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('sim_500_222_l_ho_train.csv')\n",
    "data2 = data2.sort_values(by=['series_id','time'], ignore_index=True)\n",
    "data2[\"time_idx\"] = data2.index.to_list()\n",
    "data2 = data2[['time', 'time_idx', 'series_id', 'value', 'c_t']]\n",
    "data2 = data2.rename(columns={'series_id': 'variable'})\n",
    "data2['tnc'] = ['1' if i == 'treated' else '0' for i in data2['c_t']]\n",
    "data2_processed = data2[['time', 'time_idx', 'variable', 'value','tnc']]\n",
    "# define the dataset, i.e. add metadata to pandas dataframe for the model to understand it\n",
    "max_encoder_length = 15\n",
    "max_prediction_length = 12\n",
    "\n",
    "training2 = TimeSeriesDataSet(\n",
    "    # data[lambda x: x.date <= training_cutoff],\n",
    "    data2_processed[lambda x: x.time_idx <= data2_processed[\"time_idx\"].max() - max_prediction_length*2],\n",
    "    group_ids=[\"tnc\",\"variable\"],  # column name(s) for timeseries IDs\n",
    "    target= \"value\",  # column name of target to predict\n",
    "    time_idx= \"time_idx\",  # column name of time of observation\n",
    "    max_encoder_length=max_encoder_length,  # how much history to use\n",
    "    max_prediction_length=max_prediction_length,  # how far to predict into future\n",
    "    static_categoricals=[\"tnc\",\"variable\"],\n",
    "    time_varying_unknown_reals=[\"value\"],\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"tnc\",\"variable\"], transformation=\"relu\"),\n",
    ")\n",
    "\n",
    "\n",
    "# create validation dataset using the same normalization techniques as for the training dataset\n",
    "validation2 = TimeSeriesDataSet.from_dataset(training2, data2_processed[lambda x: x.time_idx <= \\\n",
    "              data2_processed[\"time_idx\"].max() - max_prediction_length], stop_randomization=True)\n",
    "\n",
    "# convert datasets to dataloaders for training\n",
    "batch_size = 124\n",
    "train_dataloader2 = training2.to_dataloader(train=True, batch_size=batch_size, num_workers=4)\n",
    "val_dataloader2 = validation2.to_dataloader(train=False, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# create study\n",
    "study18 = optimize_hyperparameters(\n",
    "    train_dataloader2,\n",
    "    val_dataloader2,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=8,\n",
    "    max_epochs=15,\n",
    "    gradient_clip_val_range=(0.1, 0.9),\n",
    "    hidden_size_range=(30, 64),\n",
    "    hidden_continuous_size_range=(30, 64),\n",
    "    attention_head_size_range=(3, 5),\n",
    "    learning_rate_range=(0.01, 0.3),\n",
    "    dropout_range=(0.1, 0.9),\n",
    "    trainer_kwargs=dict(limit_train_batches=batch_size),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    ")\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"pytorch_lightning_optuna_sim_500_222_l_ho.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study18, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study18.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('sim_500_222_nl_he_train.csv')\n",
    "data2 = data2.sort_values(by=['series_id','time'], ignore_index=True)\n",
    "data2[\"time_idx\"] = data2.index.to_list()\n",
    "data2 = data2[['time', 'time_idx', 'series_id', 'value', 'c_t']]\n",
    "data2 = data2.rename(columns={'series_id': 'variable'})\n",
    "data2['tnc'] = ['1' if i == 'treated' else '0' for i in data2['c_t']]\n",
    "data2_processed = data2[['time', 'time_idx', 'variable', 'value','tnc']]\n",
    "# define the dataset, i.e. add metadata to pandas dataframe for the model to understand it\n",
    "max_encoder_length = 15\n",
    "max_prediction_length = 12\n",
    "\n",
    "training2 = TimeSeriesDataSet(\n",
    "    # data[lambda x: x.date <= training_cutoff],\n",
    "    data2_processed[lambda x: x.time_idx <= data2_processed[\"time_idx\"].max() - max_prediction_length*2],\n",
    "    group_ids=[\"tnc\",\"variable\"],  # column name(s) for timeseries IDs\n",
    "    target= \"value\",  # column name of target to predict\n",
    "    time_idx= \"time_idx\",  # column name of time of observation\n",
    "    max_encoder_length=max_encoder_length,  # how much history to use\n",
    "    max_prediction_length=max_prediction_length,  # how far to predict into future\n",
    "    static_categoricals=[\"tnc\",\"variable\"],\n",
    "    time_varying_unknown_reals=[\"value\"],\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"tnc\",\"variable\"], transformation=\"relu\"),\n",
    ")\n",
    "\n",
    "\n",
    "# create validation dataset using the same normalization techniques as for the training dataset\n",
    "validation2 = TimeSeriesDataSet.from_dataset(training2, data2_processed[lambda x: x.time_idx <= \\\n",
    "              data2_processed[\"time_idx\"].max() - max_prediction_length], stop_randomization=True)\n",
    "\n",
    "# convert datasets to dataloaders for training\n",
    "batch_size = 124\n",
    "train_dataloader2 = training2.to_dataloader(train=True, batch_size=batch_size, num_workers=4)\n",
    "val_dataloader2 = validation2.to_dataloader(train=False, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# create study\n",
    "study19 = optimize_hyperparameters(\n",
    "    train_dataloader2,\n",
    "    val_dataloader2,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=8,\n",
    "    max_epochs=15,\n",
    "    gradient_clip_val_range=(0.1, 0.9),\n",
    "    hidden_size_range=(30, 64),\n",
    "    hidden_continuous_size_range=(30, 64),\n",
    "    attention_head_size_range=(3, 5),\n",
    "    learning_rate_range=(0.01, 0.3),\n",
    "    dropout_range=(0.1, 0.9),\n",
    "    trainer_kwargs=dict(limit_train_batches=batch_size),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    ")\n",
    "\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"pytorch_lightning_optuna_sim_500_222_nl_he.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study19, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study19.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('sim_500_222_nl_ho_train.csv')\n",
    "data2 = data2.sort_values(by=['series_id','time'], ignore_index=True)\n",
    "data2[\"time_idx\"] = data2.index.to_list()\n",
    "data2 = data2[['time', 'time_idx', 'series_id', 'value', 'c_t']]\n",
    "data2 = data2.rename(columns={'series_id': 'variable'})\n",
    "data2['tnc'] = ['1' if i == 'treated' else '0' for i in data2['c_t']]\n",
    "data2_processed = data2[['time', 'time_idx', 'variable', 'value','tnc']]\n",
    "# define the dataset, i.e. add metadata to pandas dataframe for the model to understand it\n",
    "max_encoder_length = 15\n",
    "max_prediction_length = 12\n",
    "\n",
    "training2 = TimeSeriesDataSet(\n",
    "    # data[lambda x: x.date <= training_cutoff],\n",
    "    data2_processed[lambda x: x.time_idx <= data2_processed[\"time_idx\"].max() - max_prediction_length*2],\n",
    "    group_ids=[\"tnc\",\"variable\"],  # column name(s) for timeseries IDs\n",
    "    target= \"value\",  # column name of target to predict\n",
    "    time_idx= \"time_idx\",  # column name of time of observation\n",
    "    max_encoder_length=max_encoder_length,  # how much history to use\n",
    "    max_prediction_length=max_prediction_length,  # how far to predict into future\n",
    "    static_categoricals=[\"tnc\",\"variable\"],\n",
    "    time_varying_unknown_reals=[\"value\"],\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"tnc\",\"variable\"], transformation=\"relu\"),\n",
    ")\n",
    "\n",
    "\n",
    "# create validation dataset using the same normalization techniques as for the training dataset\n",
    "validation2 = TimeSeriesDataSet.from_dataset(training2, data2_processed[lambda x: x.time_idx <= \\\n",
    "              data2_processed[\"time_idx\"].max() - max_prediction_length], stop_randomization=True)\n",
    "\n",
    "# convert datasets to dataloaders for training\n",
    "batch_size = 124\n",
    "train_dataloader2 = training2.to_dataloader(train=True, batch_size=batch_size, num_workers=4)\n",
    "val_dataloader2 = validation2.to_dataloader(train=False, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# create study\n",
    "study20 = optimize_hyperparameters(\n",
    "    train_dataloader2,\n",
    "    val_dataloader2,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=8,\n",
    "    max_epochs=15,\n",
    "    gradient_clip_val_range=(0.1, 0.9),\n",
    "    hidden_size_range=(30, 64),\n",
    "    hidden_continuous_size_range=(30, 64),\n",
    "    attention_head_size_range=(3, 5),\n",
    "    learning_rate_range=(0.01, 0.3),\n",
    "    dropout_range=(0.1, 0.9),\n",
    "    trainer_kwargs=dict(limit_train_batches=batch_size),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    ")\n",
    "\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"pytorch_lightning_optuna_sim_500_222_nl_ho.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study20, fo ut)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study20.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('sim_500_60_l_he_train.csv')\n",
    "data2 = data2.sort_values(by=['series_id','time'], ignore_index=True)\n",
    "data2[\"time_idx\"] = data2.index.to_list()\n",
    "data2 = data2[['time', 'time_idx', 'series_id', 'value', 'c_t']]\n",
    "data2 = data2.rename(columns={'series_id': 'variable'})\n",
    "data2['tnc'] = ['1' if i == 'treated' else '0' for i in data2['c_t']]\n",
    "data2_processed = data2[['time', 'time_idx', 'variable', 'value','tnc']]\n",
    "# define the dataset, i.e. add metadata to pandas dataframe for the model to understand it\n",
    "max_encoder_length = 15\n",
    "max_prediction_length = 12\n",
    "\n",
    "training2 = TimeSeriesDataSet(\n",
    "    # data[lambda x: x.date <= training_cutoff],\n",
    "    data2_processed[lambda x: x.time_idx <= data2_processed[\"time_idx\"].max() - max_prediction_length*2],\n",
    "    group_ids=[\"tnc\",\"variable\"],  # column name(s) for timeseries IDs\n",
    "    target= \"value\",  # column name of target to predict\n",
    "    time_idx= \"time_idx\",  # column name of time of observation\n",
    "    max_encoder_length=max_encoder_length,  # how much history to use\n",
    "    max_prediction_length=max_prediction_length,  # how far to predict into future\n",
    "    static_categoricals=[\"tnc\",\"variable\"],\n",
    "    time_varying_unknown_reals=[\"value\"],\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"tnc\",\"variable\"], transformation=\"relu\"),\n",
    ")\n",
    "\n",
    "\n",
    "# create validation dataset using the same normalization techniques as for the training dataset\n",
    "validation2 = TimeSeriesDataSet.from_dataset(training2, data2_processed[lambda x: x.time_idx <= \\\n",
    "              data2_processed[\"time_idx\"].max() - max_prediction_length], stop_randomization=True)\n",
    "\n",
    "# convert datasets to dataloaders for training\n",
    "batch_size = 124\n",
    "train_dataloader2 = training2.to_dataloader(train=True, batch_size=batch_size, num_workers=4)\n",
    "val_dataloader2 = validation2.to_dataloader(train=False, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# create study\n",
    "study21 = optimize_hyperparameters(\n",
    "    train_dataloader2,\n",
    "    val_dataloader2,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=8,\n",
    "    max_epochs=15,\n",
    "    gradient_clip_val_range=(0.2, 0.5),\n",
    "    hidden_size_range=(30, 40),\n",
    "    hidden_continuous_size_range=(30, 40),\n",
    "    attention_head_size_range=(3, 4),\n",
    "    learning_rate_range=(0.01, 0.3),\n",
    "    dropout_range=(0.2, 0.6),\n",
    "    trainer_kwargs=dict(limit_train_batches=batch_size),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    ")\n",
    "\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"pytorch_lightning_optuna_sim_500_60_l_he.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study21, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study21.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('sim_500_60_l_ho_train.csv')\n",
    "data2 = data2.sort_values(by=['series_id','time'], ignore_index=True)\n",
    "data2[\"time_idx\"] = data2.index.to_list()\n",
    "data2 = data2[['time', 'time_idx', 'series_id', 'value', 'c_t']]\n",
    "data2 = data2.rename(columns={'series_id': 'variable'})\n",
    "data2['tnc'] = ['1' if i == 'treated' else '0' for i in data2['c_t']]\n",
    "data2_processed = data2[['time', 'time_idx', 'variable', 'value','tnc']]\n",
    "# define the dataset, i.e. add metadata to pandas dataframe for the model to understand it\n",
    "max_encoder_length = 15\n",
    "max_prediction_length = 12\n",
    "\n",
    "training2 = TimeSeriesDataSet(\n",
    "    # data[lambda x: x.date <= training_cutoff],\n",
    "    data2_processed[lambda x: x.time_idx <= data2_processed[\"time_idx\"].max() - max_prediction_length*2],\n",
    "    group_ids=[\"tnc\",\"variable\"],  # column name(s) for timeseries IDs\n",
    "    target= \"value\",  # column name of target to predict\n",
    "    time_idx= \"time_idx\",  # column name of time of observation\n",
    "    max_encoder_length=max_encoder_length,  # how much history to use\n",
    "    max_prediction_length=max_prediction_length,  # how far to predict into future\n",
    "    static_categoricals=[\"tnc\",\"variable\"],\n",
    "    time_varying_unknown_reals=[\"value\"],\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"tnc\",\"variable\"], transformation=\"relu\"),\n",
    ")\n",
    "\n",
    "\n",
    "# create validation dataset using the same normalization techniques as for the training dataset\n",
    "validation2 = TimeSeriesDataSet.from_dataset(training2, data2_processed[lambda x: x.time_idx <= \\\n",
    "              data2_processed[\"time_idx\"].max() - max_prediction_length], stop_randomization=True)\n",
    "\n",
    "# convert datasets to dataloaders for training\n",
    "batch_size = 124\n",
    "train_dataloader2 = training2.to_dataloader(train=True, batch_size=batch_size, num_workers=4)\n",
    "val_dataloader2 = validation2.to_dataloader(train=False, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# create study\n",
    "study22 = optimize_hyperparameters(\n",
    "    train_dataloader2,\n",
    "    val_dataloader2,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=8,\n",
    "    max_epochs=15,\n",
    "    gradient_clip_val_range=(0.2, 0.5),\n",
    "    hidden_size_range=(30, 40),\n",
    "    hidden_continuous_size_range=(30, 40),\n",
    "    attention_head_size_range=(3, 4),\n",
    "    learning_rate_range=(0.01, 0.3),\n",
    "    dropout_range=(0.2, 0.6),\n",
    "    trainer_kwargs=dict(limit_train_batches=batch_size),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    ")\n",
    "\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"pytorch_lightning_optuna_sim_500_60_l_ho.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study22, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study22.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('sim_500_60_nl_he_train.csv')\n",
    "data2 = data2.sort_values(by=['series_id','time'], ignore_index=True)\n",
    "data2[\"time_idx\"] = data2.index.to_list()\n",
    "data2 = data2[['time', 'time_idx', 'series_id', 'value', 'c_t']]\n",
    "data2 = data2.rename(columns={'series_id': 'variable'})\n",
    "data2['tnc'] = ['1' if i == 'treated' else '0' for i in data2['c_t']]\n",
    "data2_processed = data2[['time', 'time_idx', 'variable', 'value','tnc']]\n",
    "# define the dataset, i.e. add metadata to pandas dataframe for the model to understand it\n",
    "max_encoder_length = 15\n",
    "max_prediction_length = 12\n",
    "\n",
    "training2 = TimeSeriesDataSet(\n",
    "    # data[lambda x: x.date <= training_cutoff],\n",
    "    data2_processed[lambda x: x.time_idx <= data2_processed[\"time_idx\"].max() - max_prediction_length*2],\n",
    "    group_ids=[\"tnc\",\"variable\"],  # column name(s) for timeseries IDs\n",
    "    target= \"value\",  # column name of target to predict\n",
    "    time_idx= \"time_idx\",  # column name of time of observation\n",
    "    max_encoder_length=max_encoder_length,  # how much history to use\n",
    "    max_prediction_length=max_prediction_length,  # how far to predict into future\n",
    "    static_categoricals=[\"tnc\",\"variable\"],\n",
    "    time_varying_unknown_reals=[\"value\"],\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"tnc\",\"variable\"], transformation=\"relu\"),\n",
    ")\n",
    "\n",
    "\n",
    "# create validation dataset using the same normalization techniques as for the training dataset\n",
    "validation2 = TimeSeriesDataSet.from_dataset(training2, data2_processed[lambda x: x.time_idx <= \\\n",
    "              data2_processed[\"time_idx\"].max() - max_prediction_length], stop_randomization=True)\n",
    "\n",
    "# convert datasets to dataloaders for training\n",
    "batch_size = 124\n",
    "train_dataloader2 = training2.to_dataloader(train=True, batch_size=batch_size, num_workers=4)\n",
    "val_dataloader2 = validation2.to_dataloader(train=False, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# create study\n",
    "study23 = optimize_hyperparameters(\n",
    "    train_dataloader2,\n",
    "    val_dataloader2,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=8,\n",
    "    max_epochs=15,\n",
    "    gradient_clip_val_range=(0.2, 0.5),\n",
    "    hidden_size_range=(30, 40),\n",
    "    hidden_continuous_size_range=(30, 40),\n",
    "    attention_head_size_range=(3, 4),\n",
    "    learning_rate_range=(0.01, 0.3),\n",
    "    dropout_range=(0.2, 0.6),\n",
    "    trainer_kwargs=dict(limit_train_batches=batch_size),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    ")\n",
    "\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"pytorch_lightning_optuna_sim_500_60_nl_he.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study23, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study23.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('sim_500_60_nl_ho_train.csv')\n",
    "data2 = data2.sort_values(by=['series_id','time'], ignore_index=True)\n",
    "data2[\"time_idx\"] = data2.index.to_list()\n",
    "data2 = data2[['time', 'time_idx', 'series_id', 'value', 'c_t']]\n",
    "data2 = data2.rename(columns={'series_id': 'variable'})\n",
    "data2['tnc'] = ['1' if i == 'treated' else '0' for i in data2['c_t']]\n",
    "data2_processed = data2[['time', 'time_idx', 'variable', 'value','tnc']]\n",
    "# define the dataset, i.e. add metadata to pandas dataframe for the model to understand it\n",
    "max_encoder_length = 15\n",
    "max_prediction_length = 12\n",
    "\n",
    "training2 = TimeSeriesDataSet(\n",
    "    # data[lambda x: x.date <= training_cutoff],\n",
    "    data2_processed[lambda x: x.time_idx <= data2_processed[\"time_idx\"].max() - max_prediction_length*2],\n",
    "    group_ids=[\"tnc\",\"variable\"],  # column name(s) for timeseries IDs\n",
    "    target= \"value\",  # column name of target to predict\n",
    "    time_idx= \"time_idx\",  # column name of time of observation\n",
    "    max_encoder_length=max_encoder_length,  # how much history to use\n",
    "    max_prediction_length=max_prediction_length,  # how far to predict into future\n",
    "    static_categoricals=[\"tnc\",\"variable\"],\n",
    "    time_varying_unknown_reals=[\"value\"],\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"tnc\",\"variable\"], transformation=\"relu\"),\n",
    ")\n",
    "\n",
    "\n",
    "# create validation dataset using the same normalization techniques as for the training dataset\n",
    "validation2 = TimeSeriesDataSet.from_dataset(training2, data2_processed[lambda x: x.time_idx <= \\\n",
    "              data2_processed[\"time_idx\"].max() - max_prediction_length], stop_randomization=True)\n",
    "\n",
    "# convert datasets to dataloaders for training\n",
    "batch_size = 124\n",
    "train_dataloader2 = training2.to_dataloader(train=True, batch_size=batch_size, num_workers=4)\n",
    "val_dataloader2 = validation2.to_dataloader(train=False, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# create study\n",
    "study24 = optimize_hyperparameters(\n",
    "    train_dataloader2,\n",
    "    val_dataloader2,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=8,\n",
    "    max_epochs=15,\n",
    "    gradient_clip_val_range=(0.2, 0.5),\n",
    "    hidden_size_range=(30, 40),\n",
    "    hidden_continuous_size_range=(30, 40),\n",
    "    attention_head_size_range=(3, 4),\n",
    "    learning_rate_range=(0.01, 0.3),\n",
    "    dropout_range=(0.2, 0.6),\n",
    "    trainer_kwargs=dict(limit_train_batches=batch_size),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    ")\n",
    "\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"pytorch_lightning_optuna_sim_500_60_nl_ho.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study24, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study24.best_trial.params)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
