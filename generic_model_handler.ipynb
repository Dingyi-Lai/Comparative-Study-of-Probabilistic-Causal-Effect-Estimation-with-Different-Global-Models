{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "quantile_range = [0.1,0.5,0.9]\n",
    "model_identifier = 'callsMT215_without_stl_LSTMcell_cocob_without_stl_decomposition'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembled_forecasts = {}\n",
    "for q in quantile_range:\n",
    "    ensembled_forecasts[q] = pd.read_csv('./results/nn_model_results/rnn/forecasts/' +\\\n",
    "                                            model_identifier + \"_\" + str(q) +\".txt\",header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_args = ['./results/nn_model_results/rnn/ensemble_forecasts/',\n",
    "                   './results/nn_model_results/rnn/ensemble_errors/',\n",
    "                   './results/nn_model_results/rnn/processed_ensemble_forecasts/',\n",
    "                   model_identifier,\n",
    "                   'datasets/text_data/calls911/moving_window/without_stl_decomposition/callsMT2_test_7_15.txt',\n",
    "                   'datasets/text_data/calls911/callsMT2_results.txt',\n",
    "                   'datasets/text_data/calls911/callsMT2_dataset.txt',\n",
    "                   16,\n",
    "                   7,\n",
    "                   0,\n",
    "                   0,\n",
    "                   0,\n",
    "                   12,\n",
    "                   1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example values for testing\n",
    "rnn_forecast_file_path = evaluate_args[0]\n",
    "errors_directory = evaluate_args[1]\n",
    "processed_forecasts_directory = evaluate_args[2]\n",
    "txt_test_file_name = evaluate_args[4]\n",
    "actual_results_file_name = evaluate_args[5]\n",
    "original_data_file_name = evaluate_args[6]\n",
    "input_size = evaluate_args[7]\n",
    "output_size = evaluate_args[8]\n",
    "contain_zero_values = evaluate_args[9]\n",
    "address_near_zero_insability = evaluate_args[10]\n",
    "integer_conversion = evaluate_args[11]\n",
    "seasonality_period = evaluate_args[12]\n",
    "without_stl_decomposition = evaluate_args[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_directory = errors_directory + '/'\n",
    "errors_file_name = evaluate_args[3]\n",
    "errors_file_name_mean_median = 'mean_median_' + errors_file_name\n",
    "SMAPE_file_name_all_errors = 'all_smape_errors_' + errors_file_name\n",
    "MASE_file_name_all_errors = 'all_mase_errors_' + errors_file_name\n",
    "CRPS_file_name_all_errors = 'all_crps_errors_' + errors_file_name\n",
    "errors_file_full_name_mean_median = errors_directory + errors_file_name_mean_median\n",
    "SMAPE_file_full_name_all_errors = errors_directory + SMAPE_file_name_all_errors\n",
    "MASE_file_full_name_all_errors = errors_directory + MASE_file_name_all_errors\n",
    "CRPS_file_full_name_all_errors = errors_directory + CRPS_file_name_all_errors\n",
    "\n",
    "# Actual results file name\n",
    "actual_results = pd.read_csv(actual_results_file_name, sep=';', header=None)\n",
    "\n",
    "# Text test data file name\n",
    "txt_test_df = pd.read_csv(txt_test_file_name, sep=' ', header=None)\n",
    "\n",
    "# RNN forecasts file name as one of the argument which is ensembled_forecasts\n",
    "\n",
    "# Reading the original data to calculate the MASE errors\n",
    "with open(original_data_file_name, 'r') as file:\n",
    "    original_dataset = [line.strip().split(',') for line in file]\n",
    "\n",
    "# Persisting the final forecasts\n",
    "processed_forecasts_file = processed_forecasts_directory + errors_file_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_results_df = actual_results.drop(columns=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34, 69, 104, 139, 174, 209, 244, 279, 314, 349, 384, 419, 454, 489, 524, 559, 594, 629, 664, 699, 734, 769, 804, 839, 874, 909, 944, 979, 1014, 1049, 1084, 1119, 1154, 1189, 1224, 1259, 1294, 1329, 1364, 1399, 1434, 1469, 1504, 1539, 1574, 1609, 1644, 1679, 1714, 1749, 1784, 1819, 1854, 1889, 1924, 1959, 1994, 2029, 2064, 2099, 2134, 2169]\n"
     ]
    }
   ],
   "source": [
    "value = list(txt_test_df[0])\n",
    "uniqueindexes = [i-2 for i, val in enumerate(value, start=1) if val != value[i-2] and i!= 1]\n",
    "uniqueindexes.append(len(value)-1)\n",
    "print(uniqueindexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uniqueindexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_forecasts_df = None\n",
    "converted_forecasts_matrix = np.zeros((len(ensembled_forecasts[0.5]), output_size))\n",
    "converted_forecasts_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = ensembled_forecasts[0.5]\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_ts_forecasts = v.iloc[i].values\n",
    "finalindex = uniqueindexes[i]\n",
    "one_line_test_data = txt_test_df.iloc[finalindex].values\n",
    "mean_value = one_line_test_data[input_size + 2]\n",
    "level_value = one_line_test_data[input_size + 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "if without_stl_decomposition:\n",
    "    converted_forecasts_df = np.exp(one_ts_forecasts + level_value)\n",
    "else:\n",
    "    seasonal_values = one_line_test_data[(input_size + 4):(3 + input_size + output_size)]\n",
    "    converted_forecasts_df = np.exp(one_ts_forecasts + level_value + seasonal_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_forecasts_df = mean_value * converted_forecasts_df\n",
    "converted_forecasts_df[converted_forecasts_df < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([747.14157681, 735.49026393, 756.95454365, 741.5483753 ,\n",
       "       754.00993447, 743.51640905, 747.06482746])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_forecasts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>693</td>\n",
       "      <td>639</td>\n",
       "      <td>575</td>\n",
       "      <td>518</td>\n",
       "      <td>504</td>\n",
       "      <td>685</td>\n",
       "      <td>614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>69</td>\n",
       "      <td>82</td>\n",
       "      <td>83</td>\n",
       "      <td>58</td>\n",
       "      <td>105</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66</td>\n",
       "      <td>60</td>\n",
       "      <td>61</td>\n",
       "      <td>44</td>\n",
       "      <td>50</td>\n",
       "      <td>53</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>29</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>545</td>\n",
       "      <td>482</td>\n",
       "      <td>478</td>\n",
       "      <td>392</td>\n",
       "      <td>425</td>\n",
       "      <td>454</td>\n",
       "      <td>514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>206</td>\n",
       "      <td>192</td>\n",
       "      <td>197</td>\n",
       "      <td>152</td>\n",
       "      <td>192</td>\n",
       "      <td>198</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>73</td>\n",
       "      <td>55</td>\n",
       "      <td>48</td>\n",
       "      <td>27</td>\n",
       "      <td>50</td>\n",
       "      <td>49</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>328</td>\n",
       "      <td>273</td>\n",
       "      <td>230</td>\n",
       "      <td>220</td>\n",
       "      <td>204</td>\n",
       "      <td>299</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>268</td>\n",
       "      <td>220</td>\n",
       "      <td>182</td>\n",
       "      <td>188</td>\n",
       "      <td>171</td>\n",
       "      <td>210</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>111</td>\n",
       "      <td>108</td>\n",
       "      <td>56</td>\n",
       "      <td>55</td>\n",
       "      <td>83</td>\n",
       "      <td>82</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1    2    3    4    5    6    7\n",
       "0   693  639  575  518  504  685  614\n",
       "1    67   69   82   83   58  105   53\n",
       "2    66   60   61   44   50   53   65\n",
       "3    25   15   18   10   14   29   24\n",
       "4   545  482  478  392  425  454  514\n",
       "..  ...  ...  ...  ...  ...  ...  ...\n",
       "57  206  192  197  152  192  198  197\n",
       "58   73   55   48   27   50   49   46\n",
       "59  328  273  230  220  204  299  219\n",
       "60  268  220  182  188  171  210  164\n",
       "61  111  108   56   55   83   82   85\n",
       "\n",
       "[62 rows x 7 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mase_greybox(holdout, forecast, scale, na_rm=True):\n",
    "    \"\"\"\n",
    "    Calculates Mean Absolute Scaled Error as in Hyndman & Koehler, 2006.\n",
    "    \n",
    "    Reference: https://github.com/config-i1/greybox/blob/6c84c729786f33a474ef833a13b7715831bd29e6/R/error-measures.R#L267\n",
    "\n",
    "    Parameters:\n",
    "        holdout (list or numpy array): Holdout values.\n",
    "        forecast (list or numpy array): Forecasted values.\n",
    "        scale (float): The measure to scale errors with. Usually - MAE of in-sample.\n",
    "        na_rm (bool, optional): Whether to remove NA values from calculations.\n",
    "                                Default is True.\n",
    "\n",
    "    Returns:\n",
    "        float: Mean Absolute Scaled Error.\n",
    "    \"\"\"\n",
    "    if len(holdout) != len(forecast):\n",
    "        print(\"The length of the provided data differs.\")\n",
    "        print(f\"Length of holdout: {len(holdout)}\")\n",
    "        print(f\"Length of forecast: {len(forecast)}\")\n",
    "        raise ValueError(\"Cannot proceed.\")\n",
    "    else:\n",
    "        return np.mean(np.abs(np.array(holdout) - np.array(forecast)) / scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import CubicSpline\n",
    "mase_vector = []\n",
    "quantile_distr = {}\n",
    "crps_vector = []\n",
    "num_q = len(ensembled_forecasts.keys())\n",
    "for k, v in ensembled_forecasts.items():\n",
    "    # Perform spline regression for each time series\n",
    "    num_time_series = v.shape[0]\n",
    "    smoothed_data = np.zeros_like(v)\n",
    "    for i in range(num_time_series):\n",
    "\n",
    "        # post-processing\n",
    "        one_ts_forecasts = v.iloc[i].values\n",
    "        finalindex = uniqueindexes[i]\n",
    "        one_line_test_data = txt_test_df.iloc[finalindex].values\n",
    "        mean_value = one_line_test_data[input_size + 2]\n",
    "        level_value = one_line_test_data[input_size + 3]\n",
    "        if without_stl_decomposition:\n",
    "            converted_forecasts_df = np.exp(one_ts_forecasts + level_value)\n",
    "        else:\n",
    "            seasonal_values = one_line_test_data[(input_size + 4):(3 + input_size + output_size)]\n",
    "            converted_forecasts_df = np.exp(one_ts_forecasts + level_value + seasonal_values)\n",
    "        \n",
    "        if contain_zero_values:\n",
    "            converted_forecasts_df = converted_forecasts_df - 1\n",
    "\n",
    "        converted_forecasts_df = mean_value * converted_forecasts_df\n",
    "        converted_forecasts_df[converted_forecasts_df < 0] = 0\n",
    "\n",
    "        if integer_conversion:\n",
    "            converted_forecasts_df = np.round(converted_forecasts_df)\n",
    "\n",
    "        converted_forecasts_matrix[i, :] = converted_forecasts_df # one_ts_forecasts\n",
    "        \n",
    "        if k == 0.5:\n",
    "            # np.diff(np.array(original_dataset[i]), lag=seasonality_period, differences=1))\n",
    "            original_values = list(map(float, original_dataset[i]))\n",
    "            lagged_diff = [original_values[i] - original_values[i - seasonality_period] for i in range(seasonality_period, len(original_values))]\n",
    "            mase_vector.append(mase_greybox(np.array(actual_results_df.iloc[i]), converted_forecasts_df, np.mean(np.abs(lagged_diff))))\n",
    "        \n",
    "    crps_vector.append(converted_forecasts_matrix)\n",
    "cs = CubicSpline(list(ensembled_forecasts.keys()), crps_vector, bc_type='natural')\n",
    "# smoothed_data[:, i] = cs(num_q)\n",
    "\n",
    "# for i in range(num_time_series):\n",
    "#     ts_without_cs = []\n",
    "#     for k, v in quantile_distr:\n",
    "#         ts_without_cs.append(quantile_distr[k][])\n",
    "\n",
    "\n",
    "#     if k == 0.5:\n",
    "\n",
    "#     else:\n",
    "#         # Store the results\n",
    "#         quantile_distr[k] = smoothed_data\n",
    "#         # print(crps_y_pred, v)\n",
    "#         crps_y_pred.extend(v.values)\n",
    "#         crps_y_true.extend(smoothed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "crps_y_pred = np.transpose(cs(list(ensembled_forecasts.keys())), (1, 0, 2))\n",
    "crps_y_pred2 = cs(list(ensembled_forecasts.keys())).transpose((0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_weighted_quantile_loss(\n",
    "    y_pred: np.ndarray, y_true: np.ndarray, quantiles: list\n",
    ") -> float:\n",
    "    y_true_rep = y_true[:, None].repeat(len(quantiles), axis=1)\n",
    "    quantiles = np.array([float(q) for q in quantiles])\n",
    "    # print(quantiles_repeated.shape, y_pred.shape, y_true.shape)\n",
    "    quantile_losses = 2 * np.sum(\n",
    "        np.abs(\n",
    "            (y_pred - y_true_rep)\n",
    "            * ((y_true_rep <= y_pred) - quantiles[:, None])\n",
    "        ),\n",
    "        axis=-1,\n",
    "    )  # shape [num_time_series, num_quantiles]\n",
    "    # denom = np.sum(np.abs(y_true_rep))  # shape [1]\n",
    "    denom = np.sum(np.abs(y_true_rep))  # shape [1]\n",
    "    # weighted_losses = quantile_losses.sum(0) / denom  # shape [num_quantiles]\n",
    "    weighted_losses = quantile_losses / denom  # shape [num_quantiles]\n",
    "    # return weighted_losses.mean()\n",
    "    return weighted_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,7,62) (62,3,7) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/aubrey/Documents/GitHub/Master-s_Thesis/generic_model_handler.ipynb Cell 21\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aubrey/Documents/GitHub/Master-s_Thesis/generic_model_handler.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m crps \u001b[39m=\u001b[39m mean_weighted_quantile_loss(crps_y_pred2, np\u001b[39m.\u001b[39;49marray(actual_results_df), ensembled_forecasts\u001b[39m.\u001b[39;49mkeys())\n",
      "\u001b[1;32m/Users/aubrey/Documents/GitHub/Master-s_Thesis/generic_model_handler.ipynb Cell 21\u001b[0m line \u001b[0;36mmean_weighted_quantile_loss\u001b[0;34m(y_pred, y_true, quantiles)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aubrey/Documents/GitHub/Master-s_Thesis/generic_model_handler.ipynb#X41sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m quantiles \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39mfloat\u001b[39m(q) \u001b[39mfor\u001b[39;00m q \u001b[39min\u001b[39;00m quantiles])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aubrey/Documents/GitHub/Master-s_Thesis/generic_model_handler.ipynb#X41sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# print(quantiles_repeated.shape, y_pred.shape, y_true.shape)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aubrey/Documents/GitHub/Master-s_Thesis/generic_model_handler.ipynb#X41sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m quantile_losses \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msum(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aubrey/Documents/GitHub/Master-s_Thesis/generic_model_handler.ipynb#X41sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     np\u001b[39m.\u001b[39mabs(\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aubrey/Documents/GitHub/Master-s_Thesis/generic_model_handler.ipynb#X41sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         (y_pred \u001b[39m-\u001b[39;49m y_true_rep)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aubrey/Documents/GitHub/Master-s_Thesis/generic_model_handler.ipynb#X41sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         \u001b[39m*\u001b[39m ((y_true_rep \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m y_pred) \u001b[39m-\u001b[39m quantiles[:, \u001b[39mNone\u001b[39;00m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aubrey/Documents/GitHub/Master-s_Thesis/generic_model_handler.ipynb#X41sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     ),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aubrey/Documents/GitHub/Master-s_Thesis/generic_model_handler.ipynb#X41sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aubrey/Documents/GitHub/Master-s_Thesis/generic_model_handler.ipynb#X41sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m )  \u001b[39m# shape [num_time_series, num_quantiles]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aubrey/Documents/GitHub/Master-s_Thesis/generic_model_handler.ipynb#X41sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# denom = np.sum(np.abs(y_true_rep))  # shape [1]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aubrey/Documents/GitHub/Master-s_Thesis/generic_model_handler.ipynb#X41sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m denom \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39mabs(y_true_rep))  \u001b[39m# shape [1]\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,7,62) (62,3,7) "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "crps = mean_weighted_quantile_loss(crps_y_pred2, np.array(actual_results_df), ensembled_forecasts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07881886039567859"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crps.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003813815825597352"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crps.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13576872, 0.07881886, 0.021869  ])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crps.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07881886039567862"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crps.sum(0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62, 3, 7)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.transpose(cs(list(ensembled_forecasts.keys())), (1, 0, 2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 7, 62)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs(list(ensembled_forecasts.keys())).transpose((0, 2, 1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62, 3)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 62)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.transpose(crps).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13576872, 0.07881886, 0.021869  ])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.transpose(crps).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Training Started for sim_10_60_l_he_LSTMcell_cocob_without_stl_decomposition\n"
     ]
    }
   ],
   "source": [
    "# Record the running time\n",
    "\n",
    "# Inbuilt or External Modules\n",
    "import argparse # customized arguments in .bash\n",
    "import csv # input and output .csv data\n",
    "import glob # file matching using wildcards\n",
    "import numpy as np # for numerical computing\n",
    "import os # OS routines\n",
    "# import pdb # debugger, but it doesn't work remotely\n",
    "import pandas as pd\n",
    "\n",
    "# Customized Modules\n",
    "from configs.global_configs import hyperparameter_tuning_configs\n",
    "from configs.global_configs import model_testing_configs\n",
    "# from error_calculator.final_evaluation import evaluate\n",
    "from utility_scripts.ensembling_forecasts import ensembling_forecasts\n",
    "# from utility_scripts.invoke_final_evaluation import invoke_script # for invoking R\n",
    "from utility_scripts.hyperparameter_scripts.hyperparameter_config_reader import read_initial_hyperparameter_values, read_optimal_hyperparameter_values\n",
    "from utility_scripts.persist_optimized_config_results import persist_results\n",
    "# import SMAC utilities\n",
    "# import the config space and the different types of parameters\n",
    "\n",
    "# stacking model\n",
    "# from rnn_architectures.stacking_model_p import StackingModel\n",
    "\n",
    "\n",
    "LSTM_USE_PEEPHOLES = True # LSTM with “peephole connections\"\n",
    "BIAS = False # in tf.keras.layers.dense\n",
    "\n",
    "# arguments with no default values\n",
    "dataset_name = \"sim_10_60_l_he\"\n",
    "no_of_series = int(10)\n",
    "initial_hyperparameter_values_file = \"configs/initial_hyperparameter_values/ems_adagrad\"\n",
    "binary_train_file_path_train_mode = \"datasets/binary_data/sim/moving_window/sim_10_60_linear_heterogeneous_train_12_15.tfrecords\"\n",
    "binary_validation_file_path_train_mode = \"datasets/binary_data/sim/moving_window/sim_10_60_linear_heterogeneous_validation_12_15.tfrecords\"\n",
    "binary_test_file_path_test_mode = \"datasets/binary_data/sim/moving_window/sim_10_60_linear_heterogeneous_test_12_15.tfrecords\"\n",
    "output_size = 12\n",
    "txt_test_file_path = \"datasets/text_data/sim/moving_window/sim_10_60_linear_heterogeneous_test_12_15.txt\"\n",
    "actual_results_file_path = \"datasets/text_data/sim/sim_10_60_linear_heterogeneous_test_actual.csv\"\n",
    "original_data_file_path = \"datasets/text_data/sim/sim_10_60_linear_heterogeneous_train.csv\"\n",
    "seasonality_period = 12\n",
    "seed = 1234\n",
    "cell_type = \"LSTM\"\n",
    "optimizer = \"cocob\"\n",
    "evaluation_metric = \"CRPS\"\n",
    "stl_decomposition_identifier = \"without_stl_decomposition\"\n",
    "model_identifier = dataset_name + \"_\" + cell_type + \"cell\" + \"_\" +  optimizer + \"_\" + \\\n",
    "                       stl_decomposition_identifier\n",
    "input_size = 16\n",
    "contain_zero_values = 0\n",
    "address_near_zero_instability = 0\n",
    "integer_conversion = 0\n",
    "quantile_range = [0.1,0.5,0.9]\n",
    "without_stl_decomposition = bool(int(1))\n",
    "print(\"Model Training Started for {}\".format(model_identifier))\n",
    "\n",
    "\n",
    "# # define the key word arguments for the different model types\n",
    "# model_kwargs = {\n",
    "#     'use_bias': BIAS,\n",
    "#     'use_peepholes': LSTM_USE_PEEPHOLES,\n",
    "#     'input_size': input_size,\n",
    "#     'output_size': output_size,\n",
    "#     'optimizer': optimizer,\n",
    "#     'quantile_range': quantile_range,\n",
    "#     'evaluation_metric': evaluation_metric,\n",
    "#     'no_of_series': no_of_series,\n",
    "#     'binary_train_file_path': binary_train_file_path_train_mode,\n",
    "#     'binary_test_file_path': binary_test_file_path_test_mode,\n",
    "#     'binary_validation_file_path': binary_validation_file_path_train_mode,\n",
    "#     'contain_zero_values': contain_zero_values,\n",
    "#     'address_near_zero_instability': address_near_zero_instability,\n",
    "#     'integer_conversion': integer_conversion,\n",
    "\n",
    "#     'seed': seed,\n",
    "#     'cell_type': cell_type,\n",
    "#     'without_stl_decomposition': without_stl_decomposition\n",
    "# }\n",
    "\n",
    "# # select the model type\n",
    "# model = StackingModel(**model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cell_dimension': 21.0, 'gaussian_noise_stdev': 0.0005034929257383058, 'l2_regularization': 0.00014375, 'max_epoch_size': 10.0, 'max_num_epochs': 16.0, 'minibatch_size': 7.0, 'num_hidden_layers': 2.0, 'random_normal_initializer_stdev': 0.00023125}\n",
      "tuning finished\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFRecordReader.train_data_parser_for_testing of <tfrecords_handler.moving_window.tfrecord_reader.TFRecordReader object at 0x18168cb80>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFRecordReader.train_data_parser_for_testing of <tfrecords_handler.moving_window.tfrecord_reader.TFRecordReader object at 0x18168cb80>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFRecordReader.test_data_input_parser of <tfrecords_handler.moving_window.tfrecord_reader.TFRecordReader object at 0x18168cb80>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFRecordReader.test_data_input_parser of <tfrecords_handler.moving_window.tfrecord_reader.TFRecordReader object at 0x18168cb80>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFRecordReader.test_data_lengths_parser of <tfrecords_handler.moving_window.tfrecord_reader.TFRecordReader object at 0x18168cb80>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFRecordReader.test_data_lengths_parser of <tfrecords_handler.moving_window.tfrecord_reader.TFRecordReader object at 0x18168cb80>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Model: \"stacking_model_p\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inputs (InputLayer)         [(None, None, 16)]        0         \n",
      "                                                                 \n",
      " masking (Masking)           (None, None, 16)          0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 12)          192       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 192 (768.00 Byte)\n",
      "Trainable params: 192 (768.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aubrey/miniconda3/envs/Master_Thesis/lib/python3.9/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# not training again but just read in\n",
    "optimized_configuration = read_optimal_hyperparameter_values(\"./results/nn_model_results/rnn/optimized_configurations/\" + model_identifier + \".txt\")\n",
    "print(optimized_configuration)\n",
    "\n",
    "print(\"tuning finished\")\n",
    "\n",
    "for seed in range(1, 11):\n",
    "    forecasts = model.test_model(optimized_configuration, seed)\n",
    "\n",
    "    model_identifier_extended = model_identifier + \"_\" + str(seed)\n",
    "    # for k, v in forecasts.items():\n",
    "        # rnn_forecasts_file_path = model_testing_configs.FORECASTS_DIRECTORY + model_identifier_extended + 'q_' + str(k) + '.txt'\n",
    "        \n",
    "        # with open(rnn_forecasts_file_path, \"w\") as output:\n",
    "        #     writer = csv.writer(output, lineterminator='\\n')\n",
    "        #     writer.writerows(forecasts[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_optimal_hyperparameter_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# not training again but just read in\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m optimized_configuration \u001b[38;5;241m=\u001b[39m \u001b[43mread_optimal_hyperparameter_values\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results/nn_model_results/rnn/optimized_configurations/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model_identifier \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(optimized_configuration)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtuning finished\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'read_optimal_hyperparameter_values' is not defined"
     ]
    }
   ],
   "source": [
    "# not training again but just read in\n",
    "optimized_configuration = read_optimal_hyperparameter_values(\"./results/nn_model_results/rnn/optimized_configurations/\" + model_identifier + \".txt\")\n",
    "print(optimized_configuration)\n",
    "\n",
    "print(\"tuning finished\")\n",
    "\n",
    "for seed in range(1, 11):\n",
    "    forecasts = model.test_model(optimized_configuration, seed)\n",
    "\n",
    "    model_identifier_extended = model_identifier + \"_\" + str(seed)\n",
    "    for k, v in forecasts.items():\n",
    "        rnn_forecasts_file_path = model_testing_configs.FORECASTS_DIRECTORY + model_identifier_extended + 'q_' + str(k) + '.txt'\n",
    "        \n",
    "        with open(rnn_forecasts_file_path, \"w\") as output:\n",
    "            writer = csv.writer(output, lineterminator='\\n')\n",
    "            writer.writerows(forecasts[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "testing_dataset_for_test = tf.data.TFRecordDataset(filenames=[binary_test_file_path_test_mode],\n",
    "                                               compression_type=\"ZLIB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TFRecordDatasetV2 element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_dataset_for_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 16]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the expected shapes of data after padding\n",
    "test_padded_shapes = ([None, input_size])\n",
    "test_padded_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFRecordReader.test_data_input_parser of <tfrecords_handler.moving_window.tfrecord_reader.TFRecordReader object at 0x7fa34bff9c30>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFRecordReader.test_data_input_parser of <tfrecords_handler.moving_window.tfrecord_reader.TFRecordReader object at 0x7fa34bff9c30>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFRecordReader.test_data_lengths_parser of <tfrecords_handler.moving_window.tfrecord_reader.TFRecordReader object at 0x7fa34bff9c30>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFRecordReader.test_data_lengths_parser of <tfrecords_handler.moving_window.tfrecord_reader.TFRecordReader object at 0x7fa34bff9c30>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "from tfrecords_handler.moving_window.tfrecord_reader import TFRecordReader\n",
    "meta_data_size = 2\n",
    "tfrecord_reader = TFRecordReader(input_size, output_size, meta_data_size)\n",
    "testing_dataset_input_parsed = testing_dataset_for_test.map(tfrecord_reader.test_data_input_parser)\n",
    "testing_dataset_input_padded = testing_dataset_input_parsed.padded_batch(no_of_series, test_padded_shapes)\n",
    "\n",
    "testing_dataset_lengths_parsed = testing_dataset_for_test.map(tfrecord_reader.test_data_lengths_parser)\n",
    "testing_dataset_lengths = testing_dataset_lengths_parsed.batch(no_of_series)\n",
    "\n",
    "for lengths in testing_dataset_lengths:\n",
    "    testing_dataset_lengths = lengths.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([34, 34, 34, 34, 34, 34, 34, 34, 34, 34])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_dataset_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PaddedBatchDataset element_spec=TensorSpec(shape=(None, None, 16), dtype=tf.float32, name=None)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_dataset_input_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from external_packages.peephole_lstm_cell import PeepholeLSTMCell\n",
    "# from quantile_utils.CRPS_QL import pinball_loss, PinballLoss\n",
    "# import tensorflow_addons as tfa\n",
    "def build_model(random_normal_initializer_stdev, num_hidden_layers, cell_dimension, l2_regularization, q, optimizer):\n",
    "    # def __build_model(self, random_normal_initializer_stdev, num_hidden_layers, cell_dimension, q, optimizer):\n",
    "        initializer = tf.keras.initializers.TruncatedNormal(stddev=random_normal_initializer_stdev)\n",
    "\n",
    "        # model from the functional API\n",
    "        input = tf.keras.Input(shape=(None, input_size), name='inputs')\n",
    "        masked_output = tf.keras.layers.Masking(mask_value=0.0)(input)\n",
    "\n",
    "        # lstm stack\n",
    "        next_input = masked_output\n",
    "        for i in range(num_hidden_layers):\n",
    "            lstm_output = tf.keras.layers.RNN(tf.keras.layers.LSTMCell(cell_dimension, kernel_initializer=initializer), return_sequences=True) (next_input)\n",
    "            next_input = lstm_output\n",
    "\n",
    "        dense_layer_output = tf.keras.layers.Dense(output_size, use_bias=BIAS, kernel_initializer=initializer) (masked_output)\n",
    "\n",
    "        # build the model\n",
    "        model = tf.keras.Model(inputs=input, outputs=dense_layer_output, name='stacking_model_p')\n",
    "\n",
    "        # plot the model to validate\n",
    "        model.summary()\n",
    "        def custom_mae(y_true, y_pred):\n",
    "            error = tf.keras.losses.mae(y_true, y_pred)\n",
    "            l2_loss = 0.0\n",
    "            for var in model.trainable_variables:\n",
    "                l2_loss += tf.nn.l2_loss(var)\n",
    "\n",
    "            l2_loss = tf.math.multiply(l2_regularization, l2_loss)\n",
    "\n",
    "            total_loss = error + l2_loss\n",
    "            return total_loss\n",
    "        \n",
    "        model.compile(loss=custom_mae,\n",
    "                      optimizer=optimizer)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded in comparison",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m cell_dimension\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m21\u001b[39m\n\u001b[1;32m      5\u001b[0m l2_regularization\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.00014375\u001b[39m\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_normal_initializer_stdev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_hidden_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell_dimension\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_regularization\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 6\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(random_normal_initializer_stdev, num_hidden_layers, cell_dimension, l2_regularization, q, optimizer)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_model\u001b[39m(random_normal_initializer_stdev, num_hidden_layers, cell_dimension, l2_regularization, q, optimizer):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# def __build_model(self, random_normal_initializer_stdev, num_hidden_layers, cell_dimension, q, optimizer):\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m         initializer \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[38;5;241m.\u001b[39mTruncatedNormal(stddev\u001b[38;5;241m=\u001b[39mrandom_normal_initializer_stdev)\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;66;03m# model from the functional API\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, input_size), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/lazy_loader.py:147\u001b[0m, in \u001b[0;36mKerasLazyLoader.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialized:\n\u001b[1;32m    146\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize()\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_keras_version\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras_3\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    148\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv1\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    149\u001b[0m       \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_submodule \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    150\u001b[0m       item\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompat.v1.\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.compat.v1.keras` is not available with Keras 3. Keras 3 has \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno support for TF 1 APIs. You can install the `tf_keras` package \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.compat.v1.keras` to `tf_keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/lazy_loader.py:147\u001b[0m, in \u001b[0;36mKerasLazyLoader.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialized:\n\u001b[1;32m    146\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize()\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_keras_version\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras_3\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    148\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv1\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    149\u001b[0m       \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_submodule \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    150\u001b[0m       item\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompat.v1.\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.compat.v1.keras` is not available with Keras 3. Keras 3 has \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno support for TF 1 APIs. You can install the `tf_keras` package \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.compat.v1.keras` to `tf_keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n",
      "    \u001b[0;31m[... skipping similar frames: KerasLazyLoader.__getattr__ at line 147 (2968 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/lazy_loader.py:147\u001b[0m, in \u001b[0;36mKerasLazyLoader.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialized:\n\u001b[1;32m    146\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize()\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_keras_version\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras_3\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    148\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv1\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    149\u001b[0m       \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_submodule \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    150\u001b[0m       item\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompat.v1.\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.compat.v1.keras` is not available with Keras 3. Keras 3 has \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno support for TF 1 APIs. You can install the `tf_keras` package \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.compat.v1.keras` to `tf_keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/lazy_loader.py:143\u001b[0m, in \u001b[0;36mKerasLazyLoader.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, item):\n\u001b[0;32m--> 143\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_mode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_initialized\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(types\u001b[38;5;241m.\u001b[39mModuleType, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)\n\u001b[1;32m    145\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialized:\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded in comparison"
     ]
    }
   ],
   "source": [
    "random_normal_initializer_stdev = 0.00023125\n",
    "q=0.1\n",
    "num_hidden_layers=2\n",
    "cell_dimension=21\n",
    "l2_regularization=0.00014375\n",
    "\n",
    "\n",
    "model = build_model(random_normal_initializer_stdev, num_hidden_layers, cell_dimension, l2_regularization, q, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
